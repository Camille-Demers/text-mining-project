{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e08799",
   "metadata": {},
   "source": [
    "# **1. Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce6a9d6",
   "metadata": {},
   "source": [
    "## Crawl URLs to extract all internal links "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f4c66",
   "metadata": {},
   "source": [
    "**XENU Link Sleuth**\\\n",
    "https://home.snafu.de/tilman/xenulink.html \n",
    "\n",
    "*Le logiciel XENU Link Sleuth a finalement été retenu pour cette tâche* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f80c7f",
   "metadata": {},
   "source": [
    "## Scrape textual data from crawled URLs\n",
    "**BeautifulSoup HTML Parser**\\\n",
    "Réf : https://realpython.com/python-web-scraping-practical-introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14233647",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/03-corpus/0-listes_sites/'\n",
    "acteur = \"msss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6eb72",
   "metadata": {},
   "source": [
    "La liste des URLs à scrapper pour chaque corpus est contenue dans un fichier CSV. \n",
    "On commence donc par lire le CSV pour extraire nos URLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a19cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "\n",
    "# encoding=\"utf-8\"\n",
    "with open(path + acteur + \"/\" + acteur + '.csv') as f:\n",
    "    csv = read_csv(f, sep=\";\")\n",
    "\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = csv['Address'].tolist()\n",
    "liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, ssl, os, sys\n",
    "from bs4 import BeautifulSoup\n",
    "#from requests.packages.urllib3.util.retry import Retry\n",
    "from slugify import slugify\n",
    "\n",
    "def getTextURL(url):    \n",
    "    html = requests.get(url, headers = {'User-Agent': 'My User Agent 1.0'}, verify=False)\n",
    "    html.encoding = 'utf-8'\n",
    "    html = html.text\n",
    "   \n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    data = soup.get_text(separator=' ').replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(u'\\xa0', u' ')\n",
    "    data = re.compile(r\"\\s+\").sub(\" \", data).strip() + \"\\n\\n\"\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7cb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/03-corpus/2-data/1-fr/'\n",
    "for site in liste: \n",
    "    data = \"\"\n",
    "    try: \n",
    "        text = getTextURL(site)\n",
    "        data += text\n",
    "\n",
    "        # encoding = liste[1][liste[0].index(site)] - pour prendre l'encodage qui a été extrait par le crawleur\n",
    "        file_path = path + acteur + \"/\" + acteur + \"-\" + str(liste.index(site)) + \".txt\"\n",
    "        with open(file_path, 'w', encoding = \"utf-8\") as f:\n",
    "            f.write(site + \"\\n\")\n",
    "            f.write(data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"ERROR \" + \" - \" + site)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0964c3",
   "metadata": {},
   "source": [
    "## Combine .txt files into corpora\n",
    "Réf : https://www.geeksforgeeks.org/how-to-read-multiple-text-files-from-folder-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les données (fichiers .txt)\n",
    "import os\n",
    "\n",
    "acteur = \"chu\"\n",
    "path = '/Users/camilledemers/Documents/03-corpus/2-data/1-fr/'\n",
    "corpus = []\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path + acteur + \"/\")\n",
    "len(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd0c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = path + acteur + \"/\" + file\n",
    "        \n",
    "        with open(file_path, 'r', encoding = \"UTF-8\") as f:\n",
    "            data = f.readlines()\n",
    "            corpus.append(data[1])\n",
    "\n",
    "with open (path + acteur + \"/\" + acteur + '-corpus_FR.txt', 'w', encoding= 'UTF-8') as f: \n",
    "    for s in corpus:\n",
    "        f.write(s) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0bbc0",
   "metadata": {},
   "source": [
    "## Combine whole FR/EN corpus together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6aad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/03-corpus/2-data/1-fr/'\n",
    "acteur = 'ramq'\n",
    "corpus = ''\n",
    "\n",
    "os.chdir(path + acteur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = path + acteur + '/' + file\n",
    "        \n",
    "        with open(file_path, 'r', encoding = \"UTF-8\") as f:\n",
    "            data = f.readlines()\n",
    "            corpus += data[1] + ' '\n",
    "\n",
    "with open (path + acteur + '/' + acteur + '_corpus.txt', 'w', encoding= 'UTF-8') as f: \n",
    "    f.write(corpus)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a33210152f7d2bd255fb16656f372b633dbf298ed202bbbac20290b0375cadb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
