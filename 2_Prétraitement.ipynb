{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c662c7d",
   "metadata": {},
   "source": [
    "## **3. Prétraitement**\n",
    "- Segmentation (phrases)\n",
    "- Tokenization (mots)\n",
    "- Filtrage (stopwords)\n",
    "- Extraction de termes complexes (MWE / n-grammes)\n",
    "- Extraction de concordances (KWIC) pour un ensemble de mots-clés d'intérêt\n",
    "- Extraction de termes MeSH et SNOMED présents dans les données\n",
    "- Étiquetage morphosyntaxique (POS Tagging) \n",
    "- Lemmatisation\n",
    "- Chunking / Filtrage par patrons syntaxiques (basés sur les termes MeSH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a4b16",
   "metadata": {},
   "source": [
    "**NLTK**\\\n",
    "https://www.nltk.org/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31145e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(['popular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/03-corpus/2-data/1-fr/'\n",
    "acteur = 'cisss_ciusss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5561d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + acteur + '/' + acteur + '_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = ''.join([x for x in f.read().lower() if x.isprintable()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ech = corpus[:round(len(corpus)/10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c732a6",
   "metadata": {},
   "source": [
    "### **Segmentation** (phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize \n",
    "\n",
    "sents = [s.strip('.') for s in sent_tokenize(ech)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980c335",
   "metadata": {},
   "source": [
    "### **Tokenisation** (mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Seulement les caractères alphabétiques\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tokenizer_re.tokenize(s) for s in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab9f5e",
   "metadata": {},
   "source": [
    "### **Filtrage** (antidictionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer l'antidictionnaire pour filtrer les données\n",
    "from pandas import *\n",
    "\n",
    "# Stopwords fréquents en français\n",
    "path = \"/Users/camilledemers/Documents/04-filtrage/stopwords.csv\"\n",
    "with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = read_csv(f)\n",
    "    stopwords = [t.lower() for t in stopwords['Stopwords'].tolist()]\n",
    "\n",
    "\n",
    "# Stopwords fréquents en anglais\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/stop_words_english.txt'\n",
    "with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "    sw = [w.strip('\\n').lower() for w in f.readlines()]\n",
    "\n",
    "stopwords += sw\n",
    "\n",
    "# Signes de ponctuation\n",
    "import string \n",
    "punct = [s for s in string.punctuation] \n",
    "punct += ['»' ,'©', '']\n",
    "\n",
    "stopwords += punct\n",
    "\n",
    "#Prénoms (curieusement, il y en a beaucoup dans les données)\n",
    "\n",
    "# Mis en commentaire pour l'instant car ça allonge le délai de traitement\n",
    "\n",
    "# path = '/Users/camilledemers/Documents/04-filtrage/Prenoms.csv'\n",
    "# with open(path, 'r', encoding='utf-8') as f:\n",
    "#     sw = read_csv(f)\n",
    "#     sw = [str(t).lower() for t in sw['01_prenom'].tolist()]\n",
    "\n",
    "# stopwords += sw\n",
    "\n",
    "#Noms de famille \n",
    "# path = '/Users/camilledemers/Documents/04-filtrage/nomsFamille.csv'\n",
    "# with open(path, 'r', encoding='utf-8') as f:\n",
    "#     sw = read_csv(f)\n",
    "#     sw = [str(t).lower() for t in sw['Nom'].tolist()]\n",
    "\n",
    "# stopwords += sw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0ac5e",
   "metadata": {},
   "source": [
    "### **Filtrage (MWE - stopwords formés de plusieurs tokens)**\n",
    "Surtout pour filtrer les expressions relatives à l'architecture d'information / navigation Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/mwe_stopwords.txt'\n",
    "\n",
    "with open (path, 'r', encoding='utf-8') as f:\n",
    "    mwe_sw = [tuple(tokenizer_re.tokenize(t)) for t in f.readlines()]\n",
    "    #mwe_sw = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer_mwe = MWETokenizer(mwe_sw, separator=' ')\n",
    "\n",
    "mwe_sw = [tokenizer_mwe.tokenize(w)[0] for w in mwe_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[t for t in tokenizer_mwe.tokenize(sent) if t not in mwe_sw] for sent in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b5f4d",
   "metadata": {},
   "source": [
    "### **Phrases / N-Grammes (MWE)**\n",
    "https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addba715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une fonction pour importer en CSV un tableau de termes triés par distribution de fréquences\n",
    "import pandas as pd\n",
    "\n",
    "def tabCSV (tab, titre):\n",
    "    path = '/Users/camilledemers/Documents/04-filtrage/' + acteur + '/'\n",
    "    tab = pd.DataFrame(list(tab.items()), columns= [\"Terme\", \"Fréquence\"])\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "    tab.to_csv(path + titre + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import flatten, everygrams\n",
    "ngrammes = [x for xs in [list(everygrams(sent, min_len=2, max_len=10)) for sent in tokens] for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a008d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On retire maintenant les n-grammes qui débutent ou terminent par :\n",
    "# - un stopword\n",
    "# - un chiffre\n",
    "# - un mot de deux lettres ou moins\n",
    "def filtreNgrams(ngramme):\n",
    "    return [\" \".join(t) for t in ngramme if not t[0] in stopwords and not t[0].isnumeric() and not t[-1] in stopwords and not t[-1].isnumeric() and len(t[0]) > 2 and len(t[-1]) > 2] \n",
    "\n",
    "terms = filtreNgrams(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist(terms)\n",
    "tabCSV(freq, acteur + '_n-grams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ef6e6",
   "metadata": {},
   "source": [
    "### **KWIC (Keyword in Context)**\n",
    "Termes d'intérêt : \n",
    "- « Programme »\n",
    "- « Service(s) de » \n",
    "- « Intervenant(e) en »\n",
    "- « Professionnel de »\n",
    "- « Institut (du/de) »\n",
    "- « Groupe de recherche en »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans notre cas on veut que ça débute par le mot-clé donc le contexte est un peu plus simple\n",
    "# penser à généraliser avec des expressions régulières \n",
    "\n",
    "kw = ['programme', 'service', 'intervenant', 'institut', 'groupe de recherche']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c50b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrant = pd.DataFrame(columns=['Mot-clé','Concordance', \"Fréquence\"])\n",
    "kwic = {w : [] for w in kw} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255df07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in terms:\n",
    "    for w in kw:\n",
    "        if t.startswith(w):\n",
    "            kwic[w].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic = {term: FreqDist(kwic[term]) for term in kwic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in kw:\n",
    "    df = pd.DataFrame(kwic[term].items(), columns=['Concordance', \"Fréquence\"])\n",
    "    df.sort_values([\"Fréquence\"], \n",
    "        axis=0,\n",
    "        ascending=[False], \n",
    "        inplace=True)\n",
    "\n",
    "    df.insert(0, 'Mot-clé', term)\n",
    "    extrant = pd.concat([extrant, df])\n",
    "\n",
    "path = '/Users/camilledemers/Documents/04-filtrage' + '/' + acteur + '/'\n",
    "extrant.to_csv(path + acteur + '_KWIC' +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff167c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour la suite du traitement, on ne retient que les N expressions les plus fréquentes dans le corpus (ex. 5000)\n",
    "#terms = [t[0] for t in list(freq.most_common(10000))] --> ça ne fonctionne pas ensuite pour compter les nb de fois qu'un terme MeSH se trouve dans le corpus\n",
    "# OUI : voir cours classification LNG3120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45949eb3",
   "metadata": {},
   "source": [
    "### **Extraction de termes MeSH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/MeSH/mesh-fr.txt'\n",
    "\n",
    "with open (path, 'r', encoding='utf-8') as f:\n",
    "    mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "    tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "    mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "    mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "    #mesh = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_mesh = tokenizer_mesh.tokenize(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = []\n",
    "\n",
    "for t in extr_mesh:\n",
    "    if t in mesh:\n",
    "        termes_mesh.append(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = FreqDist(termes_mesh)\n",
    "tabCSV(termes_mesh, acteur + '_MeSH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685131ee",
   "metadata": {},
   "source": [
    "### **Extraction de termes SNOMED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/SNOMED/SNOMED_fr.csv'\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    sm = read_csv(f, sep=';')\n",
    "    sm = list(dict.fromkeys([str(t).strip().lower() for t in sm['term'].tolist()]))\n",
    "\n",
    "    sm = [tuple(tokenizer_re.tokenize(w)) for w in sm if len(w.split()) > 1]\n",
    "    tokenizer_sm = MWETokenizer(sm, separator = ' ')\n",
    "\n",
    "    sm = [tokenizer_sm.tokenize(w)[0].lower() for w in sm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_sm = tokenizer_sm.tokenize(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = []\n",
    "\n",
    "for t in extr_sm:\n",
    "    if t in sm:\n",
    "        termes_sm.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = FreqDist(termes_sm)\n",
    "tabCSV(termes_sm, acteur + '_SNOMED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e83245",
   "metadata": {},
   "source": [
    "### **POS Tagging**\n",
    "https://github.com/miotto/treetagger-python/blob/master/README.rst  \n",
    "https://treetaggerwrapper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a327eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = [tagger.tag_text(term) for term in terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d06dc",
   "metadata": {},
   "source": [
    "### **Lemmatisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "lemmatizer = FrenchLefffLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e714ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = []\n",
    "tuples_lemmes = []\n",
    "for terme in tagged:\n",
    "    exp = \" \".join([(t.split(\"\\t\")[0]) for t in terme])\n",
    "    lemme = \" \".join([lemmatizer.lemmatize(t) for t in terme])\n",
    "    chunk = \" \".join([(t.split(\"\\t\")[1]) for t in terme])\n",
    "    tuples.append([exp, chunk])\n",
    "    tuples_lemmes.append([lemme, chunk])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386aa5c",
   "metadata": {},
   "source": [
    "### **Filtrage - Patrons syntaxiques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec603aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/MeSH/mesh_patterns-fr.csv'\n",
    "\n",
    "with open (path, 'r') as f:\n",
    "    patterns = read_csv(f)\n",
    "    patterns = patterns['Structure'].tolist()[:50] # On prend les 50 structures syntaxiques les plus fréquentes dans les MeSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_lemmes = [t for t in tuples_lemmes if t[1] in patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [t for t in tuples if t[1] in patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f585f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/' + acteur + '/'\n",
    "tab = pd.DataFrame(tuples, columns= [\"Expression\", \"Structure syntaxique\"])\n",
    "tab = pd.DataFrame(tab.groupby([\"Expression\", \"Structure syntaxique\"]).size().reset_index(name=\"Fréquence\"))\n",
    "tab.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "tab.to_csv(path + acteur + '_phrases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b591b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/' + acteur + '/'\n",
    "tab = pd.DataFrame(tuples_lemmes, columns= [\"Expression\", \"Structure syntaxique\"])\n",
    "tab = pd.DataFrame(tab.groupby([\"Expression\", \"Structure syntaxique\"]).size().reset_index(name=\"Fréquence\"))\n",
    "tab.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "tab.to_csv(path + acteur + '_phrases_lemmatized.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a33210152f7d2bd255fb16656f372b633dbf298ed202bbbac20290b0375cadb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
