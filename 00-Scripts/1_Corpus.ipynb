{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e08799",
   "metadata": {},
   "source": [
    "# **1. Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce6a9d6",
   "metadata": {},
   "source": [
    "## Crawl URLs to extract all internal links "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f4c66",
   "metadata": {},
   "source": [
    "**XENU Link Sleuth**  \n",
    "https://home.snafu.de/tilman/xenulink.html \n",
    "\n",
    "*Le logiciel XENU Link Sleuth a été retenu pour cette tâche* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f80c7f",
   "metadata": {},
   "source": [
    "## Scrape textual data from crawled URLs\n",
    "**BeautifulSoup HTML Parser**  \n",
    "Réf : https://realpython.com/python-web-scraping-practical-introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "732fbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pandas import *\n",
    "\n",
    "corpus_global = []\n",
    "base_path = '../01-corpus/1-crawler/'\n",
    "for file in os.listdir(base_path):\n",
    "    if file.endswith('.csv'):\n",
    "        with open(base_path + file, encoding = 'utf-8',) as f:\n",
    "            corpus_global.append({'acteur': file[:-4]}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef638c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, ssl, os, sys, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "#from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "def getTextHTML(url):    \n",
    "    html = requests.get(url, headers = {'User-Agent': 'My User Agent 1.0'}, verify=False)\n",
    "    html.encoding = 'utf-8'\n",
    "    html = html.text\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Balises non pertinentes\n",
    "    tags_to_remove = ['head', 'header', 'script', 'footer', 'nav']#, 'form'] # Enlever 'form' pour le site du CHU Ste Justine\n",
    "\n",
    "    # Classes CSS spécifiques aux différents sites\n",
    "    attr_to_remove = ['div[class=\"contenu-fluide piv\"]', 'div[role=\"navigation\"]', 'div[class=\"section__wrapper section__wrapper--padding tac grid--inline-block\"]', 'div[id=\"slidebox\"]',\n",
    "    'div[class=\"col-md-12 mise-a-jour\"]', 'h1[class=\"sr-only\"]', 'a[class=\"cd-top js-cd-top\"]', 'div[class=\"nocontent\"]', 'p[class=\"footer-lien-resonances\"]', 'p[class=\"suivre\"]',\n",
    "    'section[class=\"field field-name-field-date-de-mise-jour field-type-datetime field-label-inline clearfix view-mode-full\"]', 'a[class=\"active\"]', 'p[class=\"footer-resonances\"]',\n",
    "    'div[class=\"item-list item-list-pager\"]', 'ul[class=\"pub-solr-sub-menu\"]', 'a[href=\"#main-content\"]', 'div[id=\"block-sociauxcrchum\"]', 'div[class=\"visually-hidden\"]',\n",
    "    'div[id=\"Breadcrumb\"]', 'div[id=\"pageInfo\"]', 'div[id=\"breadcrumb\"]', 'div[class=\"pagesCreation\"]', 'a[href=\"#contenu\"]', 'div[class=\"bandeau\"]',\n",
    "    'div[id=\"seeAlso\"]', 'a[href=\"/nous-ecrire.aspx\"]', 'li[class=\"CMSListMenuLI\"]', 'li[class=\"CMSListMenuLI navFirst\"]', 'li[class=\"CMSListMenuLI navLast\"]',\n",
    "    'div[class=\"alert alert-danger\"]', 'span[class=\"alertoverflow\"]', 'div[class=\"alert alert-warning alert-dismissible\"]', 'ul[class=\"menu\"]', 'div[id=\"letters-filter\"]',\n",
    "    'ul[class=\"pager\"]', 'a[href=\"#main-menu\"]', 'ul[class=\"custom_menu\"]', 'h2[class=\"element-invisible\"]', 'div[class=\"breadcrumb\"]',\n",
    "    'a[class=\"all-cta\"]', 'div[class=\"sub-menu-inner container\"]', 'div[class=\"fixed-dk-nav\"]', 'div[class=\"fixed-dk-nav-container\"]', 'div[class=\"container-inner\"]',\n",
    "    'div[class=\"socials\"]', 'div[class=\"breadcrumbs\"]', 'a[class=\"btn-print\"]', 'ul[class=\"list-buttons\"]', 'p[class=\"visually-hidden\"]', \n",
    "    'a[class=\"back-to-top\"]', 'a[class=\"sr-only sr-only-focusable\"]', 'ol[class=\"breadcrumb\"]', 'div[class=\"container-fluid piv_bas\"]', 'div[class=\"col-12 formBasPage\"]',\n",
    "    'div[class=\"container-fluid rangee-footer\"]', 'a[class=\"visuallyHidden passerContenu\"]', 'div[id=\"bandeau-alerte\"]', 'div[class=\"menu-sec-wrapper col-12 col-lg-12\"]',\n",
    "    'a[href=\"#layout-content\"]', 'div[class=\"paragraph feedback\"]', 'p[class=\"last-update\"]', 'ul[class=\"footer__menu--list\"]', 'div[class=\"footer__info\"]',\n",
    "    'section[class=\"hello-bar\"]', 'section[class=\"breadcrumb\"]', 'div[class=\"menu-page\"]', 'div[class=\"no-print menu-non-voyant\"]', 'div[class=\"navigation\"]',\n",
    "    'div[class=\"pure-bloc pure-u-1 pure-u-md-1-3 pure-u-lg-1-4 side-menu\"]', 'div[class=\"pied\"]', 'div[class=\"social\"]', 'div[class=\"piv-bas\"]',\n",
    "    'div[class=\"partage\"]', 'div[class=\"pied-print no-screen\"]', 'div[class=\"carte dynamic-carte-interactive-display ui-carte-panel\"]',\n",
    "    'div[id=\"piv\"]', 'ul[class=\"social-nav top-bar-social\"]', 'div[class=\"sidebar\"]', 'ul[class=\"side-menu\"]', 'div[class=\"zoom-button-wrapper\"]',\n",
    "    'a[href=\"#maincontent\"]', 'a[href=\"#content\"]', 'p[id=\"breadcrumbs\"]', 'div[class=\"mega-menu-wrap\"]', 'div[class=\"menu_2\"]', 'div[class=\"welcome\"]',\n",
    "    'div[class=\"header_two\"]', 'div[class=\"footer\"]', 'div[class=\"footer-wrapper\"]', 'div[class=\"custom-accessibility-tools js-only\"]', 'section[role=\"navigation\"]',\n",
    "    'div[class=\"container-fluid container-blue container-dl-menu\"]', 'div[class=\"col-xs-12 dl-menuwrapper menu-mobile visible-x\"]', 'div[class=\"entete-print no-screen\"]']\n",
    "    \n",
    "    for t in tags_to_remove:\n",
    "        tags = soup.find_all(t)\n",
    "        for tag in tags:\n",
    "            tag.decompose()\n",
    "\n",
    "    for t in attr_to_remove:\n",
    "        attr = soup.select(t)\n",
    "        for a in attr:\n",
    "            a.decompose()\n",
    "\n",
    "\n",
    "    data = soup.get_text(separator=' # ').replace(\"\\n\", \" \").replace(\"\\r\", \" \") \n",
    "    data = re.compile(r\"\\s+\").sub(\" \", data).strip()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad239e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def getTextPDF(url):\n",
    "    pdf_link = requests.get(url, headers = {'User-Agent': 'My User Agent 1.0'}, verify=False)\n",
    "    with io.BytesIO(pdf_link.content) as f:\n",
    "        reader = PdfReader(f)\n",
    "        nb_pages = len(reader.pages)\n",
    "        text = ''\n",
    "        for i in range(nb_pages):\n",
    "            page = reader.pages[i]\n",
    "            text += page.extract_text().lower().replace('\\n', '')\n",
    "        if text :\n",
    "            return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4afaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_list(df):\n",
    "    output = []\n",
    "    pdfs = df[df['Type'] == 'application/pdf']['Address'].tolist()\n",
    "    htmls = df[df['Type'] == 'text/html']['Address'].tolist() \n",
    "    # Il y a aussi des pages pour lesquelles le crawler n'a pas été en mesure de consigner une valeur\n",
    "    # On va essayer d'aller chercher leur contenu avec la fonction getTextHTML \n",
    "    nans = df[df[\"Type\"].isnull()]['Address'].tolist()  \n",
    "    htmls = htmls + nans\n",
    "\n",
    "    for url in htmls :\n",
    "        try: \n",
    "            text = getTextHTML(url)\n",
    "            if not '���' in text:\n",
    "                output.append({'Address': url, 'type': 'text/html', 'text':text.encode('utf-8', errors= 'replace').decode('utf-8')})\n",
    "        except Exception as e:\n",
    "            print(\"ERROR \" + \" - \" + url)\n",
    "            print(e)\n",
    "\n",
    "    for url in pdfs :\n",
    "        try: \n",
    "            text = getTextPDF(url)\n",
    "            if not '���' in text:\n",
    "                output.append({'Address': url, 'type': 'application/pdf', 'text':text.encode('utf-8', errors= 'replace').decode('utf-8')})\n",
    "        except Exception as e:\n",
    "            print(\"ERROR \" + \" - \" + url)\n",
    "            print(e)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14233647",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../01-corpus/1-crawler/'\n",
    "\n",
    "\n",
    "regex = '.*png.*|.*jpeg.*|.*jpg.*|.*docx.*|.*js.*|.*font.*|.*gif.*|.*formulaire.*|.*?f%5B0%5D.*|.*img.*|.*%5Bfilter%.*|.*css.*|.*scripts.*|.*zip.*|.*xlsx.*|.*cms.*|.*/images/.*|.*sondage.*|.*/depenses/.*|.*demandes-acces.*'\n",
    "\n",
    "# La liste des URLs à scrapper pour chaque corpus est contenue dans un fichier CSV. \n",
    "#On commence donc par lire le CSV pour extraire nos URLS.\n",
    "for x in range(len(corpus_global)) : \n",
    "    acteur = corpus_global[x]['acteur']\n",
    "    if acteur == 'ciusss_mcq':\n",
    "        # if not pdfs:\n",
    "        #   regex += '|.*pdf.*'\n",
    "\n",
    "        # encoding= 'ISO-8859-1' \"utf-8\"\n",
    "        with open(path + acteur + '.csv', encoding='UTF-8') as f:\n",
    "            csv = read_csv(f, sep=';')\n",
    "            # Nettoyer ce qui ne devrait pas se trouver là\n",
    "            csv = csv[~csv[\"Address\"].str.contains(regex, case=False)]\n",
    "\n",
    "        fr = csv[~csv[\"Address\"].str.contains('/en/')][['Address', 'Type']] # Données en français\n",
    "        en = csv[csv[\"Address\"].str.contains('/en/')][['Address', 'Type']] # Données en anglais\n",
    "        print(\"On va tenter d'aspirer {} pages Web\".format(len(fr) + len(en)))\n",
    "\n",
    "        sites_fr = scrape_list(fr)\n",
    "        if(len(en) > 0):\n",
    "            sites_en = scrape_list(en)\n",
    "\n",
    "        output_path = '../03-corpus/2-data/1-fr/'\n",
    "\n",
    "        sites_fr = pd.DataFrame(sites_fr)\n",
    "        sites_fr = csv.merge(sites_fr, how='right', on='Address')\n",
    "        sites_fr = sites_fr[['Address', 'Title', 'Type', 'text']]\n",
    "\n",
    "        sites_fr.to_csv(output_path + acteur + '.csv', sep=';', escapechar='/')\n",
    "        corpus_global[x]['N_fr'] = len(sites_fr)\n",
    "        if(len(en) > 0):\n",
    "            output_path = '../03-corpus/2-data/1-en/'\n",
    "            sites_en = pd.DataFrame(sites_en)\n",
    "            sites_en = csv.merge(sites_en, how='right', on='Address')\n",
    "            sites_en = sites_en[['Address', 'Title', 'Type', 'text']]\n",
    "            sites_en.to_csv(output_path + acteur + '_en.csv', sep=';', escapechar='/')\n",
    "            corpus_global[x]['N_en'] = len(sites_en)\n",
    "\n",
    "        print('On a fini avec {}'.format(acteur))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79bb76bbc4f9ba1f8df5efe8db67aae07079a51dc7b5004f49990e90f5993a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
