{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Pondération statistique** (TF-IDF / OKapiBM25)  \n",
    "\n",
    "- https://stackoverflow.com/questions/46580932/calculate-tf-idf-using-sklearn-for-n-grams-in-python    \n",
    "- http://scikit-learn.sourceforge.net/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer  \n",
    "- https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n",
    "- https://pypi.org/project/rank-bm25/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, path\n",
    "from pandas import *\n",
    "import os, shutil, re\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "regex = \"[\\w+-]+|\\([\\s+\\w+\\d+-]+\\)|\\w+|\\w\"\n",
    "tokenizex = RegexpTokenizer(regex)\n",
    "\n",
    "corpus_global = []\n",
    "base_path = '../02-filtrage/output/'\n",
    "for file in listdir(base_path):\n",
    "    if file.endswith('.csv'):\n",
    "        with open(base_path + file, encoding = 'utf-8',) as f:\n",
    "            corpus_global.append({'acteur': file[:-20], 'N_fr': 0, 'N_MeSH' : 0, 'N_Taxo' : 0}) \n",
    "            \n",
    "N_termes = []\n",
    "corpus_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(corpus_global)) :  \n",
    "    acteur = corpus_global[x]['acteur']\n",
    "\n",
    "    base_path = '../02-filtrage/output/'\n",
    "\n",
    "    with open(base_path+acteur+'_candidate-terms.csv', encoding='utf-8') as f:\n",
    "        csv = read_csv(f).drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "    corpus_global[x]['N_MeSH'] = len(csv[csv['isMeSHTerm'] == True])\n",
    "    corpus_global[x]['N_Taxo'] = len(csv[csv['isTaxoTerm'] == True])\n",
    "\n",
    "    # On va remplacer les apostrophes par des espaces ; sklearn ne les aime pas \n",
    "    csv['Terme formatté'] = csv[\"Terme\"].apply(lambda x: x.replace(\"'\", ' '))\n",
    "    vocabulaire = [t.lower() for t in list(csv['Terme formatté'])]\n",
    "    print(acteur, 'On a un vocabulaire de {} formes.'.format(len(vocabulaire)))\n",
    "    N_termes += vocabulaire\n",
    "\n",
    "    # Lire le corpus\n",
    "    base_path = '../01-corpus/2-data/1-fr/'\n",
    "    file_path = path.join(base_path, acteur + '.csv')\n",
    "        \n",
    "    with open(file_path, \"r\", encoding = \"UTF-8\") as f:\n",
    "        data = read_csv(file_path, sep=',')\n",
    "        text = data['text'].dropna().tolist()\n",
    "        nb_docs = len(text)\n",
    "\n",
    "    corpus_global[x]['N_fr'] = nb_docs\n",
    "    print(\"On a un corpus de {} documents.\".format(nb_docs))\n",
    "\n",
    "    # Nettoyage\n",
    "    corpus = [str(t).strip('\\n').lower().replace('’', '\\'') for t in text]\n",
    "    spaces = '\\s+'\n",
    "\n",
    "    corpus = [re.sub(spaces, ' ', t) for t in corpus]\n",
    "    corpus = [t.replace(\"  \", \" \" ) for t in corpus]\n",
    "\n",
    "\n",
    "\n",
    "    #On va commencer par utiliser le CountVectorizer pour valider que l'implémentation de sklearn arrive bien au même compte que nous \n",
    "    vectorizer = CountVectorizer(tokenizer = tokenizex.tokenize, vocabulary=vocabulaire, ngram_range=(1,10), token_pattern=regex)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    vectorizer.get_feature_names_out()\n",
    "\n",
    "    features_names = vectorizer.get_feature_names_out()\n",
    "    corpus_index = [corpus.index(n) for n in corpus]\n",
    "\n",
    "\n",
    "    df = DataFrame(X.T.todense(), index=features_names, columns=corpus_index).transpose()\n",
    "\n",
    "    csv['TF (sklearn)'] = 0\n",
    "    for t in vocabulaire:\n",
    "        csv.loc[csv['Terme formatté'] == t, 'TF (sklearn)'] = df[t].sum()\n",
    "\n",
    "\n",
    "    csv['DF (sklearn)'] = 0\n",
    "    for t in vocabulaire:\n",
    "        freqdoc = len(df[df[t] != 0])\n",
    "        csv.loc[csv['Terme formatté'] == t, 'DF (sklearn)'] = freqdoc\n",
    "    csv\n",
    "\n",
    "    # Calcul TF-IDF\n",
    "    # vocabulary = vocabulaire\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer = tokenizex.tokenize, vocabulary=vocabulaire, ngram_range=(1,10), lowercase=False, token_pattern=regex)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    features_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    corpus_index = [corpus.index(n) for n in corpus]\n",
    "\n",
    "    df = DataFrame(tfidf.T.todense(), index=features_names, columns=corpus_index).transpose()\n",
    "    terms_tfidf = {term: df[term].max() for term in df}\n",
    "    zeros = {term : value for term, value in terms_tfidf.items() if terms_tfidf[term] == 0}\n",
    "    print('TF-IDF = 0',zeros)\n",
    "    print('% TF-IDF = 0', (len(zeros) / len(vocabulaire) *100))\n",
    "    \n",
    "    terms_weighted = DataFrame(terms_tfidf.items(), columns=['Terme formatté', 'TF-IDF'])\n",
    "    terms_weighted.sort_values([\"TF-IDF\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "\n",
    "    terms_weighted = terms_weighted.drop_duplicates(keep='first')\n",
    "\n",
    "    terms_weighted = merge(csv, terms_weighted, on='Terme formatté').drop_duplicates(\n",
    "    subset = ['Terme', 'Fréquence (TF)'],\n",
    "    keep = 'first').reset_index(drop = True)\n",
    "\n",
    "\n",
    "    # Calcul OKapi BM25\n",
    "    tokenizer = RegexpTokenizer(\"(\\w+\\'|\\w+-\\w+|\\(|\\)|\\w+)\")\n",
    "    bm25 = BM25Okapi([tokenizer.tokenize(t) for t in corpus])\n",
    "\n",
    "    vocabulaire = csv[\"Terme\"].dropna().tolist()\n",
    "    tokenized_queries = [tokenizer.tokenize(str(t)) for t in vocabulaire]\n",
    "\n",
    "    features_names = [t for t in set(vocabulaire)]\n",
    "    corpus_index = [corpus.index(n) for n in corpus]\n",
    "\n",
    "    tab = [bm25.get_scores(query) for query in tokenized_queries]\n",
    "    df = DataFrame(tab, index=features_names, columns=corpus_index).transpose()\n",
    "\n",
    "    terms_okapi = {term: df[term].max() for term in df}\n",
    "    tab = DataFrame(terms_okapi.items(), columns=['Terme', 'OkapiBM25'])\n",
    "    tab.sort_values([\"OkapiBM25\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "\n",
    "    tab = merge(terms_weighted, tab, on=\"Terme\")\n",
    "    tab.sort_values([\"OkapiBM25\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "    base_path = '../03-transformation/'\n",
    "    file_path = base_path + acteur  + '_weighting_OKapiBM25.csv'\n",
    "    tab.to_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79bb76bbc4f9ba1f8df5efe8db67aae07079a51dc7b5004f49990e90f5993a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
