{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c662c7d",
   "metadata": {},
   "source": [
    "## **2. Prétraitement**\n",
    "- Segmentation (phrases)\n",
    "- Tokenization (mots)\n",
    "- Étiquetage morphosyntaxique (POS Tagging) \n",
    "- (Lemmatisation - *sur la glace ; ça pourrait permettre d'éviter les faux négatifs dûs à des variations singulier/pluriel quand on essaie d'extraire les termes qui existent déjà dans la taxonomie*)\n",
    "- Filtrage (stopwords)\n",
    "- Extraction de termes complexes (MWE / n-grammes / segments répétés)\n",
    "- Chunking / Filtrage par patrons syntaxiques (basés sur les patrons fréquents dans les MeSH)\n",
    "- Extraction de collocations significatives (en fonction du Log-likelihood ratio)\n",
    "- Extraction de concordances (KWIC) pour un ensemble de mots-clés d'intérêt\n",
    "- Extraction de termes MeSH présents dans les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e374a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "import shutil, re, random\n",
    "from os import listdir, chdir, path\n",
    "from pathlib import Path\n",
    "from pandas import *\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "#nltk.download(['popular'])\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w\\'|\\w+\")\n",
    "from nltk import bigrams, trigrams, ngrams, everygrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ab2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pandas as pd\n",
    "base_path = '../04-filtrage/MeSH/'\n",
    "\n",
    "tree = ET.parse(base_path + 'fredesc2019.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "data_mesh = [{'mesh_id' : x.find('DescriptorUI').text.strip('\\n'), \\\n",
    "         'label_fr' : x.find('DescriptorName').find('String').text.split('[')[0], \\\n",
    "         'label_en' : x.find('DescriptorName').find('String').text.split('[')[1].strip(']'), \\\n",
    "         'synonymes (en/fr)' : flatten([[term.find('String').text for term in concept.find('TermList').findall('Term')] for concept in x.find('ConceptList').findall('Concept')]) \\\n",
    "         } for x in root.findall('DescriptorRecord')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86775448",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_global = []\n",
    "base_path = '../03-corpus/1-crawler/'\n",
    "for file in listdir(base_path):\n",
    "    if file.endswith('.csv'):\n",
    "        with open(base_path + file, encoding = 'utf-8',) as f:\n",
    "            corpus_global.append({'acteur': file[:-4]}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e583f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciusss_nordmtl\n",
      "On a un corpus de 1538 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 644048 tokens.\n",
      "Le POS tagging devrait prendre environ 1 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 4764921 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 127444 occurrences de ngrammes.\n",
      "Et 11470 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 29 % des termes\n",
      "On avait 127444 ngrammes, on en a maintenant 90256.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 49 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : ciusss_nordmtl\n",
      "ciusss_ouestmtl\n",
      "On a un corpus de 235 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 88656 tokens.\n",
      "Le POS tagging devrait prendre environ 0 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 640500 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 6716 occurrences de ngrammes.\n",
      "Et 755 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 21 % des termes\n",
      "On avait 6716 ngrammes, on en a maintenant 5294.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 0 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : ciusss_ouestmtl\n",
      "ciusss_saglac\n",
      "On a un corpus de 853 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 228105 tokens.\n",
      "Le POS tagging devrait prendre environ 0 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 1665132 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 24942 occurrences de ngrammes.\n",
      "Et 2404 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 25 % des termes\n",
      "On avait 24942 ngrammes, on en a maintenant 18738.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 0 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : ciusss_saglac\n",
      "cusm\n",
      "On a un corpus de 597 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 290161 tokens.\n",
      "Le POS tagging devrait prendre environ 0 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 2098495 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 26937 occurrences de ngrammes.\n",
      "Et 2478 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 18 % des termes\n",
      "On avait 26937 ngrammes, on en a maintenant 22032.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 0 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : cusm\n",
      "icm\n",
      "On a un corpus de 950 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 435687 tokens.\n",
      "Le POS tagging devrait prendre environ 0 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 3231550 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 85923 occurrences de ngrammes.\n",
      "Et 8163 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 31 % des termes\n",
      "On avait 85923 ngrammes, on en a maintenant 59249.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 33 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : icm\n",
      "inesss\n",
      "On a un corpus de 2871 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 1537917 tokens.\n",
      "Le POS tagging devrait prendre environ 2 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 11504416 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 660400 occurrences de ngrammes.\n",
      "Et 23864 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 27 % des termes\n",
      "On avait 660400 ngrammes, on en a maintenant 482221.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 279 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : inesss\n",
      "inspq\n",
      "On a un corpus de 8573 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 9489464 tokens.\n",
      "Le POS tagging devrait prendre environ 11 minutes.\n",
      "POS : Done\n",
      "iucpq\n",
      "On a un corpus de 1369 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 719821 tokens.\n",
      "Le POS tagging devrait prendre environ 1 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 5296522 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 185764 occurrences de ngrammes.\n",
      "Et 12897 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 34 % des termes\n",
      "On avait 185764 ngrammes, on en a maintenant 123018.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 36 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : iucpq\n",
      "laval_sante\n",
      "On a un corpus de 1087 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 582355 tokens.\n",
      "Le POS tagging devrait prendre environ 1 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 4370037 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 65882 occurrences de ngrammes.\n",
      "Et 5484 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 27 % des termes\n",
      "On avait 65882 ngrammes, on en a maintenant 48189.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 17 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : laval_sante\n",
      "msss\n",
      "On a un corpus de 9136 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 6418634 tokens.\n",
      "Le POS tagging devrait prendre environ 7 minutes.\n",
      "POS : Done\n",
      "pinel\n",
      "On a un corpus de 61 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 37680 tokens.\n",
      "Le POS tagging devrait prendre environ 0 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 272468 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 7015 occurrences de ngrammes.\n",
      "Et 363 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 37 % des termes\n",
      "On avait 7015 ngrammes, on en a maintenant 4400.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 0 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : pinel\n",
      "quebec_sante\n",
      "On a un corpus de 1686 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 1267628 tokens.\n",
      "Le POS tagging devrait prendre environ 1 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 9275903 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 227222 occurrences de ngrammes.\n",
      "Et 17287 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 27 % des termes\n",
      "On avait 227222 ngrammes, on en a maintenant 165148.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 158 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : quebec_sante\n",
      "ramq\n",
      "santeestrie\n",
      "On a un corpus de 652 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 464209 tokens.\n",
      "Le POS tagging devrait prendre environ 1 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 3375904 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 161574 occurrences de ngrammes.\n",
      "Et 12914 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 34 % des termes\n",
      "On avait 161574 ngrammes, on en a maintenant 106824.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 172 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : santeestrie\n",
      "sante_mtl\n",
      "On a un corpus de 3935 documents.\n",
      "Avec le RegExpTokenizer, notre corpus contient 1935686 tokens.\n",
      "Le POS tagging devrait prendre environ 2 minutes.\n",
      "POS : Done\n",
      "Avant filtrage, on a 13969732 ngrammes.\n",
      "Extraction de termes complexes : Done\n",
      "Filtrage des mots-vides : Done\n",
      "Filtrage statistique : Done\n",
      "Après filtrage, on a 590994 occurrences de ngrammes.\n",
      "Et 27823 ngrammes uniques.\n",
      "Le filtrage syntaxique élimine environ 31 % des termes\n",
      "On avait 590994 ngrammes, on en a maintenant 408136.\n",
      "Après avoir calculé le log-likelihood ratio, on a retiré 1154 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 0 % de nos n-grammes.\n",
      "Filtrage par fréquence documentaire: Done\n",
      "Extraction de termes MeSH : Done\n",
      "Extraction de termes de la taxo : Done\n",
      "Mapping MeSH IDs : Done\n",
      "On a terminé avec cet acteur : sante_mtl\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(corpus_global)) : \n",
    "    acteur = corpus_global[x]['acteur']\n",
    "    print(acteur)\n",
    "\n",
    "    lng = 'fr'\n",
    "\n",
    "    if lng == 'fr':\n",
    "        file = acteur +'.csv'\n",
    "    if lng == 'en':\n",
    "        file = acteur + '_en.csv'\n",
    "\n",
    "    def lire_corpus(acteur = acteur, langue=lng):\n",
    "        base_path = '../03-corpus/2-data/'\n",
    "        \n",
    "        folder_path = path.join(base_path, '1-' + langue, acteur)\n",
    "        all_files = glob.glob(path.join(folder_path, \"*.csv\"))\n",
    "        tags = [f.split('_')[1][:-4] for f in listdir(folder_path)]\n",
    "\n",
    "        df = DataFrame()\n",
    "        for f, tag in zip(all_files, tags):\n",
    "            csv = read_csv(f, encoding='utf-8', sep=',')\n",
    "            csv = csv[~csv[\"Address\"].str.contains('pdf')] #  Problèmes \n",
    "            #csv = csv['text']\n",
    "            # Using DataFrame.insert() to add a column\n",
    "            df = concat([df, csv]) [['Corpus', 'Sous-corpus', 'Address', 'Title', 'Type', 'text']]\n",
    "            #df = concat([df, csv]) [['Corpus', 'Sous-corpus', 'Title', 'Type', 'text']]\n",
    "        return df\n",
    "\n",
    "    data = lire_corpus(acteur) \n",
    "    nb_docs = len(data)\n",
    "    print(\"On a un corpus de {} documents.\".format(nb_docs))\n",
    "    corpus_global[x]['N_fr'] = nb_docs\n",
    "\n",
    "    base_path = '../03-corpus/2-data/'\n",
    "    folder_path = path.join(base_path, '1-' + lng, acteur)\n",
    "\n",
    "    data.to_csv(folder_path + '.csv')\n",
    "    data\n",
    "\n",
    "    #Nettoyage\n",
    "    punct = '[!#$%&•►*+,;\\/\\\\<=>?@[\\]^_{|}~©«»—“”–—]'\n",
    "    spaces = '\\s+'\n",
    "    postals = '([a-zA-Z]+\\d+|\\d+[a-zA-Z]+)+'\n",
    "    # phones = '\\d{3}\\s\\d{3}-\\d{4}' #très simple (trop)\n",
    "\n",
    "    text = [str(t).strip('\\n').lower().replace('’', '\\'') for t in data['text'].tolist()]\n",
    "    text = [re.sub(spaces, ' ', t) for t in text]\n",
    "    text = [re.sub(postals, ' STOP ', t) for t in text]\n",
    "    text = [re.sub(punct, ' STOP ', t) for t in text]\n",
    "    text = [t.replace(\"  \", \" \" ) for t in text]\n",
    "\n",
    "    def join_corpus(corpus):\n",
    "        return \" \".join(corpus)\n",
    "    \n",
    "    corpus = join_corpus(text)\n",
    "\n",
    "    # Filtre d'expressions complexes\n",
    "    def filter_mwesw(corpus):\n",
    "        file_mwesw = '../04-filtrage/mwe_stopwords.txt'\n",
    "        with open (file_mwesw, 'r', encoding='utf-8') as f:\n",
    "            mwe_sw = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "        for mwe in mwe_sw:\n",
    "            corpus = corpus.replace(mwe, ' STOP ').replace('  ', \" \")\n",
    "        return corpus\n",
    "\n",
    "    corpus = filter_mwesw(corpus)\n",
    "\n",
    "    # Ici, on tokenise une première fois avec le Regex Tokenizer de NLTK pour voir combien de temps ça devrait \n",
    "    # prendre au Tree Tagger pour tokeniser et tagger notre corpus\n",
    "    def tok(corpus):\n",
    "        # Seulement les caractères alphabétiques\n",
    "        tokens = tokenizer_re.tokenize(corpus)\n",
    "        print(\"Avec le RegExpTokenizer, notre corpus contient {} tokens.\".format(len(tokens)))\n",
    "        temps = round(len(tokens) / 15000 / 60)\n",
    "        print('Le POS tagging devrait prendre environ {} minutes.'.format(temps))\n",
    "\n",
    "    tok(corpus)\n",
    "\n",
    "    # Tokenisation / POS tagging\n",
    "    def tagging(corpus):\n",
    "        output = []\n",
    "        for t in tagger.tag_text(corpus):\n",
    "            try: \n",
    "                output.append([t.split('\\t')[0], t.split('\\t')[1]])\n",
    "            except Exception as e:\n",
    "                output.append(('STOP', 'NAM'))\n",
    "        return output\n",
    "\n",
    "    tagged = tagging(corpus)\n",
    "    tokens = [t[0] for t in tagged]\n",
    "    print('POS : Done')\n",
    "\n",
    "    def extr_ngrams(tagged):\n",
    "        ngrammes= list(everygrams(tagged, min_len=2, max_len=8))\n",
    "        print(\"Avant filtrage, on a {} ngrammes.\".format(len(ngrammes)))\n",
    "        return ngrammes\n",
    "\n",
    "    ngrammes = extr_ngrams(tagged)\n",
    "    print('Extraction de termes complexes : Done')\n",
    "\n",
    "    def extract_patterns(ngrammes):\n",
    "        patterns = []\n",
    "        for ng in ngrammes:\n",
    "            phrase = tuple([t[0] for t in ng])\n",
    "            pattern = [t[1] for t in ng]\n",
    "            patterns.append([phrase, pattern])\n",
    "        return patterns\n",
    "\n",
    "    phrases = extract_patterns(ngrammes)\n",
    "    frequencies = FreqDist(everygrams(tokens, min_len=1, max_len=10))\n",
    "\n",
    "    frequencies\n",
    "\n",
    "    #Filtrage - mots vides\n",
    "    # Stopwords fréquents en français \n",
    "    file_path = \"../04-filtrage/stopwords.txt\"\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "    # Stopwords fréquents en anglais\n",
    "    file_path = '../04-filtrage/stop_words_english.txt'\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords += [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "    def filtrer_stopwords(x): # On s'assure aussi que le premier terme du ngramme est un NOM pour avoir des syntagmes nominaux\n",
    "        if lng == 'en':\n",
    "            nom = 'NN'\n",
    "        if lng == 'fr':\n",
    "            nom = 'NOM'\n",
    "        return [term for term in x if not 'STOP' in term[0] and not term[0][0] in stopwords and not term[0][-1] in stopwords \\\n",
    "            and not 'NUM' in term[1] and term[1][0] == nom and not '.' in term[0] and not '-' in term[0] and not ':' in term[0]\\\n",
    "            # Une parenthèse fermante peut juste se trouver comme dernier token\n",
    "            # Si une parenthèse est ouverte, elle doit aussi être fermée (et vice versa)\n",
    "            and not ')' in term[0][:-1] and not ('(' in term[0] and not ')' in term[0]) \\\n",
    "            and not (')' in term[0] and not '(' in term[0])]\n",
    "\n",
    "    phrases = filtrer_stopwords(phrases)\n",
    "    print('Filtrage des mots-vides : Done')\n",
    "\n",
    "    # Filtrage - longueur \n",
    "    def filter_len(x):\n",
    "        return [term for term in x if \\\n",
    "            (len(term[0][0]) > 2 or term[0][0] == '(')  and (len(term[0][-1]) > 2 or term[0][-1] == ')') and \\\n",
    "            len(term[0][0]) < 18 and len(term[0][-1]) < 18]\n",
    "\n",
    "    phrases = filter_len(phrases)\n",
    "\n",
    "    # Filtrage statistique\n",
    "    def filter_freq(x):\n",
    "        return [term for term in x if frequencies[tuple(term[0])] > 5]\n",
    "\n",
    "    phrases = filter_freq(phrases)\n",
    "    print('Filtrage statistique : Done')\n",
    "    phrases = [[term[0], \" \".join(term[1])] for term in phrases]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        phrase.append(frequencies[tuple(phrase[0])])\n",
    "\n",
    "    print(\"Après filtrage, on a {} occurrences de ngrammes.\".format(len(phrases))) \n",
    "    print(\"Et {} ngrammes uniques.\".format(len(DataFrame(phrases).drop_duplicates())))\n",
    "\n",
    "    # Filtrage - patrons syntaxiques\n",
    "    file_patterns = '../04-filtrage/MeSH/mesh_patterns-fr.csv'\n",
    "\n",
    "    with open (file_patterns, 'r') as f:\n",
    "        patterns = read_csv(f)\n",
    "        patterns = patterns['Structure'].tolist() # Pour prendre les structures syntaxiques attestées dans les MeSH\n",
    "\n",
    "    def filter_patterns(phrases):\n",
    "        return [t for t in phrases if t[1] in patterns and not 'NUM' in t[1]] # and not 'NOM NOM' in t[1]\n",
    "\n",
    "    terms = filter_patterns(phrases)\n",
    "\n",
    "    print(\"Le filtrage syntaxique élimine environ {} % des termes\".format(round((len(phrases) - len(terms)) / len(phrases) * 100)))\n",
    "    print(\"On avait {} ngrammes, \".format(len(phrases)) + \"on en a maintenant {}.\".format(len(terms)))\n",
    "\n",
    "    for phrase in terms:\n",
    "        phrase[0] = tuple(phrase[0])\n",
    "\n",
    "\n",
    "    terms_patterns = DataFrame(terms, columns = [\"Expression\", \"Structure syntaxique\", \"Fréquence\"])\n",
    "    terms_patterns = terms_patterns.to_dict('records')\n",
    "    dict_patterns = {}\n",
    "    for term in terms_patterns:\n",
    "        exp = term['Expression']\n",
    "        pattern = term['Structure syntaxique']\n",
    "        dict_patterns[exp] = pattern\n",
    "\n",
    "    # Extraction de collocations significatives\n",
    "    def loglikelihood_ratio(c_prior, c_n, c_ngram, N):\n",
    "        \"\"\"\n",
    "        Compute the ratio of two hypotheses of likelihood and return the ratio.\n",
    "        The formula here and test verification values are taken from \n",
    "        Manning & Schūtze _Foundations of Statistical Natural Language Processing_ p.172-175\n",
    "        Parameters:\n",
    "        c_prior: count of word 1 if bigrams or count of [w1w2 .. w(n-1)] if ngram\n",
    "        c_n : count of word 2 if bigrams or count of wn if ngram\n",
    "        c12: count of bigram (w1, w2) if bigram or count of ngram if ngram\n",
    "        N: the number of words in the corpus\n",
    "        \"\"\"\n",
    "\n",
    "        p = c_n / N\n",
    "        p1 = c_ngram / c_prior\n",
    "        p2 = (c_n - c_ngram) / (N - c_prior)   \n",
    "        # We proactively trap a runtimeWarning: divide by zero encountered in log,\n",
    "        # which may occur with extreme collocations\n",
    "        import warnings\n",
    "        with warnings.catch_warnings(): # this will reset our filterwarnings setting\n",
    "            warnings.filterwarnings('error')\n",
    "            try:\n",
    "                return (np.log(binom.pmf(c_ngram, c_prior, p)) \n",
    "                        + np.log(binom.pmf(c_n - c_ngram, N - c_prior, p)) \n",
    "                        - np.log(binom.pmf(c_ngram, c_prior, p1) )\n",
    "                        - np.log(binom.pmf(c_n - c_ngram, N - c_prior, p2)))             \n",
    "            except Warning:\n",
    "                return np.inf \n",
    "\n",
    "    # Pour le calcul des probabilités, on a besoin de traiter séparément les ngrammes selon la valeur de n\n",
    "    N = len(tokens)\n",
    "    fd_tokens = nltk.FreqDist(tokens)\n",
    "\n",
    "    def llr_ngrammes(terms):\n",
    "        llr = []\n",
    "        ngrammes = [term[0] for term in terms]\n",
    "            \n",
    "        for t in ngrammes:\n",
    "            if len(t) == 1:\n",
    "                try:\n",
    "                    llr.append({'Terme' : str(t[0]), 'Structure syntaxique': dict_patterns[t], 'Fréquence (TF)' : fd_tokens[str(t[0])], 'LLR': '-', 'p-value': '-'})\n",
    "                except Exception as e:\n",
    "                    print(t, str(e))\n",
    "            else:\n",
    "                c_prior = frequencies[t[:-1]] # Antécédent = P(w1w2..w_n-1) (si on considère que P(w1w2...wn) = P(wn) | P(w1w2...w_n-1)\n",
    "                c_n = fd_tokens[t[-1]]     # Dernier mot du ngramme  P(wn)\n",
    "                c_ngram = frequencies[t]             # Le ngramme lui-même P(w1w2w3..wn)\n",
    "\n",
    "                try:\n",
    "                    res = -2 * loglikelihood_ratio(c_prior, c_n, c_ngram, N)\n",
    "                    p_value = chi2.sf(res, 1) # 1 degrees of freedom\n",
    "\n",
    "                    if p_value < 0.001 or (res == float('-inf')):\n",
    "                        #llr.append({'Collocation' : \" \".join(t).replace(\"' \", \"'\").replace(\"( \", \"(\").replace(\" )\", \")\"), 'Structure syntaxique': dict_patterns[\" \".join(t).replace(\"' \", \"'\")], 'Fréquence' : c_ngram, 'LLR': res, 'p-value': p_value})\n",
    "                        llr.append({'Terme' : t, 'Structure syntaxique': dict_patterns[t], 'Fréquence (TF)' : c_ngram, 'LLR': res, 'p-value': p_value})\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(t, str(e))\n",
    "                \n",
    "        return llr\n",
    "\n",
    "    significant_coll = llr_ngrammes(terms)\n",
    "\n",
    "    print('Après avoir calculé le log-likelihood ratio, on a retiré {} collocations qui n\\'étaient pas statistiquement significatives.'.format(len(terms) - len(significant_coll)))\n",
    "    print('Ça représente environ {} % de nos n-grammes.'.format(round((len(terms) - len(significant_coll)) / len(terms) *100 )))\n",
    "\n",
    "    df = DataFrame(significant_coll).sort_values(by = \"Fréquence (TF)\", ascending=False).drop_duplicates()\n",
    "\n",
    "    # On veut faire join pour tous les termes[collocation]\n",
    "    def join_term(x):\n",
    "        if type(x) == tuple:\n",
    "            return \" \".join(x).replace(\"' \", \"'\").replace(\"( \", \"(\").replace(\" )\", \")\")\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    df['Terme'] = df['Terme'].apply(lambda x: join_term(x))\n",
    "\n",
    "    if lng == 'en':\n",
    "        acteur = acteur + '_' + lng\n",
    "\n",
    "    output_path = path.join('../04-filtrage/output/', acteur + '_candidate-terms.csv') \n",
    "\n",
    "    df.to_csv(output_path)\n",
    "\n",
    "    df = df.drop(columns=['p-value'])\n",
    "\n",
    "    # Filtrage - fréquence documentaire\n",
    "    dfs = {term: len([doc for doc in text if term in doc]) for term in df['Terme'].tolist()}\n",
    "\n",
    "    max_df = round(0.98 * nb_docs)   # Pour rejeter les termes qui se retrouvent dans plus de 95% des documents \n",
    "    min_df = round(0.01 * nb_docs)   # Pour rejeter les termes qui se retrouvent dans moins de 1% des documents\n",
    "\n",
    "    dfs = {term:df for term,df in dfs.items() if df < max_df and df > min_df} # Si df = 0, c'est un artefact créé par le prétraitement lui-même\n",
    "    print('Filtrage par fréquence documentaire: Done')\n",
    "\n",
    "    dfs = DataFrame(list(dfs.items()),columns = ['Terme','Fréquence documentaire (DF)']) \n",
    "    df = df.merge(dfs, on='Terme').drop_duplicates()\n",
    "\n",
    "    df.sort_values(['Fréquence (TF)'], \n",
    "                axis=0,\n",
    "                ascending=[False], \n",
    "                inplace=True)\n",
    "\n",
    "    list_terms = df['Terme'].tolist()\n",
    "\n",
    "    # Extraction de termes MeSH\n",
    "    df['isMeSHTerm']= False # On set à False puis on va changer pour True si on trouve le terme\n",
    "    df['MeSHID'] = None\n",
    "    df['MesH_prefLabel_fr'] = None\n",
    "    df['MesH_prefLabel_en'] = None\n",
    "\n",
    "    from nltk.tokenize import MWETokenizer\n",
    "    file_path = '../04-filtrage/MeSH/mesh-fr.txt'\n",
    "\n",
    "    with open (file_path, 'r', encoding='utf-8') as f:\n",
    "        mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "        tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "        mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "        mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "        #mesh = [tuple(t.strip('.').lower().split()) for t in f.readlines()]\n",
    "\n",
    "    extr_mesh = tokenizer_mesh.tokenize(list_terms)\n",
    "\n",
    "    for t in extr_mesh:\n",
    "        if t in mesh:\n",
    "            df.loc[df['Terme'] == t, 'isMeSHTerm'] = True\n",
    "\n",
    "    # Termes MeSH présents dans notre corpus \n",
    "    corpus_global[x]['N_MeSH'] = len(df[df['isMeSHTerm'] == True])\n",
    "    print('Extraction de termes MeSH : Done')\n",
    "\n",
    "    # Extraction de termes existant dans la taxonomie\n",
    "    df['isTaxoTerm'] = 'False' # On set à False puis on va changer pour True si on trouve le terme\n",
    "\n",
    "    file_path = '../04-filtrage/default_taxo_labels.csv'\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        default = read_csv(f, sep=';')\n",
    "        taxo_terms = list(dict.fromkeys([str(t).strip().lower() for t in default['Label'].tolist()]))\n",
    "\n",
    "    for t in df['Terme'].tolist():\n",
    "        if t in taxo_terms:\n",
    "            df.loc[df['Terme'] == t, 'isTaxoTerm'] = True\n",
    "    \n",
    "    corpus_global[x]['N_Taxo'] = len(df[df['isTaxoTerm'] == True])\n",
    "    print('Extraction de termes de la taxo : Done')\n",
    "\n",
    "    for t in df[df['isMeSHTerm'] == True]['Terme'].tolist():\n",
    "        for d in data_mesh:\n",
    "            if t in [str(x).lower() for x in d['synonymes (en/fr)']]:\n",
    "                df.loc[df['Terme'] == t, 'MeSHID'] = d['mesh_id']\n",
    "                df.loc[df['Terme'] == t, 'MesH_prefLabel_fr'] = d['label_fr']\n",
    "                df.loc[df['Terme'] == t, 'MesH_prefLabel_en'] = d['label_en']\n",
    "    print('Mapping MeSH IDs : Done')\n",
    "\n",
    "    df = df[['Terme', 'Structure syntaxique',\t'Fréquence (TF)', 'Fréquence documentaire (DF)', 'LLR', 'isMeSHTerm', 'isTaxoTerm', 'MeSHID', 'MesH_prefLabel_fr', 'MesH_prefLabel_en']]\n",
    "\n",
    "    df.insert(0, 'Corpus', acteur)\n",
    "\n",
    "    if lng == 'en':\n",
    "        acteur = acteur + '_' + lng\n",
    "\n",
    "    output_path = path.join('../04-filtrage/output/', acteur + '_candidate-terms.csv') \n",
    "    df.to_csv(output_path)\n",
    "\n",
    "    print('On a terminé avec cet acteur : ' + acteur)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79bb76bbc4f9ba1f8df5efe8db67aae07079a51dc7b5004f49990e90f5993a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
