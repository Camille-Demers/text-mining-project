{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c662c7d",
   "metadata": {},
   "source": [
    "## **2. Prétraitement**\n",
    "- Segmentation (phrases)\n",
    "- Tokenization (mots)\n",
    "- Filtrage (stopwords)\n",
    "- Extraction de termes complexes (MWE / n-grammes / segments répétés)\n",
    "- Étiquetage morphosyntaxique (POS Tagging) \n",
    "- Chunking / Filtrage par patrons syntaxiques (basés sur les patrons fréquents dans les MeSH)\n",
    "- Lemmatisation\n",
    "- Extraction de concordances (KWIC) pour un ensemble de mots-clés d'intérêt\n",
    "- Extraction de termes MeSH et SNOMED présents dans les données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604b66c",
   "metadata": {},
   "source": [
    "*Suivi des modifications apportées*  \n",
    "**2022-06-17**  \n",
    "- Traiter un ensemble de documents .txt plutôt qu'un seul gros corpus ✓\n",
    "- Retenir seulement les termes qui ont une fréquence supérieure à x ✓\n",
    "- Rammener l'étiquetage morphosyntaxique et le filtrage par patrons syntaxiques plus tôt afin de filtrer les KWIC ✓\n",
    "  \n",
    "**2022-06-22**  \n",
    "- Ajouter le tag associé au sous-corpus dans les paths (acteur + path) ✓\n",
    "- Convertir les tags de TreeTagger selon les tags utilisés par le Lefff pour les fournir au lemmatiseur (performance meilleure lorsqu'on lui indique la POS) ✓\n",
    "\n",
    "**2022-06-23**  \n",
    "- Modifier le lemmatiseur pour lui fournir les POS tags et qu'il performe mieux ✓\n",
    "\n",
    "*Modifications suivantes*\n",
    "- Étiquetage morpho-syntaxique et Lemmatisation plus tôt dans le traitement pour améliorer la performance et la rapidité des deux outils.\n",
    "- En faire une fonction def nlp(corpus) pour pouvoir la relancer à l'étape de la pondération statistique ✓ (voir Notebook *3_Pondération_statistique.ipynb*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece38e9",
   "metadata": {},
   "source": [
    "### **Lire le corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a62415e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4342"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, shutil, re\n",
    "from pathlib import Path\n",
    "\n",
    "acteur = 'inspq'\n",
    "sous_corpus = False \n",
    "tag = ''\n",
    "\n",
    "# Change the directory\n",
    "if sous_corpus:\n",
    "    path = '/Users/camilledemers/Documents/03-corpus/2-sous-corpus/'\n",
    "    path = path + acteur + '/' + tag +'/'\n",
    "\n",
    "else: \n",
    "    path = '/Users/camilledemers/Documents/03-corpus/2-data/1-fr/'\n",
    "    path = path + acteur + '/'\n",
    "\n",
    "os.chdir(path)\n",
    "len(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d9679933",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\") and not file.endswith('-corpus_FR.txt') and not 'PDF' in file:\n",
    "        file_path = path + file\n",
    "        \n",
    "        with open(file_path, 'r', encoding = \"UTF-8\") as f:\n",
    "            data = f.readlines()\n",
    "            text = re.sub('\\d', '', data[1].strip('\\n').lower().replace('’', '\\''))\n",
    "            text = text.replace('  ', ' ')\n",
    "            corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8030c460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y avait 0 documents PDF dans notre dossier, pour l'instant on ne les traitera pas.\n"
     ]
    }
   ],
   "source": [
    "count_pdf = 0\n",
    "for file in os.listdir():\n",
    "    if 'PDF' in file:\n",
    "        count_pdf +=1\n",
    "\n",
    "print('Il y avait {} documents PDF dans notre dossier, pour l\\'instant on ne les traitera pas.'.format(count_pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4fda380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a un corpus de 4342 documents.\n"
     ]
    }
   ],
   "source": [
    "corpus = corpus[:round(len(corpus))]\n",
    "\n",
    "nb_docs = len(corpus)\n",
    "\n",
    "print(\"On a un corpus de {} documents.\".format(nb_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c732a6",
   "metadata": {},
   "source": [
    "### **Segmentation** (phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a4b16",
   "metadata": {},
   "source": [
    "**NLTK**  \n",
    "https://www.nltk.org/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31145e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(['popular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb0c1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize \n",
    "\n",
    "sents = [[s.strip('.') for s in sent_tokenize(doc)] for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328558c",
   "metadata": {},
   "source": [
    "*Il faudrait probablement modifier le script pour rammener le POS tagging et la lemmatisation ici (TreeTagger tokenise lui-même déjà)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980c335",
   "metadata": {},
   "source": [
    "### **Tokenisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5206beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Seulement les caractères alphabétiques\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w\\'|\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "afbd6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[tokenizer_re.tokenize(s) for s in doc] for doc in sents]\n",
    "\n",
    "len_corpus = len(nltk.flatten(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "047334bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec le RegExpTokenizer, notre corpus contient 39223816 tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Avec le RegExpTokenizer, notre corpus contient {} tokens.\".format(len_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab9f5e",
   "metadata": {},
   "source": [
    "### **Filtrage** (antidictionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6d96dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer l'antidictionnaire pour filtrer les données\n",
    "from pandas import *\n",
    "\n",
    "# Stopwords fréquents en français\n",
    "path = \"/Users/camilledemers/Documents/04-filtrage/stopwords.csv\"\n",
    "with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = read_csv(f)\n",
    "    stopwords = [t.lower() for t in stopwords['Stopwords'].tolist()]\n",
    "\n",
    "\n",
    "# Stopwords fréquents en anglais\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/stop_words_english.txt'\n",
    "with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "    sw = [w.strip('\\n').lower() for w in f.readlines()]\n",
    "\n",
    "stopwords += sw\n",
    "\n",
    "# Signes de ponctuation\n",
    "import string \n",
    "punct = [s for s in string.punctuation] \n",
    "punct += ['»' ,'©', '']\n",
    "\n",
    "stopwords += punct\n",
    "\n",
    "\n",
    "# Mis en commentaire pour l'instant car ça allonge le délai de traitement\n",
    "#Prénoms (curieusement, il y en a beaucoup dans les données)\n",
    "#path = '/Users/camilledemers/Documents/04-filtrage/Prenoms.csv'\n",
    "#with open(path, 'r', encoding='utf-8') as f:\n",
    "#     sw = read_csv(f)\n",
    "#     sw = [str(t).lower() for t in sw['01_prenom'].tolist()]\n",
    "\n",
    "#stopwords += sw\n",
    "\n",
    "#Noms de famille \n",
    "# path = '/Users/camilledemers/Documents/04-filtrage/nomsFamille.csv'\n",
    "# with open(path, 'r', encoding='utf-8') as f:\n",
    "#     sw = read_csv(f)\n",
    "#     sw = [str(t).lower() for t in sw['Nom'].tolist()]\n",
    "\n",
    "# stopwords += sw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0ac5e",
   "metadata": {},
   "source": [
    "### **Filtrage (MWE - stopwords formés de plusieurs tokens)**\n",
    "Surtout pour filtrer les expressions relatives à l'architecture d'information / navigation Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "02e6dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/mwe_stopwords.txt'\n",
    "\n",
    "with open (path, 'r', encoding='utf-8') as f:\n",
    "    mwe_sw = [tuple(tokenizer_re.tokenize(t)) for t in f.readlines()]\n",
    "    #mwe_sw = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4d08401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer_mwe = MWETokenizer(mwe_sw, separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "53d3ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_sw = [tokenizer_mwe.tokenize(w)[0] for w in mwe_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ddb87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[[t for t in tokenizer_mwe.tokenize(sent) if t not in mwe_sw] for sent in doc] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b5f4d",
   "metadata": {},
   "source": [
    "### **Phrases / N-Grammes (MWE)**\n",
    "https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "addba715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d4fc7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import everygrams\n",
    "ngrammes = [[list(everygrams(sent, min_len=2, max_len=4)) for sent in doc] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1d405",
   "metadata": {},
   "source": [
    "### **Filtrage (N-grammes)**\n",
    "\n",
    "On retire les n-grammes qui apparaissent moins de 30 fois dans tout le corpus ou qui débutent ou terminent par :\n",
    "- un stopword\n",
    "- un mot de 1 lettre ou moins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3d97d",
   "metadata": {},
   "source": [
    "Pour le reste du traitement, on arrête de considérer les frontières entre les phrases et entre les documents (nos ngrammes les respectent donc on n'en a plus besoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc4c59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la distribution de fréquence de chaque ngramme dans tout le corpus\n",
    "# Pour ça, on va 'applatir' la liste des ngrammes pour ne plus tenir compte des frontières entre les phrases et entre les documents\n",
    "def ng_flat(ngramme):\n",
    "    liste    = [] \n",
    "    for doc in ngrammes:\n",
    "        for sent in doc:\n",
    "            for ngram in sent:\n",
    "                liste.append(ngram)\n",
    "    return liste\n",
    "\n",
    "ngrammes = ng_flat(ngrammes)\n",
    "freq = FreqDist(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df8b7f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114960688"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a008d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, re\n",
    "\n",
    "ngrammes = [ngram for ngram in ngrammes if freq[ngram] > 30 and \\\n",
    "        not ngram[0] in stopwords and len(ngram[0]) > 2 \\\n",
    "        and not ngram[-1] in stopwords and len(ngram[-1]) > 2 \\\n",
    "        and not (ngram[0] == ngram[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55790845"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7da90ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\" \".join(ngram).replace('\\' ', '\\'') for ngram in ngrammes]\n",
    "freq = FreqDist(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "be52a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulaire = freq.keys()\n",
    "\n",
    "def tabCSV(tab):\n",
    "    tab = DataFrame(tab.items(), columns= [\"Expression\", \"Fréquence\"])\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "\n",
    "    #path = '/Users/camilledemers/Documents/04-filtrage/' + acteur + '/'\n",
    "    #if sous_corpus:\n",
    "    #    path += tag + '/'\n",
    "    #    file_path = path + acteur + '_' + tag\n",
    "        \n",
    "    \n",
    "    #else:\n",
    "    #    file_path = path \n",
    "\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a3f79a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/' \n",
    "\n",
    "if sous_corpus:\n",
    "    path = path + acteur + '/' + tag + '/'\n",
    "    titre =   tag\n",
    "else :\n",
    "    path = path + acteur + '/'\n",
    "    titre = acteur\n",
    "\n",
    "tab = tabCSV(freq)\n",
    "\n",
    "tab.to_csv(path + titre + '_n-grams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e83245",
   "metadata": {},
   "source": [
    "### **POS Tagging** (TreeTagger)\n",
    "https://github.com/miotto/treetagger-python/blob/master/README.rst  \n",
    "https://treetaggerwrapper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47508adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49585685",
   "metadata": {},
   "source": [
    "### **Mapping POS Tags** (FRMG)\n",
    "\n",
    "Pour utiliser adéquatement notre lemmatiseur par la suite (FrenchLefffLemmatizer), on va mapper les étiquettes morphosyntaxiques du TreeTagger à celles que prend le lemmatiseur (celles issues de FRMG)\n",
    "\n",
    "http://alpage.inria.fr/frmgwiki/content/tagset-frmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "13d76bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/mapping_treeTagger_lefff.csv'\n",
    "\n",
    "with open(path) as f:\n",
    "    csv = read_csv(f)\n",
    "\n",
    "treeTag = [term for term in csv['TreeTagger'].tolist()] \n",
    "lefff = [term for term in csv['Lefff'].tolist()]\n",
    "\n",
    "mapping = {term : lefff[treeTag.index(term)] for term in treeTag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20ab5679",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = [[phrase, \" \".join([mapping[t.split('\\t')[1]] for t in tagger.tag_text(phrase)])] for phrase in phrases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386aa5c",
   "metadata": {},
   "source": [
    "### **Filtrage (Patrons syntaxiques)**  \n",
    "Lossio-Ventura, J. A., Jonquet, C., Roche, M., & Teisseire, M. (2014). Biomedical Terminology Extraction : A new combination of Statistical and Web Mining Approaches. 421. https://hal-lirmm.ccsd.cnrs.fr/lirmm-01056598\n",
    "\n",
    "On veut aller extraire les structures syntaxiques les plus courantes dans les MeSH pour filtrer notre corpus selon celles-ci (inspiré de la méthodologie de l'article ci-dessus ; voir le Notebook *Mesh_extract.ipynb*). Pour ce faire, nous allons donc ne sélectionner que les ngrammes qui y correspondent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15945967",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/MeSH/mesh_patterns-fr.csv'\n",
    "\n",
    "with open (path, 'r') as f:\n",
    "    patterns = read_csv(f)\n",
    "    patterns = patterns['Structure'].tolist()[:50] # Pour prendre seulement les 200 structures syntaxiques les plus fréquentes dans les MeSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e03045",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [t for t in tagged if t[1] in patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/'\n",
    "tab = pd.DataFrame(terms, columns= [\"Expression\", \"Structure syntaxique\"])\n",
    "tab = pd.DataFrame(tab.groupby([\"Expression\", \"Structure syntaxique\"]).size().reset_index(name=\"Fréquence\"))\n",
    "tab.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "\n",
    "if sous_corpus:\n",
    "    path = path + acteur + '/' + tag + '/'\n",
    "    titre =   tag\n",
    "else :\n",
    "    path = path + acteur + '/'\n",
    "    titre = acteur\n",
    "\n",
    "                    \n",
    "tab.to_csv(path + titre + '_phrases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb27e72",
   "metadata": {},
   "source": [
    "### **Lemmatisation** (FrenchLefffLemmatizer)\n",
    "\n",
    "https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c874693",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [] \n",
    "for term in terms:\n",
    "    exp = tokenizer_re.tokenize(term[0]) # Un seul token dans un ngramme\n",
    "    pos = term[1].split() # Étiquette morphosyntaxique\n",
    "\n",
    "    term = [exp, pos]\n",
    "\n",
    "    n = len(exp) # == len(exp[1])\n",
    "\n",
    "    extr = []\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            extr.append((term[0][i], term[1][i]))\n",
    "        except:\n",
    "            print(\" \".join(term[0]))\n",
    "\n",
    "    input.append(extr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2580b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = FrenchLefffLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []\n",
    "\n",
    "for term in input:\n",
    "    term_lemmatized = []\n",
    "    for t in term:\n",
    "        if(lemmatizer.lemmatize(t[0], t[1]) == []):\n",
    "            term_lemmatized.append(lemmatizer.lemmatize(t[0]))\n",
    "        else:\n",
    "            term_lemmatized.append(lemmatizer.lemmatize(t[0], t[1])[0][0]) # [0][0] pour avoir le lemme seul et non (lemme, pos)\n",
    "    \n",
    "    lemmas.append(term_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce68d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\" \".join(t) for t in lemmas if len(t[0]) > 1 and len(t[-1]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc2bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist(lemmas)\n",
    "tab = pd.DataFrame(freq.items(), columns = [\"Expression lemmatisée\", \"Fréquence\"])\n",
    "tab.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "                    \n",
    "path = '/Users/camilledemers/Documents/04-filtrage/' \n",
    "if sous_corpus:\n",
    "    path = path + acteur + '/' + tag + '/'\n",
    "    titre =   tag\n",
    "else :\n",
    "    path = path + acteur + '/'\n",
    "    titre = acteur\n",
    "\n",
    "\n",
    "tab.to_csv(path + titre + '_phrases_lemmatized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ef6e6",
   "metadata": {},
   "source": [
    "### **KWIC (Keyword in Context)**\n",
    "Termes d'intérêt : \n",
    "- « Programme »\n",
    "- « Plan »\n",
    "- « Service(s) de » \n",
    "- « Intervenant(e) en »\n",
    "- « Professionnel de »\n",
    "- « Institut (du/de) »\n",
    "- « Groupe de recherche en »\n",
    "- « Personne »\n",
    "- « Infirmière (en) »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans notre cas on veut que ça débute par le mot-clé donc le contexte est un peu plus simple\n",
    "# penser à généraliser avec des expressions régulières\n",
    "kw = ['programme', 'plan ', 'service', 'intervenant', 'infirmière en', 'institut', 'groupe de recherche', 'personne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c50b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrant = pd.DataFrame(columns=['Mot-clé','Concordance', 'Fréquence'])\n",
    "kwic = {w : [] for w in kw} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255df07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in phrases: # on pourrait aussi chercher dans les terms, mais on perd certains termes d'intérêt avec le filtrage syntaxique\n",
    "    for w in kw:\n",
    "        if t.startswith(w):\n",
    "            kwic[w].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic = {term: FreqDist(kwic[term]) for term in kwic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in kw:\n",
    "    df = pd.DataFrame(kwic[term].items(), columns=['Concordance', \"Fréquence\"])\n",
    "    df.sort_values([\"Fréquence\"], \n",
    "        axis=0,\n",
    "        ascending=[False], \n",
    "        inplace=True)\n",
    "\n",
    "    df.insert(0, 'Mot-clé', term)\n",
    "    extrant = pd.concat([extrant, df])\n",
    "\n",
    "path = '/Users/camilledemers/Documents/04-filtrage' + '/'\n",
    "if sous_corpus:\n",
    "    path = path + acteur + '/' + tag + '/'\n",
    "    titre =   tag\n",
    "else :\n",
    "    path = path + acteur + '/'\n",
    "    titre = acteur\n",
    "\n",
    "\n",
    "extrant.to_csv(path + titre + '_KWIC' +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c7cd3",
   "metadata": {},
   "source": [
    "### **Filtrage (fréquence)**\n",
    "Pour la suite du traitement, on ne retient que les N expressions (lemmatisées ou non) les plus fréquentes dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sans lemmatiser\n",
    "freq_terms = FreqDist([t[0] for t in terms])\n",
    "exp_freq = freq_terms.most_common(1000) # On garde un maximum de 1000 termes\n",
    "\n",
    "path = '/Users/camilledemers/Documents/05-transformation/' + acteur + '/'\n",
    "\n",
    "if sous_corpus:\n",
    "    path = path + tag + '/'\n",
    "    titre =   tag\n",
    "else :\n",
    "    titre = acteur\n",
    "\n",
    "                    \n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "tab = pd.DataFrame([t[0] for t in exp_freq], columns=[\"Terme\"])\n",
    "tab.to_csv(path + titre + '_terms.csv')\n",
    "\n",
    "# En lemmatisant\n",
    "freq_terms = FreqDist([t for t in lemmas])\n",
    "exp_freq = freq_terms.most_common(1000) # On garde un maximum de 1000 termes\n",
    "\n",
    "path = '/Users/camilledemers/Documents/05-transformation/' + acteur + '/'\n",
    "\n",
    "if sous_corpus:\n",
    "    path = path + tag + '/'\n",
    "    titre =   tag\n",
    "else :\n",
    "    titre = acteur\n",
    "\n",
    "                    \n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "tab = pd.DataFrame([t[0] for t in exp_freq], columns=[\"Terme\"])\n",
    "tab.to_csv(path + titre + '_terms-lemmatized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45949eb3",
   "metadata": {},
   "source": [
    "### **Extraction de termes MeSH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/MeSH/mesh-fr.txt'\n",
    "\n",
    "with open (path, 'r', encoding='utf-8') as f:\n",
    "    mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "    tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "    mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "    mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "    #mesh = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_terms = FreqDist([t[0] for t in terms])\n",
    "exp_freq = freq_terms.most_common()\n",
    "\n",
    "extr_mesh = tokenizer_mesh.tokenize([t[0] for t in exp_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = []\n",
    "\n",
    "for t in extr_mesh:\n",
    "    if t in mesh:\n",
    "        termes_mesh.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage' + '/'\n",
    "if sous_corpus:\n",
    "    path = path + acteur + '/' + tag + '/'\n",
    "    titre =   tag\n",
    "else :\n",
    "    path = path + acteur + '/'\n",
    "    titre = acteur\n",
    "\n",
    "df = DataFrame(termes_mesh)\n",
    "\n",
    "df.to_csv(path + titre + '_MeSH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685131ee",
   "metadata": {},
   "source": [
    "### **Extraction de termes SNOMED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/SNOMED/SNOMED_fr.csv'\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    sm = read_csv(f, sep=';')\n",
    "    sm = list(dict.fromkeys([str(t).strip().lower() for t in sm['term'].tolist()]))\n",
    "\n",
    "    sm = [tuple(tokenizer_re.tokenize(w)) for w in sm if len(w.split()) > 1]\n",
    "    tokenizer_sm = MWETokenizer(sm, separator = ' ')\n",
    "\n",
    "    sm = [tokenizer_sm.tokenize(w)[0].lower() for w in sm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_sm = tokenizer_sm.tokenize([t[0] for t in exp_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = []\n",
    "\n",
    "for t in extr_sm:\n",
    "    if t in sm:\n",
    "        termes_sm.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage' + '/'\n",
    "if sous_corpus:\n",
    "    path = path + acteur + '/' + tag + '/'\n",
    "    titre =   tag\n",
    "else :\n",
    "    path = path + acteur + '/'\n",
    "    titre = acteur\n",
    "\n",
    "df = DataFrame(termes_sm)\n",
    "\n",
    "df.to_csv(path + titre + '_SNOMED.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a33210152f7d2bd255fb16656f372b633dbf298ed202bbbac20290b0375cadb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
