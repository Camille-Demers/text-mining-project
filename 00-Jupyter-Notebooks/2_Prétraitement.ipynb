{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c662c7d",
   "metadata": {},
   "source": [
    "## **2. Prétraitement**\n",
    "- Segmentation (phrases)\n",
    "- Tokenization (mots)\n",
    "- Étiquetage morphosyntaxique (POS Tagging) \n",
    "- (Lemmatisation - *sur la glace ; ça pourrait permettre d'éviter les faux négatifs dûs à des variations singulier/pluriel quand on essaie d'extraire les termes qui existent déjà dans la taxonomie*)\n",
    "- Filtrage (stopwords)\n",
    "- Extraction de termes complexes (MWE / n-grammes / segments répétés)\n",
    "- Chunking / Filtrage par patrons syntaxiques (basés sur les patrons fréquents dans les MeSH)\n",
    "- Extraction de collocations significatives (en fonction du Log-likelihood ratio)\n",
    "- Extraction de concordances (KWIC) pour un ensemble de mots-clés d'intérêt\n",
    "- Extraction de termes MeSH présents dans les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e374a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, re, random\n",
    "from os import listdir, chdir, path\n",
    "from pathlib import Path\n",
    "from pandas import *\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "#nltk.download(['popular'])\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w\\'|\\w+\")\n",
    "from nltk import bigrams, trigrams, ngrams, everygrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7af048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_global = []\n",
    "base_path = '../03-corpus/1-crawler/'\n",
    "for file in listdir(base_path):\n",
    "    if file.endswith('.csv'):\n",
    "        with open(base_path + file, encoding = 'utf-8',) as f:\n",
    "            corpus_global.append({'acteur': file[:-4], 'N_fr': 0, 'N_en' : 0}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec78871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "lng = 'fr'\n",
    "\n",
    "# À modifier pour 'lng=en'\n",
    "def lire_corpus(acteur, langue=lng):\n",
    "    base_path = '../03-corpus/2-data/'\n",
    "    \n",
    "    folder_path = path.join(base_path, '1-' + langue, acteur)\n",
    "    all_files = glob.glob(path.join(folder_path, \"*.csv\"))\n",
    "    tags = [f.split('_')[1][:-4] for f in listdir(folder_path)]\n",
    "\n",
    "    df = DataFrame()\n",
    "    for f, tag in zip(all_files, tags):\n",
    "        csv = read_csv(f, encoding='utf-8', sep=',')\n",
    "        #csv = csv[~csv[\"Address\"].str.contains('pdf')] #  Problèmes \n",
    "        #csv = csv['text']\n",
    "        # Using DataFrame.insert() to add a column\n",
    "        #df = concat([df, csv]) [['Corpus', 'Sous-corpus', 'Title', 'text']]\n",
    "        df = concat([df, csv]) [['Corpus', 'Sous-corpus', 'Address', 'Title', 'Type', 'text']].drop_duplicates(subset=['text'])\n",
    "    return df\n",
    "\n",
    "def filter_mwesw(corpus):\n",
    "    file_mwesw = '../04-filtrage/mwe_stopwords.txt'\n",
    "    with open (file_mwesw, 'r', encoding='utf-8') as f:\n",
    "        mwe_sw = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "    for mwe in mwe_sw:\n",
    "        corpus = corpus.replace(mwe, ' STOP ').replace('  ', \" \")\n",
    "    return corpus\n",
    "\n",
    "def join(corpus):\n",
    "    return \" \".join(corpus)\n",
    "\n",
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG=lng)\n",
    "\n",
    "def tok(corpus):\n",
    "    # Seulement les caractères alphabétiques\n",
    "    tokens = tokenizer_re.tokenize(corpus)\n",
    "    print(\"Avec le RegExpTokenizer, notre corpus contient {} tokens.\".format(len(tokens)))\n",
    "    temps = round(len(tokens) / 15000 / 60)\n",
    "    print('Le POS tagging devrait prendre environ {} minutes.'.format(temps))\n",
    "\n",
    "def tagging(corpus):\n",
    "    output = []\n",
    "    for t in tagger.tag_text(corpus):\n",
    "        try: \n",
    "            output.append([t.split('\\t')[0], t.split('\\t')[1]])\n",
    "        except Exception as e:\n",
    "            output.append(('STOP', 'NAM'))\n",
    "\n",
    "    return output\n",
    "\n",
    "# from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "# lemmatizer = FrenchLefffLemmatizer()\n",
    "\n",
    "# file_path = '../04-filtrage/mapping_treeTagger_lefff.csv'\n",
    "# with open(file_path) as f:\n",
    "#    csv = read_csv(f)\n",
    "# treeTag = [term for term in csv['TreeTagger'].tolist()] \n",
    "# lefff = [term for term in csv['Lefff'].tolist()]\n",
    "# mapping = {term : lefff[treeTag.index(term)] for term in treeTag}\n",
    "\n",
    "def extr_ngrams(tagged):\n",
    "    ngrammes= list(everygrams(tagged, min_len=2, max_len=8))\n",
    "    print(\"Avant filtrage, on a {} ngrammes.\".format(len(ngrammes)))\n",
    "    return ngrammes\n",
    "\n",
    "def extract_patterns(ngrammes):\n",
    "    patterns = []\n",
    "    for ng in ngrammes:\n",
    "        phrase = tuple([t[0] for t in ng])\n",
    "        pattern = [t[1] for t in ng]\n",
    "        patterns.append([phrase, pattern])\n",
    "    return patterns\n",
    "\n",
    "# Importer l'antidictionnaire pour filtrer les données\n",
    "\n",
    "# Stopwords fréquents en français \n",
    "file_path = \"../04-filtrage/stopwords.txt\"\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "\n",
    "# Stopwords fréquents en anglais\n",
    "file_path = '../04-filtrage/stop_words_english.txt'\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords += [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "def filtrer_stopwords(x): # On s'assure aussi que le premier terme du ngramme est un NOM pour avoir des syntagmes nominaux\n",
    "    if lng == 'en':\n",
    "        nom = 'NN'\n",
    "    if lng == 'fr':\n",
    "        nom = 'NOM'\n",
    "    return [term for term in x if not 'STOP' in term[0] and not term[0][0] in stopwords and not term[0][-1] in stopwords \\\n",
    "        and not 'NUM' in term[1] and term[1][0] == nom and not '.' in term[0] and not '-' in term[0] and not ':' in term[0]\\\n",
    "        \n",
    "        # Une parenthèse fermante peut juste se trouver comme dernier token\n",
    "        # Si une parenthèse est ouverte, elle doit aussi être fermée (et vice versa)\n",
    "        and not ')' in term[0][:-1] and not ('(' in term[0] and not ')' in term[0]) \\\n",
    "        and not (')' in term[0] and not '(' in term[0])]\n",
    "\n",
    "def filter_len(x):\n",
    "    return [term for term in x if \\\n",
    "        (len(term[0][0]) > 2 or term[0][0] == '(')  and (len(term[0][-1]) > 2 or term[0][-1] == ')') and \\\n",
    "        len(term[0][0]) < 18 and len(term[0][-1]) < 18]\n",
    "\n",
    "def filter_freq(x, freqd):\n",
    "    return [term for term in x if freqd[tuple(term[0])] > 5]\n",
    "\n",
    "file_patterns = '../04-filtrage/MeSH/mesh_patterns-fr.csv'\n",
    "\n",
    "with open (file_patterns, 'r') as f:\n",
    "    patterns = read_csv(f)\n",
    "    patterns = patterns['Structure'].tolist()[:200] # Pour prendre les 200 structures syntaxiques les plus fréquentes dans les MeSH\n",
    "\n",
    "def filter_patterns(phrases):\n",
    "    return [t for t in phrases if t[1] in patterns and not 'NUM' in t[1]] # and not 'NOM NOM' in t[1]\n",
    "\n",
    "def loglikelihood_ratio(c_prior, c_n, c_ngram, N):\n",
    "    \"\"\"\n",
    "    Compute the ratio of two hypotheses of likelihood and return the ratio.\n",
    "    The formula here and test verification values are taken from \n",
    "    Manning & Schūtze _Foundations of Statistical Natural Language Processing_ p.172-175\n",
    "    Parameters:\n",
    "    c_prior: count of word 1 if bigrams or count of [w1w2 .. w(n-1)] if ngram\n",
    "    c_n : count of word 2 if bigrams or count of wn if ngram\n",
    "    c12: count of bigram (w1, w2) if bigram or count of ngram if ngram\n",
    "    N: the number of words in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    p = c_n / N\n",
    "    p1 = c_ngram / c_prior\n",
    "    p2 = (c_n - c_ngram) / (N - c_prior)   \n",
    "    # We proactively trap a runtimeWarning: divide by zero encountered in log,\n",
    "    # which may occur with extreme collocations\n",
    "    import warnings\n",
    "    with warnings.catch_warnings(): # this will reset our filterwarnings setting\n",
    "        warnings.filterwarnings('error')\n",
    "        try:\n",
    "            return (np.log(binom.pmf(c_ngram, c_prior, p)) \n",
    "                    + np.log(binom.pmf(c_n - c_ngram, N - c_prior, p)) \n",
    "                    - np.log(binom.pmf(c_ngram, c_prior, p1) )\n",
    "                    - np.log(binom.pmf(c_n - c_ngram, N - c_prior, p2)))             \n",
    "        except Warning:\n",
    "            return np.inf \n",
    "\n",
    "\n",
    "def llr_ngrammes(terms, freq, fdtok, patterns):\n",
    "    llr = []\n",
    "\n",
    "    ngrammes = [term[0] for term in terms]\n",
    "        \n",
    "    for t in ngrammes:\n",
    "        if len(t) == 1:\n",
    "            try:\n",
    "                llr.append({'Terme' : str(t[0]), 'Structure syntaxique': patterns[t], 'Fréquence (TF)' : fdtok[str(t[0])], 'LLR': '-', 'p-value': '-'})\n",
    "            except Exception as e:\n",
    "                print(t, str(e))\n",
    "        else:\n",
    "            c_prior = freq[t[:-1]] # Antécédent = P(w1w2..w_n-1) (si on considère que P(w1w2...wn) = P(wn) | P(w1w2...w_n-1)\n",
    "            c_n = fdtok[t[-1]]     # Dernier mot du ngramme  P(wn)\n",
    "            c_ngram = freq[t]             # Le ngramme lui-même P(w1w2w3..wn)\n",
    "\n",
    "            try:\n",
    "                res = -2 * loglikelihood_ratio(c_prior, c_n, c_ngram, N)\n",
    "                p_value = chi2.sf(res, 1) # 1 degrees of freedom\n",
    "\n",
    "                if p_value < 0.001 or (res == float('-inf')):\n",
    "                    #llr.append({'Collocation' : \" \".join(t).replace(\"' \", \"'\").replace(\"( \", \"(\").replace(\" )\", \")\"), 'Structure syntaxique': dict_patterns[\" \".join(t).replace(\"' \", \"'\")], 'Fréquence' : c_ngram, 'LLR': res, 'p-value': p_value})\n",
    "                    llr.append({'Terme' : t, 'Structure syntaxique': patterns[t], 'Fréquence (TF)' : c_ngram, 'LLR': res, 'p-value': p_value})\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(t, str(e))\n",
    "            \n",
    "    return llr\n",
    "\n",
    "def join_term(x):\n",
    "    if type(x) == tuple:\n",
    "        return \" \".join(x).replace(\"' \", \"'\").replace(\"( \", \"(\").replace(\" )\", \")\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pandas as pd\n",
    "base_path = '../04-filtrage/MeSH/'\n",
    "\n",
    "tree = ET.parse(base_path + 'fredesc2019.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "data_mesh = [{'mesh_id' : x.find('DescriptorUI').text.strip('\\n'), \\\n",
    "         'label_fr' : x.find('DescriptorName').find('String').text.split('[')[0], \\\n",
    "         'label_en' : x.find('DescriptorName').find('String').text.split('[')[1].strip(']'), \\\n",
    "         'synonymes (en/fr)' : flatten([[term.find('String').text for term in concept.find('TermList').findall('Term')] for concept in x.find('ConceptList').findall('Concept')]) \\\n",
    "         } for x in root.findall('DescriptorRecord')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ff337d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chum 2238\n",
      "Avec le RegExpTokenizer, notre corpus contient 870350 tokens.\n",
      "Le POS tagging devrait prendre environ 1 minutes.\n",
      "Avant filtrage, on a 6378708 ngrammes.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\p1115145\\Documents\\text-mining-project\\00-Jupyter-Notebooks\\2_Prétraitement.ipynb Cellule 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pre%CC%81traitement.ipynb#W4sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m phrases \u001b[39m=\u001b[39m extract_patterns(ngrammes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pre%CC%81traitement.ipynb#W4sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# phrases_lem = extract_patterns(ngrammes_lem)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pre%CC%81traitement.ipynb#W4sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pre%CC%81traitement.ipynb#W4sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# def freq(phrases):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pre%CC%81traitement.ipynb#W4sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# def freq(phrases):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pre%CC%81traitement.ipynb#W4sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m#     return FreqDist([t[0] for t in phrases])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pre%CC%81traitement.ipynb#W4sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m frequencies \u001b[39m=\u001b[39m FreqDist(everygrams(tokens, min_len\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, max_len\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pre%CC%81traitement.ipynb#W4sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m# Filtrage (antidictionnaire) \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pre%CC%81traitement.ipynb#W4sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m phrases \u001b[39m=\u001b[39m filtrer_stopwords(phrases)\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\probability.py:102\u001b[0m, in \u001b[0;36mFreqDist.__init__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, samples\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39m    Construct a new frequency distribution.  If ``samples`` is\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39m    given, then the frequency distribution will be initialized\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39m    :type samples: Sequence\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     Counter\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, samples)\n\u001b[0;32m    104\u001b[0m     \u001b[39m# Cached number of samples in this FreqDist\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_N \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\collections\\__init__.py:577\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[39m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[39mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[39mof elements to their counts.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    574\u001b[0m \n\u001b[0;32m    575\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 577\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(iterable, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\probability.py:140\u001b[0m, in \u001b[0;36mFreqDist.update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39mOverride ``Counter.update()`` to invalidate the cached N\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_N \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\collections\\__init__.py:670\u001b[0m, in \u001b[0;36mCounter.update\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mupdate(iterable)\n\u001b[0;32m    669\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m         _count_elements(\u001b[39mself\u001b[39;49m, iterable)\n\u001b[0;32m    671\u001b[0m \u001b[39mif\u001b[39;00m kwds:\n\u001b[0;32m    672\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(kwds)\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\probability.py:126\u001b[0m, in \u001b[0;36mFreqDist.__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39mOverride ``Counter.__setitem__()`` to invalidate the cached N\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_N \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__setitem__\u001b[39;49m(key, val)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(len(corpus_global)-5) : \n",
    "    acteur = corpus_global[x]['acteur']\n",
    "\n",
    "    data = lire_corpus(acteur) \n",
    "    nb_docs = len(data)\n",
    "    print(acteur, str(nb_docs))\n",
    "    corpus_global[x]['N_fr'] = nb_docs\n",
    "\n",
    "    base_path = '../03-corpus/2-data/'\n",
    "    folder_path = path.join(base_path, '1-' + lng, acteur)\n",
    "\n",
    "    #data.to_csv(folder_path + '.csv')\n",
    "\n",
    "    #Nettoyage\n",
    "    punct = '[!#$%&•►*+,;\\/\\\\<=>?@[\\]^_{|}~©«»—“”–—]'\n",
    "    spaces = '\\s+'\n",
    "    postals = '([a-zA-Z]+\\d+|\\d+[a-zA-Z]+)+'\n",
    "    # phones = '\\d{3}\\s\\d{3}-\\d{4}' #très simple (trop)\n",
    "\n",
    "    text = [str(t).strip('\\n').lower().replace('’', '\\'') for t in data['text'].tolist()]\n",
    "    text = [re.sub(spaces, ' ', t) for t in text]\n",
    "    text = [re.sub(postals, ' STOP ', t) for t in text]\n",
    "    text = [re.sub(punct, ' STOP ', t) for t in text]\n",
    "    text = [t.replace(\"  \", \" \" ) for t in text]\n",
    "\n",
    "    corpus = join(text)\n",
    "    corpus = filter_mwesw(corpus)\n",
    "    \n",
    "\n",
    "    # Ici, on tokenise une première fois avec le Regex Tokenizer de NLTK pour voir combien de temps ça devrait \n",
    "    # prendre au Tree Tagger pour tokeniser et tagger notre corpus\n",
    "    tok(corpus)\n",
    "\n",
    "    tagged = tagging(corpus)\n",
    "    tokens = [t[0] for t in tagged]\n",
    "\n",
    "    # df_tagged = DataFrame(tagged, columns=['Token', 'Tag TreeTagger'])\n",
    "    # df_tagged['Tag Lefff'] = df_tagged['Tag TreeTagger'].map(mapping)\n",
    "    # df_tagged['Lemme'] = df_tagged['Token'] # S'il est pas capable de lemmatiser ensuite, on garde l'expression telle quelle\n",
    "\n",
    "    # tagged_dict = df_tagged.to_dict('records')\n",
    "\n",
    "    # # tokens = df_tagged['Token'].tolist()\n",
    "    # # tags = df_tagged['Tag Lefff'].tolist()\n",
    "    # # tagged_lefff = [(t, tg) for t,tg in zip(tokens, tags)]\n",
    "\n",
    "    # for term in tagged_dict:\n",
    "    #     if lemmatizer.lemmatize(word=term['Token'], pos=term['Tag Lefff']) == []:\n",
    "    #         term_l = lemmatizer.lemmatize(word=term['Token'])\n",
    "        \n",
    "    #     elif type(lemmatizer.lemmatize(word=term['Token'], pos=term['Tag Lefff'])) == str:\n",
    "    #         term_l  = lemmatizer.lemmatize(word=term['Token'], pos=term['Tag Lefff'])\n",
    "\n",
    "    #     else:\n",
    "    #         term_l = lemmatizer.lemmatize(word=term['Token'], pos=term['Tag Lefff'])[0][0]\n",
    "\n",
    "    #     term['Lemme'] = term_l\n",
    "\n",
    "    # tagged = DataFrame(tagged_dict)\n",
    "    # tagged = tagged.drop(columns=['Tag Lefff']).values.tolist()\n",
    "\n",
    "\n",
    "    # Extraction de n-grammes\n",
    "    ngrammes = extr_ngrams(tagged)\n",
    "    #ngrammes_lem = extr_ngrams(tagged_lem)\n",
    "\n",
    "\n",
    "    # Extraction de patrons syntaxiques\n",
    "    phrases = extract_patterns(ngrammes)\n",
    "    # phrases_lem = extract_patterns(ngrammes_lem)\n",
    "\n",
    "    # def freq(phrases):\n",
    "    #     return FreqDist([\" \".join(t[0]).replace(\"' \", \"'\") for t in phrases])\n",
    "\n",
    "    # def freq(phrases):\n",
    "    #     return FreqDist([t[0] for t in phrases])\n",
    "\n",
    "    frequencies = FreqDist(everygrams(tokens, min_len=2, max_len=8))\n",
    "\n",
    "    # Filtrage (antidictionnaire) \n",
    "    phrases = filtrer_stopwords(phrases)\n",
    "\n",
    "    # Filtrage (mots <2 caractères ou >18 caractères)\n",
    "    phrases = filter_len(phrases)\n",
    "\n",
    "    # Filtrage (fréquence > 5)\n",
    "    phrases = filter_freq(phrases, frequencies)\n",
    "\n",
    "    # Chunking\n",
    "    #phrases = [[\" \".join(term[0]).replace(\"' \", \"'\"), \" \".join(term[1])] for term in phrases]\n",
    "    phrases = [[term[0], \" \".join(term[1])] for term in phrases]\n",
    "    # phrases_lemmatized = filtrer_phrases(phrases_lemmatized, freq_lemmatized)\n",
    "\n",
    "    for phrase in phrases:\n",
    "        phrase.append(frequencies[tuple(phrase[0])])\n",
    "\n",
    "    # Filtrage (patrons syntaxiques)\n",
    "    terms = filter_patterns(phrases)\n",
    "\n",
    "    print(\"Le filtrage syntaxique élimine environ {} % des termes\".format(round((len(phrases) - len(terms)) / len(phrases) * 100)))\n",
    "    print(\"On avait {} ngrammes, \".format(len(phrases)) + \"on en a maintenant {}.\".format(len(terms)))\n",
    "\n",
    "\n",
    "    # def extract_terms(liste_terms):\n",
    "    #     file_path = '../04-filtrage/output/'\n",
    "    #     tab = DataFrame(terms, columns= [\"Expression\", \"Structure syntaxique\", \"Fréquence\"]).drop_duplicates(subset='Expression', keep=\"last\")\n",
    "    #     tab.sort_values([\"Fréquence\"], \n",
    "    #                         axis=0,\n",
    "    #                         ascending=[False], \n",
    "    #                         inplace=True)\n",
    "\n",
    "    #     return \n",
    "\n",
    "    #     # file_path = path.join(file_path, tag, tag)                    \n",
    "    #     # tab.to_csv(file_path + '_terms.csv')\n",
    "\n",
    "    # extract_terms(terms)\n",
    "    # #extract_terms(terms_lemmatized)\n",
    "\n",
    "    for phrase in terms:\n",
    "        phrase[0] = tuple(phrase[0])\n",
    "\n",
    "\n",
    "    terms_patterns = DataFrame(terms, columns = [\"Expression\", \"Structure syntaxique\", \"Fréquence\"])\n",
    "    terms_patterns = terms_patterns.to_dict('records')\n",
    "    dict_patterns = {}\n",
    "    for term in terms_patterns:\n",
    "        exp = term['Expression']\n",
    "        pattern = term['Structure syntaxique']\n",
    "        dict_patterns[exp] = pattern\n",
    "\n",
    "    # Collocations significatives (log-likelihood ratio)\n",
    "    N = len(tokens)\n",
    "    fd_tokens = nltk.FreqDist(tokens)\n",
    "\n",
    "    significant_coll = llr_ngrammes(terms, frequencies, fd_tokens, dict_patterns)\n",
    "    #terms = [{'Collocation' : t[0], 'Structure syntaxique': t[1], 'Fréquence' : t[2]} for t in terms] # Si on veut pas appliquer le LLR\n",
    "        \n",
    "    print('Après avoir calculé le log-likelihood ratio, on a retiré {} collocations qui n\\'étaient pas statistiquement significatives.'.format(len(terms) - len(significant_coll)))\n",
    "\n",
    "    df = DataFrame(significant_coll).sort_values(by = \"Fréquence (TF)\", ascending=False).drop_duplicates()\n",
    "\n",
    "    # On veut faire join pour tous les termes[collocation]\n",
    "    df['Terme'] = df['Terme'].apply(lambda x: join_term(x))\n",
    "\n",
    "    if lng == 'en':\n",
    "        acteur = acteur + '_' + lng\n",
    "\n",
    "    df = df.drop(columns=['p-value'])\n",
    "\n",
    "    # Filtrage (fréquence documentaire)\n",
    "    dfs = {term: len([doc for doc in text if term in doc]) for term in df['Terme'].tolist()}\n",
    "    max_df = round(0.98 * nb_docs)        # Pour rejeter les termes qui se retrouvent dans plus de 98% des documents \n",
    "    min_df = round(0.01 * nb_docs)         # Pour rejeter les termes qui se retrouvent dans moins de 1% des documents\n",
    "\n",
    "    dfs = {term:df for term,df in dfs.items() if df < max_df and df > min_df} # Si df = 0, c'est un artefact créé par le prétraitement lui-même\n",
    "    zeros =  {term:df for term,df in dfs.items() if df == 0}  \n",
    "    print('N(df == 0) : '+ str(len(zeros)))\n",
    "    \n",
    "    dfs = DataFrame(list(dfs.items()),columns = ['Terme','Fréquence documentaire (DF)']) \n",
    "    df = df.merge(dfs, on='Terme').drop_duplicates()\n",
    "\n",
    "    df.sort_values(['Fréquence (TF)'], \n",
    "                axis=0,\n",
    "                ascending=[False], \n",
    "                inplace=True)\n",
    "\n",
    "    list_terms = df['Terme'].tolist()\n",
    "\n",
    "    # Détection de termes appartenant aux MeSH\n",
    "    df['isMeSHTerm']= False # On set à False puis on va changer pour True si on trouve le terme\n",
    "    df['MeSHID'] = None\n",
    "    df['MesH_prefLabel_fr'] = None\n",
    "    df['MesH_prefLabel_en'] = None\n",
    "\n",
    "    from nltk.tokenize import MWETokenizer\n",
    "    file_path = '../04-filtrage/MeSH/mesh-fr.txt'\n",
    "\n",
    "    with open (file_path, 'r', encoding='utf-8') as f:\n",
    "        mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "        tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "        mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "        mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "        #mesh = [tuple(t.strip('.').lower().split()) for t in f.readlines()]\n",
    "\n",
    "    extr_mesh = tokenizer_mesh.tokenize(list_terms)\n",
    "\n",
    "    for t in extr_mesh:\n",
    "        if t in mesh:\n",
    "            df.loc[df['Terme'] == t, 'isMeSHTerm'] = True\n",
    "\n",
    "    # Extraction de termes existant dans la taxonomie\n",
    "    df['isTaxoTerm']= 'False' # On set à False puis on va changer pour True si on trouve le terme\n",
    "\n",
    "    file_path = '../04-filtrage/default_taxo_labels.csv'\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        default = read_csv(f, sep=';')\n",
    "        taxo_terms = list(dict.fromkeys([str(t).strip().lower() for t in default['Label'].tolist()]))\n",
    "\n",
    "    for t in df['Terme'].tolist():\n",
    "        if t in taxo_terms:\n",
    "            df.loc[df['Terme'] == t, 'isTaxoTerm'] = True\n",
    "\n",
    "    # Mapping to MeSH ids\n",
    "    for t in df[df['isMeSHTerm'] == True]['Terme'].tolist():\n",
    "        for d in data_mesh:\n",
    "            if t in [str(x).lower() for x in d['synonymes (en/fr)']]:\n",
    "                df.loc[df['Terme'] == t, 'MeSHID'] = d['mesh_id']\n",
    "                df.loc[df['Terme'] == t, 'MesH_prefLabel_fr'] = d['label_fr']\n",
    "                df.loc[df['Terme'] == t, 'MesH_prefLabel_en'] = d['label_en']\n",
    "\n",
    "    df = df[['Terme', 'Structure syntaxique',\t'Fréquence (TF)', 'Fréquence documentaire (DF)', 'LLR', 'isMeSHTerm', 'isTaxoTerm', 'MeSHID', 'MesH_prefLabel_fr', 'MesH_prefLabel_en']]\n",
    "    df.insert(0, 'Corpus', acteur)\n",
    "\n",
    "    if lng == 'en':\n",
    "        acteur = acteur + '_' + lng\n",
    "\n",
    "    output_path = path.join('../04-filtrage/output/', acteur + '_significant-collocations.csv') \n",
    "    #df.to_csv(output_path)\n",
    "\n",
    "    print('On a fini avec ce corpus : {}.'.format(acteur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3866a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79bb76bbc4f9ba1f8df5efe8db67aae07079a51dc7b5004f49990e90f5993a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
