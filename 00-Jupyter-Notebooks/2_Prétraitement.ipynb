{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c662c7d",
   "metadata": {},
   "source": [
    "## **2. Prétraitement**\n",
    "- Segmentation (phrases)\n",
    "- Tokenization (mots)\n",
    "- Filtrage (stopwords)\n",
    "- Extraction de termes complexes (MWE / n-grammes / segments répétés)\n",
    "- Étiquetage morphosyntaxique (POS Tagging) \n",
    "- Lemmatisation\n",
    "- Chunking / Filtrage par patrons syntaxiques (basés sur les patrons fréquents dans les MeSH)\n",
    "- Extraction de concordances (KWIC) pour un ensemble de mots-clés d'intérêt\n",
    "- Extraction de termes MeSH et SNOMED présents dans les données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93fdd02",
   "metadata": {},
   "source": [
    "**Précisions sur la structuration des différents objets créés (corpus, sents, tokens, ngrammes, tagged)**  \n",
    "| Index dans la structure      | Information représentée  |\n",
    "| ----------- | ----------- |\n",
    "| `x[0]`      | 1 document dans le corpus (`corpus`)       |\n",
    "| `x[0][0]`   | 1 phrase dans un document du corpus (`sents`) |\n",
    "| `x[0][0][0]`    | 1 ngramme dans un document (`tokens` / `ngrammes`)        |\n",
    "| `x[0][0][0]`   | 1 tuple (ngramme, patron syntaxique) dans un ngramme (`tuples`) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604b66c",
   "metadata": {},
   "source": [
    "**2022-06-17**  \n",
    "  \n",
    "*Modifications apportées*\n",
    "- Traiter un ensemble de documents .txt plutôt qu'un seul gros corpus ✓\n",
    "- Retenir seulement les termes qui ont une fréquence supérieure à x ✓\n",
    "- Rammener l'étiquetage morphosyntaxique et le filtrage par patrons syntaxiques plus tôt afin de filtrer les KWIC ✓\n",
    "  \n",
    "**2022-06-22**  \n",
    "  \n",
    "*Modifications apportées* \n",
    "- Ajouter le tag associé au sous-corpus dans les paths (acteur + path) - je suis pas certaine si j'avais fini, s'il y a une erreur revoir si sous_corpus = False ou s'il faudrait ajouter un if\n",
    "- Convertir les tags de TreeTagger selon les tags utilisés par le Lefff pour les fournir au lemmatiseur (performance meilleure lorsqu'on lui indique la POS)\n",
    "\n",
    "*Modifications à ajouter* \n",
    "- Modifier le lemmatiseur pour lui fournir les POS tags et qu'il performe mieux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece38e9",
   "metadata": {},
   "source": [
    "### **Lire le corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a62415e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4445"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "path = '/Users/camilledemers/Documents/03-corpus/2-sous-corpus/'\n",
    "acteur = 'asso_ordres'\n",
    "sous_corpus = True # Faut peut-être revoir le script pour adapter en fonction de ça (je me rappelle plus si j'avais fini)\n",
    "tag = 'Nursing'\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path + acteur + '/' + tag + \"/\")\n",
    "len(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9679933",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\") and not file.endswith('-corpus_FR.txt') and not 'PDF' in file:\n",
    "        file_path = path + acteur + '/' +  tag + \"/\" + file\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding = \"UTF-8\") as f:\n",
    "                data = f.readlines()\n",
    "                corpus.append(data[1].strip('\\n').lower())\n",
    "\n",
    "        except:\n",
    "            print('Ce fichier-là n\\'a pas pu être lu : ' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4fda380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a un corpus de 4443 documents.\n"
     ]
    }
   ],
   "source": [
    "corpus = corpus[:round(len(corpus))]\n",
    "\n",
    "nb_docs = len(corpus)\n",
    "\n",
    "print(\"On a un corpus de {} documents.\".format(nb_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c732a6",
   "metadata": {},
   "source": [
    "### **Segmentation** (phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a4b16",
   "metadata": {},
   "source": [
    "**NLTK**\\\n",
    "https://www.nltk.org/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31145e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(['popular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb0c1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize \n",
    "\n",
    "sents = [[s.strip('.') for s in sent_tokenize(doc)] for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980c335",
   "metadata": {},
   "source": [
    "### **Tokenisation** (mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5206beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Seulement les caractères alphabétiques\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afbd6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[tokenizer_re.tokenize(s) for s in doc] for doc in sents]\n",
    "\n",
    "len_corpus = len(nltk.flatten(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "047334bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre corpus contient 2905985 tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Notre corpus contient {} tokens.\".format(len_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab9f5e",
   "metadata": {},
   "source": [
    "### **Filtrage** (antidictionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d96dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer l'antidictionnaire pour filtrer les données\n",
    "from pandas import *\n",
    "\n",
    "# Stopwords fréquents en français\n",
    "path = \"/Users/camilledemers/Documents/04-filtrage/stopwords.csv\"\n",
    "with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = read_csv(f)\n",
    "    stopwords = [t.lower() for t in stopwords['Stopwords'].tolist()]\n",
    "\n",
    "\n",
    "# Stopwords fréquents en anglais\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/stop_words_english.txt'\n",
    "with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "    sw = [w.strip('\\n').lower() for w in f.readlines()]\n",
    "\n",
    "stopwords += sw\n",
    "\n",
    "# Signes de ponctuation\n",
    "import string \n",
    "punct = [s for s in string.punctuation] \n",
    "punct += ['»' ,'©', '']\n",
    "\n",
    "stopwords += punct\n",
    "\n",
    "\n",
    "# Mis en commentaire pour l'instant car ça allonge le délai de traitement\n",
    "#Prénoms (curieusement, il y en a beaucoup dans les données)\n",
    "# path = '/Users/camilledemers/Documents/04-filtrage/Prenoms.csv'\n",
    "# with open(path, 'r', encoding='utf-8') as f:\n",
    "#     sw = read_csv(f)\n",
    "#     sw = [str(t).lower() for t in sw['01_prenom'].tolist()]\n",
    "\n",
    "# stopwords += sw\n",
    "\n",
    "#Noms de famille \n",
    "# path = '/Users/camilledemers/Documents/04-filtrage/nomsFamille.csv'\n",
    "# with open(path, 'r', encoding='utf-8') as f:\n",
    "#     sw = read_csv(f)\n",
    "#     sw = [str(t).lower() for t in sw['Nom'].tolist()]\n",
    "\n",
    "# stopwords += sw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0ac5e",
   "metadata": {},
   "source": [
    "### **Filtrage (MWE - stopwords formés de plusieurs tokens)**\n",
    "Surtout pour filtrer les expressions relatives à l'architecture d'information / navigation Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02e6dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/mwe_stopwords.txt'\n",
    "\n",
    "with open (path, 'r', encoding='utf-8') as f:\n",
    "    mwe_sw = [tuple(tokenizer_re.tokenize(t)) for t in f.readlines()]\n",
    "    #mwe_sw = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d08401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer_mwe = MWETokenizer(mwe_sw, separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d3ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_sw = [tokenizer_mwe.tokenize(w)[0] for w in mwe_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddb87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[[t for t in tokenizer_mwe.tokenize(sent) if t not in mwe_sw] for sent in doc] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b5f4d",
   "metadata": {},
   "source": [
    "### **Phrases / N-Grammes (MWE)**\n",
    "https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "addba715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4fc7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import everygrams\n",
    "ngrammes = [[list(everygrams(sent, min_len=2, max_len=4)) for sent in doc] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1d405",
   "metadata": {},
   "source": [
    "### **Filtrage (N-grammes)**\n",
    "\n",
    "On retire les n-grammes qui n'apparaissent qu'une seule fois dans le corpus ou qui débutent ou terminent par :\n",
    "- un stopword\n",
    "- un mot de 1 lettre ou moins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3d97d",
   "metadata": {},
   "source": [
    "Pour le reste du traitement, on arrête de considérer les frontières entre les phrases et entre les documents (nos ngrammes les respectent donc on n'en a plus besoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc4c59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la distribution de fréquence de chaque ngramme dans tout le corpus\n",
    "# Pour ça, on va 'applatir' la liste des ngrammes pour ne plus tenir compte des frontières entre les phrases et entre les documents\n",
    "def ng_flat(ngramme):\n",
    "    liste    = [] \n",
    "    for doc in ngrammes:\n",
    "        for sent in doc:\n",
    "            for ngram in sent:\n",
    "                liste.append(ngram)\n",
    "    return liste\n",
    "\n",
    "ngrammes = ng_flat(ngrammes)\n",
    "freq = FreqDist(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df8b7f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7750458"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a008d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, re\n",
    "\n",
    "ngrammes = [ngram for ngram in ngrammes if freq[ngram] > 30 and \\\n",
    "        not ngram[0] in stopwords and len(ngram[0]) > 1 and not re.search('\\d+', ngram[0])\\\n",
    "        and not ngram[-1] in stopwords and len(ngram[-1]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728174"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7da90ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\" \".join(ngram) for ngram in ngrammes]\n",
    "freq = FreqDist(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be52a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulaire = freq.keys()\n",
    "\n",
    "def tabCSV(tab, titre):\n",
    "    tab = pd.DataFrame(tab.items(), columns= [\"Expression\", \"Fréquence\"])\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "\n",
    "    path = '/Users/camilledemers/Documents/04-filtrage/' + acteur + '/'\n",
    "\n",
    "    if sous_corpus:\n",
    "        path += tag + '/'\n",
    "        file_path = path + acteur + '_' + tag\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        file_path = path \n",
    "\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "    tab.to_csv(path + tag + titre + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3f79a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/' + acteur + '/' + tag + '/'\n",
    "\n",
    "tabCSV(freq, '_n-grams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e83245",
   "metadata": {},
   "source": [
    "### **POS Tagging** (TreeTagger)\n",
    "https://github.com/miotto/treetagger-python/blob/master/README.rst  \n",
    "https://treetaggerwrapper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47508adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilledemers/opt/anaconda3/lib/python3.9/site-packages/treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "/Users/camilledemers/opt/anaconda3/lib/python3.9/site-packages/treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "/Users/camilledemers/opt/anaconda3/lib/python3.9/site-packages/treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "/Users/camilledemers/opt/anaconda3/lib/python3.9/site-packages/treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49585685",
   "metadata": {},
   "source": [
    "### **Mapping POS Tags** (FRMG)\n",
    "\n",
    "Pour utiliser adéquatement notre lemmatiseur par la suite (FrenchLefffLemmatizer), on va mapper les étiquettes morphosyntaxiques du TreeTagger à celles que prend le lemmatiseur (celles issues de FRMG)\n",
    "\n",
    "http://alpage.inria.fr/frmgwiki/content/tagset-frmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13d76bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/mapping_treeTagger_lefff.csv'\n",
    "\n",
    "with open(path) as f:\n",
    "    csv = read_csv(f)\n",
    "\n",
    "treeTag = [term for term in csv['TreeTagger'].tolist()] \n",
    "lefff = [term for term in csv['Lefff'].tolist()]\n",
    "\n",
    "mapping = {term : lefff[treeTag.index(term)] for term in treeTag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b75e8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammes = [\" \".join(ngramme) for ngramme in ngrammes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20ab5679",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_extrant = [[ngram, \" \".join([mapping[t.split('\\t')[1]] for t in tagger.tag_text(ngram)])] for ngram in ngrammes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90c4d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = [[term[0].split(), term[1]] for term in tagged_extrant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67e0b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542ec53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d386aa5c",
   "metadata": {},
   "source": [
    "### **Filtrage (Patrons syntaxiques)**  \n",
    "Lossio-Ventura, J. A., Jonquet, C., Roche, M., & Teisseire, M. (2014). Biomedical Terminology Extraction : A new combination of Statistical and Web Mining Approaches. 421. https://hal-lirmm.ccsd.cnrs.fr/lirmm-01056598\n",
    "\n",
    "On veut aller extraire les structures syntaxiques les plus courantes dans les MeSH pour filtrer notre corpus selon celles-ci (inspiré de la méthodologie de l'article ci-dessus ; voir le Notebook *Mesh_extract.ipynb*). Pour ce faire, nous allons donc ne sélectionner que les ngrammes qui y correspondent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dedf95c",
   "metadata": {},
   "source": [
    "**Reprendre ici pour arranger le lemmatiseur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15945967",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/MeSH/mesh_patterns-fr.csv'\n",
    "\n",
    "with open (path, 'r') as f:\n",
    "    patterns = read_csv(f)\n",
    "    patterns = patterns['Structure'].tolist()[:50] # Pour prendre seulement les 200 structures syntaxiques les plus fréquentes dans les MeSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1e03045",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [t for t in tagged if t[1] in patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b74b50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/' + acteur + '/' + tag + '/'\n",
    "tab = pd.DataFrame(terms, columns= [\"Expression\", \"Structure syntaxique\"])\n",
    "tab = pd.DataFrame(tab.groupby([\"Expression\", \"Structure syntaxique\"]).size().reset_index(name=\"Fréquence\"))\n",
    "tab.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "\n",
    "                    \n",
    "tab.to_csv(path + tag + '_phrases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb27e72",
   "metadata": {},
   "source": [
    "### **Lemmatisation** (FrenchLefffLemmatizer)\n",
    "\n",
    "https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2580b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2916a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = FrenchLefffLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "736a6ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aide financière', 'nc adj']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "806b4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [[term[0].split(), term[1].split()] for term in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "930d50ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aide', 'financière'], ['nc', 'adj']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e12dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tuples[0]\n",
    "test = [t.split() for t in test]\n",
    "test test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/Users/camilledemers/Documents/04-filtrage/' + tag + '/'\n",
    "#tab = pd.DataFrame(tuples_lemmes, columns= [\"Expression\", \"Variante\", \"Structure syntaxique\"])\n",
    "#tab = pd.DataFrame(tab.groupby([\"Expression\", \"Structure syntaxique\"]).size().reset_index(name=\"Fréquence\"))\n",
    "#tab.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "#tab.to_csv(path + tag + '_phrases_lemmatized-TreeTagger.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ef6e6",
   "metadata": {},
   "source": [
    "### **KWIC (Keyword in Context)**\n",
    "Termes d'intérêt : \n",
    "- « Programme »\n",
    "- « Plan »\n",
    "- « Service(s) de » \n",
    "- « Intervenant(e) en »\n",
    "- « Professionnel de »\n",
    "- « Institut (du/de) »\n",
    "- « Groupe de recherche en »\n",
    "- « Personne »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans notre cas on veut que ça débute par le mot-clé donc le contexte est un peu plus simple\n",
    "# penser à généraliser avec des expressions régulières \n",
    "\n",
    "kw = ['programme', 'plan', 'service', 'intervenant', 'institut', 'groupe de recherche', 'personne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c50b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\" \".join(ngram) for ngram in ngrammes]\n",
    "extrant = pd.DataFrame(columns=['Mot-clé','Concordance', 'Fréquence'])\n",
    "kwic = {w : [] for w in kw} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255df07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in phrases: # on pourrait aussi chercher dans les terms, mais on perd certains termes d'intérêt avec le filtrage syntaxique\n",
    "    for w in kw:\n",
    "        if t.startswith(w):\n",
    "            kwic[w].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic = {term: FreqDist(kwic[term]) for term in kwic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in kw:\n",
    "    df = pd.DataFrame(kwic[term].items(), columns=['Concordance', \"Fréquence\"])\n",
    "    df.sort_values([\"Fréquence\"], \n",
    "        axis=0,\n",
    "        ascending=[False], \n",
    "        inplace=True)\n",
    "\n",
    "    df.insert(0, 'Mot-clé', term)\n",
    "    extrant = pd.concat([extrant, df])\n",
    "\n",
    "path = '/Users/camilledemers/Documents/04-filtrage' + '/' + tag + '/'\n",
    "extrant.to_csv(path + tag + '_KWIC' +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45949eb3",
   "metadata": {},
   "source": [
    "### **Extraction de termes MeSH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/MeSH/mesh-fr.txt'\n",
    "\n",
    "with open (path, 'r', encoding='utf-8') as f:\n",
    "    mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "    tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "    mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "    mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "    #mesh = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_mesh = tokenizer_mesh.tokenize(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = []\n",
    "\n",
    "for t in extr_mesh:\n",
    "    if t in mesh:\n",
    "        termes_mesh.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = FreqDist(termes_mesh)\n",
    "tabCSV(termes_mesh, '_MeSH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685131ee",
   "metadata": {},
   "source": [
    "### **Extraction de termes SNOMED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/SNOMED/SNOMED_fr.csv'\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    sm = read_csv(f, sep=';')\n",
    "    sm = list(dict.fromkeys([str(t).strip().lower() for t in sm['term'].tolist()]))\n",
    "\n",
    "    sm = [tuple(tokenizer_re.tokenize(w)) for w in sm if len(w.split()) > 1]\n",
    "    tokenizer_sm = MWETokenizer(sm, separator = ' ')\n",
    "\n",
    "    sm = [tokenizer_sm.tokenize(w)[0].lower() for w in sm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_sm = tokenizer_sm.tokenize(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = []\n",
    "\n",
    "for t in extr_sm:\n",
    "    if t in sm:\n",
    "        termes_sm.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = FreqDist(termes_sm)\n",
    "tabCSV(termes_sm, '_SNOMED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d06dc",
   "metadata": {},
   "source": [
    "### **Lemmatisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "lemmatizer = FrenchLefffLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54863447",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.tag_text('spéciaux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('spéciaux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee15f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('spéciaux', 'adj') # Il faut aller mapper les étiquettes du Tree Tagger avec ceux que reçoit le lemmatiseur pour améliorer sa performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d4f00",
   "metadata": {},
   "source": [
    "**Tagset**  \n",
    "http://alpage.inria.fr/frmgwiki/content/tagset-frmg\n",
    "- 'adj'| Adjectif\n",
    "- 'n'\n",
    "- 'v'\n",
    "- 'det'\n",
    "- 'adv'\n",
    "- 'prep'\n",
    "- 'pro'\n",
    "- 'np' | Nom propre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c7cd3",
   "metadata": {},
   "source": [
    "### **Filtrage (fréquence)**\n",
    "Pour la suite du traitement, on ne retient que les N expressions les plus fréquentes dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_terms = FreqDist(terms)\n",
    "exp_freq = freq_terms.most_common()\n",
    "\n",
    "path = '/Users/camilledemers/Documents/05-transformation/' + tag + '/'\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_path = path + tag + '_terms.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.DataFrame([t[0] for t in exp_freq], columns=[\"Terme\"])\n",
    "tab.to_csv(file_path)\n",
    "\n",
    "print(\"Fini pour le corpus suivant : {}\".format(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3bacec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a33210152f7d2bd255fb16656f372b633dbf298ed202bbbac20290b0375cadb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
