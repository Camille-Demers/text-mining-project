{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c662c7d",
   "metadata": {},
   "source": [
    "## **2. Prétraitement**\n",
    "- Segmentation (phrases)\n",
    "- Tokenization (mots)\n",
    "- Filtrage (stopwords)\n",
    "- Extraction de termes complexes (MWE / n-grammes / segments répétés)\n",
    "- Étiquetage morphosyntaxique (POS Tagging) \n",
    "- Lemmatisation\n",
    "- Chunking / Filtrage par patrons syntaxiques (basés sur les patrons fréquents dans les MeSH)\n",
    "- Extraction de concordances (KWIC) pour un ensemble de mots-clés d'intérêt\n",
    "- Extraction de termes MeSH et SNOMED présents dans les données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93fdd02",
   "metadata": {},
   "source": [
    "**Précisions sur la structuration des différents objets créés (corpus, sents, tokens, ngrammes, tagged)**  \n",
    "| Index dans la structure      | Information représentée  |\n",
    "| ----------- | ----------- |\n",
    "| `x[0]`      | 1 document dans le corpus (`corpus`)       |\n",
    "| `x[0][0]`   | 1 phrase dans un document du corpus (`sents`) |\n",
    "| `x[0][0][0]`    | 1 ngramme dans un document (`tokens` / `ngrammes`)        |\n",
    "| `x[0][0][0]`   | 1 tuple (ngramme, patron syntaxique) dans un ngramme (`tuples`) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604b66c",
   "metadata": {},
   "source": [
    "**2022-06-17**  \n",
    "  \n",
    "*Modifications apportées*\n",
    "- Traiter un ensemble de documents .txt plutôt qu'un seul gros corpus ✓\n",
    "- Retenir seulement les termes qui ont une fréquence supérieure à x ✓\n",
    "- Rammener l'étiquetage morphosyntaxique et le filtrage par patrons syntaxiques plus tôt afin de filtrer les KWIC ✓\n",
    "\n",
    "*Modifications suivantes*:  \n",
    "- Convertir les tags de TreeTagger selon les tags utilisés par le Lefff pour les fournir au lemmatiseur (performance meilleure lorsqu'on lui indique la POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece38e9",
   "metadata": {},
   "source": [
    "### **Lire le corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "0a62415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = '/Users/camilledemers/Documents/03-corpus/2-data/1-fr/'\n",
    "acteur = 'inspq'\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path + acteur + \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d9679933",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\") and not file.endswith('-corpus_FR.txt') and not 'PDF' in file:\n",
    "        file_path = path + acteur + \"/\" + file\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding = \"UTF-8\") as f:\n",
    "                data = f.readlines()\n",
    "                corpus.append(data[1].strip('\\n').lower())\n",
    "\n",
    "        except:\n",
    "            print('Ce fichier-là n\\'a pas pu être lu : ' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a4fda380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a un corpus de 217 documents.\n"
     ]
    }
   ],
   "source": [
    "corpus = corpus[:round(len(corpus)/20)]\n",
    "\n",
    "nb_docs = len(corpus)\n",
    "\n",
    "print(\"On a un corpus de {} documents.\".format(nb_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c732a6",
   "metadata": {},
   "source": [
    "### **Segmentation** (phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a4b16",
   "metadata": {},
   "source": [
    "**NLTK**\\\n",
    "https://www.nltk.org/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "31145e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(['popular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "bb0c1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize \n",
    "\n",
    "sents = [[s.strip('.') for s in sent_tokenize(doc)] for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980c335",
   "metadata": {},
   "source": [
    "### **Tokenisation** (mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "5206beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Seulement les caractères alphabétiques\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "afbd6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[tokenizer_re.tokenize(s) for s in doc] for doc in sents]\n",
    "\n",
    "len_corpus = len(nltk.flatten(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047334bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre corpus contient 162288 tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Notre corpus contient {} tokens.\".format(len_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab9f5e",
   "metadata": {},
   "source": [
    "### **Filtrage** (antidictionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer l'antidictionnaire pour filtrer les données\n",
    "from pandas import *\n",
    "\n",
    "# Stopwords fréquents en français\n",
    "path = \"/Users/camilledemers/Documents/04-filtrage/stopwords.csv\"\n",
    "with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = read_csv(f)\n",
    "    stopwords = [t.lower() for t in stopwords['Stopwords'].tolist()]\n",
    "\n",
    "\n",
    "# Stopwords fréquents en anglais\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/stop_words_english.txt'\n",
    "with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "    sw = [w.strip('\\n').lower() for w in f.readlines()]\n",
    "\n",
    "stopwords += sw\n",
    "\n",
    "# Signes de ponctuation\n",
    "import string \n",
    "punct = [s for s in string.punctuation] \n",
    "punct += ['»' ,'©', '']\n",
    "\n",
    "stopwords += punct\n",
    "\n",
    "\n",
    "# Mis en commentaire pour l'instant car ça allonge le délai de traitement\n",
    "#Prénoms (curieusement, il y en a beaucoup dans les données)\n",
    "# path = '/Users/camilledemers/Documents/04-filtrage/Prenoms.csv'\n",
    "# with open(path, 'r', encoding='utf-8') as f:\n",
    "#     sw = read_csv(f)\n",
    "#     sw = [str(t).lower() for t in sw['01_prenom'].tolist()]\n",
    "\n",
    "# stopwords += sw\n",
    "\n",
    "#Noms de famille \n",
    "# path = '/Users/camilledemers/Documents/04-filtrage/nomsFamille.csv'\n",
    "# with open(path, 'r', encoding='utf-8') as f:\n",
    "#     sw = read_csv(f)\n",
    "#     sw = [str(t).lower() for t in sw['Nom'].tolist()]\n",
    "\n",
    "# stopwords += sw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0ac5e",
   "metadata": {},
   "source": [
    "### **Filtrage (MWE - stopwords formés de plusieurs tokens)**\n",
    "Surtout pour filtrer les expressions relatives à l'architecture d'information / navigation Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/mwe_stopwords.txt'\n",
    "\n",
    "with open (path, 'r', encoding='utf-8') as f:\n",
    "    mwe_sw = [tuple(tokenizer_re.tokenize(t)) for t in f.readlines()]\n",
    "    #mwe_sw = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer_mwe = MWETokenizer(mwe_sw, separator=' ')\n",
    "\n",
    "mwe_sw = [tokenizer_mwe.tokenize(w)[0] for w in mwe_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[[t for t in tokenizer_mwe.tokenize(sent) if t not in mwe_sw] for sent in doc] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b5f4d",
   "metadata": {},
   "source": [
    "### **Phrases / N-Grammes (MWE)**\n",
    "https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addba715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import everygrams\n",
    "ngrammes = [[list(everygrams(sent, min_len=2, max_len=8)) for sent in doc] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1d405",
   "metadata": {},
   "source": [
    "### **Filtrage (N-grammes)**\n",
    "\n",
    "On retire les n-grammes qui n'apparaissent qu'une seule fois dans le corpus ou qui débutent ou terminent par :\n",
    "- un stopword\n",
    "- un mot de 1 lettre ou moins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3d97d",
   "metadata": {},
   "source": [
    "Pour le reste du traitement, on arrête de considérer les frontières entre les phrases et entre les documents (nos ngrammes les respectent donc on n'en a plus besoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la distribution de fréquence de chaque ngramme dans tout le corpus\n",
    "# Pour ça, on va 'applatir' la liste des ngrammes pour ne plus tenir compte des frontières entre les phrases et entre les documents\n",
    "def ng_flat(ngramme):\n",
    "    liste    = [] \n",
    "    for doc in ngrammes:\n",
    "        for sent in doc:\n",
    "            for ngram in sent:\n",
    "                liste.append(ngram)\n",
    "    return liste\n",
    "\n",
    "ngrammes = ng_flat(ngrammes)\n",
    "freq = FreqDist(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b7f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014939"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a008d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, re\n",
    "\n",
    "ngrammes = [ngram for ngram in ngrammes if freq[ngram] > 10 and \\\n",
    "        not ngram[0] in stopwords and len(ngram[0]) > 1 and not re.search('^\\d+$', ngram[0])\\\n",
    "        and not ngram[-1] in stopwords and len(ngram[-1]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218940"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da90ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\" \".join(ngram) for ngram in ngrammes]\n",
    "freq = FreqDist(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulaire = freq.keys()\n",
    "\n",
    "def tabCSV(tab, titre):\n",
    "    tab = pd.DataFrame(tab.items(), columns= [\"Expression\", \"Fréquence\"])\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "    path = '/Users/camilledemers/Documents/04-filtrage/' + acteur + '/'\n",
    "    tab.to_csv(path + acteur + titre + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f79a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabCSV(freq, '_n-grams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e83245",
   "metadata": {},
   "source": [
    "### **POS Tagging**\n",
    "https://github.com/miotto/treetagger-python/blob/master/README.rst  \n",
    "https://treetaggerwrapper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47508adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = [[tagger.tag_text(term)[0].split('\\t') for term in ngram] for ngram in ngrammes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d9515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['emplois', 'NOM', 'emploi'],\n",
       " ['english', 'NOM', 'english'],\n",
       " ['inesss', 'NOM', 'inesss']]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386aa5c",
   "metadata": {},
   "source": [
    "### **Filtrage (Patrons syntaxiques)**  \n",
    "Lossio-Ventura, J. A., Jonquet, C., Roche, M., & Teisseire, M. (2014). Biomedical Terminology Extraction : A new combination of Statistical and Web Mining Approaches. 421. https://hal-lirmm.ccsd.cnrs.fr/lirmm-01056598\n",
    "\n",
    "On veut aller extraire les structures syntaxiques les plus courantes dans les MeSH pour filtrer notre corpus selon celles-ci (inspiré de la méthodologie de l'article ci-dessus ; voir le Notebook *Mesh_extract.ipynb*). Pour ce faire, nous allons donc ne sélectionner que les ngrammes qui y correspondent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15945967",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/MeSH/mesh_patterns-fr.csv'\n",
    "\n",
    "with open (path, 'r') as f:\n",
    "    patterns = read_csv(f)\n",
    "    patterns = patterns['Structure'].tolist()#[:250]  Pour prendre seulement les 250 structures syntaxiques les plus fréquentes dans les MeSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [] \n",
    "tuples_lemmes = []\n",
    "\n",
    "for ngram in tagged:\n",
    "    exp = \" \".join([t[0] for t in ngram])\n",
    "    chunk = \" \".join([t[1] for t in ngram])\n",
    "    lemme = \" \".join([t[2] for t in ngram])\n",
    "    tuples.append([exp, chunk])\n",
    "    tuples_lemmes.append([lemme, exp, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f918e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [t for t in tuples if t[1] in patterns]\n",
    "terms = [t[0] for t in tuples if t[1] in patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/' + acteur + '/'\n",
    "tab = pd.DataFrame(chunks, columns= [\"Expression\", \"Structure syntaxique\"])\n",
    "tab = pd.DataFrame(tab.groupby([\"Expression\", \"Structure syntaxique\"]).size().reset_index(name=\"Fréquence\"))\n",
    "tab.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "tab.to_csv(path + acteur + '_phrases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb27e72",
   "metadata": {},
   "source": [
    "### **Lemmatisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/camilledemers/Documents/04-filtrage/' + acteur + '/'\n",
    "tab = pd.DataFrame(tuples_lemmes, columns= [\"Expression\", \"Variante\", \"Structure syntaxique\"])\n",
    "tab = pd.DataFrame(tab.groupby([\"Expression\", \"Structure syntaxique\"]).size().reset_index(name=\"Fréquence\"))\n",
    "tab.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "tab.to_csv(path + acteur + '_phrases_lemmatized-TreeTagger.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ef6e6",
   "metadata": {},
   "source": [
    "### **KWIC (Keyword in Context)**\n",
    "Termes d'intérêt : \n",
    "- « Programme »\n",
    "- « Service(s) de » \n",
    "- « Intervenant(e) en »\n",
    "- « Professionnel de »\n",
    "- « Institut (du/de) »\n",
    "- « Groupe de recherche en »\n",
    "- « Personne »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans notre cas on veut que ça débute par le mot-clé donc le contexte est un peu plus simple\n",
    "# penser à généraliser avec des expressions régulières \n",
    "\n",
    "kw = ['programme', 'service', 'intervenant', 'institut', 'groupe de recherche', 'personne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c50b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\" \".join(ngram) for ngram in ngrammes]\n",
    "extrant = pd.DataFrame(columns=['Mot-clé','Concordance', 'Fréquence'])\n",
    "kwic = {w : [] for w in kw} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255df07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in phrases: # on pourrait aussi chercher dans les terms, mais on perd certains termes d'intérêt avec le filtrage syntaxique\n",
    "    for w in kw:\n",
    "        if t.startswith(w):\n",
    "            kwic[w].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic = {term: FreqDist(kwic[term]) for term in kwic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in kw:\n",
    "    df = pd.DataFrame(kwic[term].items(), columns=['Concordance', \"Fréquence\"])\n",
    "    df.sort_values([\"Fréquence\"], \n",
    "        axis=0,\n",
    "        ascending=[False], \n",
    "        inplace=True)\n",
    "\n",
    "    df.insert(0, 'Mot-clé', term)\n",
    "    extrant = pd.concat([extrant, df])\n",
    "\n",
    "path = '/Users/camilledemers/Documents/04-filtrage' + '/' + acteur + '/'\n",
    "extrant.to_csv(path + acteur + '_KWIC' +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45949eb3",
   "metadata": {},
   "source": [
    "### **Extraction de termes MeSH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/MeSH/mesh-fr.txt'\n",
    "\n",
    "with open (path, 'r', encoding='utf-8') as f:\n",
    "    mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "    tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "    mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "    mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "    #mesh = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_mesh = tokenizer_mesh.tokenize(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = []\n",
    "\n",
    "for t in extr_mesh:\n",
    "    if t in mesh:\n",
    "        termes_mesh.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = FreqDist(termes_mesh)\n",
    "tabCSV(termes_mesh, '_MeSH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685131ee",
   "metadata": {},
   "source": [
    "### **Extraction de termes SNOMED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "path = '/Users/camilledemers/Documents/04-filtrage/SNOMED/SNOMED_fr.csv'\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    sm = read_csv(f, sep=';')\n",
    "    sm = list(dict.fromkeys([str(t).strip().lower() for t in sm['term'].tolist()]))\n",
    "\n",
    "    sm = [tuple(tokenizer_re.tokenize(w)) for w in sm if len(w.split()) > 1]\n",
    "    tokenizer_sm = MWETokenizer(sm, separator = ' ')\n",
    "\n",
    "    sm = [tokenizer_sm.tokenize(w)[0].lower() for w in sm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_sm = tokenizer_sm.tokenize(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = []\n",
    "\n",
    "for t in extr_sm:\n",
    "    if t in sm:\n",
    "        termes_sm.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = FreqDist(termes_sm)\n",
    "tabCSV(termes_sm, '_SNOMED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d06dc",
   "metadata": {},
   "source": [
    "### **Lemmatisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "lemmatizer = FrenchLefffLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54863447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spéciaux\\tADJ\\tspécial']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_text('spéciaux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13fc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spéciaux'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('spéciaux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee15f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spécial', 'adj')]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('spéciaux', 'adj') # Il faut aller mapper les étiquettes du Tree Tagger avec ceux que reçoit le lemmatiseur pour améliorer sa performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d4f00",
   "metadata": {},
   "source": [
    "**Tagset**  \n",
    "http://alpage.inria.fr/frmgwiki/content/tagset-frmg\n",
    "- 'adj'| Adjectif\n",
    "- 'n'\n",
    "- 'v'\n",
    "- 'det'\n",
    "- 'adv'\n",
    "- 'prep'\n",
    "- 'pro'\n",
    "- 'np' | Nom propre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c7cd3",
   "metadata": {},
   "source": [
    "### **Filtrage (fréquence)**\n",
    "Pour la suite du traitement, on ne retient que les N expressions les plus fréquentes dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_terms = FreqDist(terms)\n",
    "exp_freq = freq_terms.most_common()\n",
    "\n",
    "path = '/Users/camilledemers/Documents/05-transformation/' + acteur + '/'\n",
    "file_path = path + acteur + '_terms.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.DataFrame([t[0] for t in exp_freq], columns=[\"Terme\"])\n",
    "tab.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84736575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a33210152f7d2bd255fb16656f372b633dbf298ed202bbbac20290b0375cadb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
