{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Pondération statistique** (TF-IDF)  \n",
    "\n",
    "https://stackoverflow.com/questions/46580932/calculate-tf-idf-using-sklearn-for-n-grams-in-python  \n",
    "http://scikit-learn.sourceforge.net/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn-feature-extraction-text-tfidfvectorizer  \n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../05-transformation/'\n",
    "acteur = 'ophq'\n",
    "sous_corpus = False\n",
    "tag = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lire le vocabulaire** (termes retenus au prétraitement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "\n",
    "if sous_corpus:\n",
    "    file_path = path + acteur + '/' + tag + '/' + tag + '_vocab.csv' # si on veut les termes lemmatisés : -lemmatized.csv\n",
    "\n",
    "else:\n",
    "    file_path = path + acteur + '/' + acteur + '_vocab.csv' # si on veut les termes lemmatisés : -lemmatized.csv\n",
    "    \n",
    "with open(file_path) as f:\n",
    "    csv = read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulaire = [t.lower() for t in list(csv['Terme'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('On a un vocabulaire de {} formes.'.format(len(vocabulaire)))\n",
    "vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lire le corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, re\n",
    "from pathlib import Path\n",
    "from os import path\n",
    "from pandas import *\n",
    "\n",
    "# Change the directory\n",
    "if sous_corpus:\n",
    "    base_path = '../03-corpus/2-sous-corpus/'\n",
    "    base_path = path.join(base_path, acteur, tag) \n",
    "\n",
    "else: \n",
    "    base_path = '../03-corpus/2-data/1-fr/'\n",
    "    base_path = path.join(base_path, acteur + '.csv')\n",
    "        \n",
    "with open(base_path, \"r\", encoding = \"UTF-8\") as f:\n",
    "    data = read_csv(base_path)\n",
    "    text = data['text'].tolist()\n",
    "    corpus = [(re.sub('\\d', '', t.strip('\\n').lower().replace('’', '\\''))) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus[:round(len(corpus))]\n",
    "\n",
    "nb_docs = len(corpus)\n",
    "\n",
    "print(\"On a donc un corpus de {} documents.\".format(nb_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Appliquer le prétraitement**\n",
    "Si les termes passées comme vocabulaire sont lemmatisés, changer le paramètre lem pour True au moment d'appliquer la fonction nlp(corpus)  \n",
    "Le TfIdfVectorizer de sklearn va extraire lui-mêmes les ngrammes, faire le filtrage des mots fonctionnels et calculer le tf-idf pour nos termes d'intérêt ;  \n",
    "Or, si les termes qu'on lui donne comme vocabulaire ont été lemmatisés, on veut donc aussi lui passer un corpus lemmatisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "def nlp(corpus, lem=False): \n",
    "    if not lem:\n",
    "        # Tokenisation\n",
    "        tokenizer = RegexpTokenizer(r\"\\w\\'|\\w+\")\n",
    "\n",
    "        tokens = [tokenizer.tokenize(doc) for  doc in corpus]\n",
    "        len_corpus = len(nltk.flatten(tokens))\n",
    "        print(\"Avec le RegExpTokenizer, notre corpus contient {} tokens.\".format(len_corpus))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    else:\n",
    "        # POS tagging\n",
    "        input = [\" \".join(nltk.flatten(doc)).replace(\"' \", \"'\") for doc in tokens]\n",
    "        import treetaggerwrapper\n",
    "        tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')\n",
    "\n",
    "        path = '../04-filtrage/mapping_treeTagger_lefff.csv'\n",
    "\n",
    "        with open(path) as f:\n",
    "            csv = read_csv(f)\n",
    "\n",
    "        treeTag = [term for term in csv['TreeTagger'].tolist()] \n",
    "        lefff = [term for term in csv['Lefff'].tolist()]\n",
    "\n",
    "        mapping = {term : lefff[treeTag.index(term)] for term in treeTag}\n",
    "\n",
    "        tagged= [tagger.tag_text(doc) for doc in input]\n",
    "\n",
    "        tuples_doc = []\n",
    "        for doc in tagged:\n",
    "            tuples = []\n",
    "            for t in doc:\n",
    "                token = t.split('\\t')[0]\n",
    "                pos = mapping[t.split('\\t')[1]]\n",
    "\n",
    "                tuples.append([token, pos])\n",
    "            tuples_doc.append(tuples)\n",
    "\n",
    "        #Lemmatisation\n",
    "        lemmatizer = FrenchLefffLemmatizer()\n",
    "        docs_lemmas = []\n",
    "\n",
    "        for doc in tuples_doc:\n",
    "            doc_lemma = []\n",
    "            for t in doc:\n",
    "                term_lemmatized = \"\"\n",
    "                if(lemmatizer.lemmatize(t[0], t[1]) == []):\n",
    "                    term_lemmatized = lemmatizer.lemmatize(t[0])\n",
    "                else:\n",
    "                    term_lemmatized = lemmatizer.lemmatize(t[0], t[1])[0][0] # [0][0] pour avoir le lemme seul et non (lemme, pos)\n",
    "            \n",
    "                if len(term_lemmatized) >1 :\n",
    "                    doc_lemma.append(term_lemmatized)\n",
    "            docs_lemmas.append(doc_lemma)\n",
    "\n",
    "        docs_lemmas = [\" \".join(doc) for doc in docs_lemmas]\n",
    "\n",
    "        return docs_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nlp(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/mwe_stopwords.txt'\n",
    "\n",
    "with open (file_path, 'r', encoding='utf-8') as f:\n",
    "    mwe_sw = [t.lower().strip('\\n') for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# max_df : ignore words that appear in 85% of documents, \n",
    "# min df:  ignore words that appear in less than 1% of documents \n",
    "# vocabulary = vocabulaire\n",
    "\n",
    "# Sans utiliser le vocabulaire\n",
    "# tfidf = TfidfVectorizer(min_df=0.1, stop_words=None, ngram_range=(2,4), max_df=0.85, use_idf=True, max_features=200)\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "# vocabulary = vocabulaire\n",
    "tfidf = TfidfVectorizer(vocabulary = vocabulaire, tokenizer=identity_tokenizer, ngram_range=(2,4), use_idf=True, lowercase=False, stop_words= mwe_sw)\n",
    "tfs = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = tfidf.get_feature_names_out()\n",
    "corpus_index = [corpus.index(n) for n in corpus]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tfs.T.todense(), index=features_names, columns=corpus_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_path = '../05-transformation/' + acteur + '/'\n",
    "Path(base_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if sous_corpus:\n",
    "    path = path + tag + '/'\n",
    "    titre = tag\n",
    "\n",
    "else:\n",
    "    titre = acteur\n",
    "\n",
    "df.to_csv(base_path + titre + '_matrice-TFIDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_weighted = []\n",
    "rows, cols = tfs.nonzero()\n",
    "for row, col in zip(rows,cols):\n",
    "    terms_weighted.append([features_names[col], tfs[row,col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_weighted = DataFrame(terms_weighted, columns=['Terme', 'TF-IDF'])\n",
    "terms_weighted.sort_values([\"TF-IDF\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termes= set(features_names)\n",
    "\n",
    "liste_filtre = {term:0 for term in termes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in liste_filtre:\n",
    "    for score in terms_weighted:\n",
    "        max = terms_weighted[terms_weighted['Terme'] == term].max()['TF-IDF']\n",
    "        liste_filtre[term] = max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_tries = pd.DataFrame(liste_filtre.items(), columns=['Terme', 'TF-IDF'])\n",
    "termes_tries.sort_values([\"TF-IDF\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "\n",
    "termes_tries.to_csv(base_path + titre + '_weighting_TF-IDF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OKapi BM25**\n",
    "https://hal.archives-ouvertes.fr/hal-00760158 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rank_bm25 import BM25Okapi\n",
    "# bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    def __init__(self, b=0.75, k1=1.6):\n",
    "        self.vectorizer = TfidfVectorizer(norm=None, smooth_idf=False)\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Fit IDF to documents X \"\"\"\n",
    "        self.vectorizer.fit(X)\n",
    "        y = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        self.avdl = y.sum(1).mean()\n",
    "\n",
    "    def transform(self, q, X):\n",
    "        \"\"\" Calculate BM25 between query q and documents X \"\"\"\n",
    "        b, k1, avdl = self.b, self.k1, self.avdl\n",
    "\n",
    "        # apply CountVectorizer\n",
    "        X = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        len_X = X.sum(1).A1\n",
    "        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
    "        assert sparse.isspmatrix_csr(q)\n",
    "\n",
    "        # convert to csc for better column slicing\n",
    "        X = X.tocsc()[:, q.indices]\n",
    "        denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n",
    "        # idf(t) = log [ n / df(t) ] + 1 in sklearn, so it need to be coneverted\n",
    "        # to idf(t) = log [ n / df(t) ] with minus 1\n",
    "        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
    "        numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)                                                          \n",
    "        return (numer / denom).sum(1).A1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33210152f7d2bd255fb16656f372b633dbf298ed202bbbac20290b0375cadb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
