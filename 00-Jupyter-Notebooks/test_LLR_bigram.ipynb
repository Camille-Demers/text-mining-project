{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log likelihood ratio - Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lire le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom, chi2\n",
    "\n",
    "import shutil, re\n",
    "from os import listdir, chdir, path\n",
    "from pathlib import Path\n",
    "\n",
    "acteurs = ['asso_ordres', 'chsld', 'chu_iu', 'cisss_ciusss', 'cliniques_medicales', 'csbe', 'gmf', 'inesss', 'inspq', 'msss', 'ophq', 'quebec_sante', 'ramq', 'sante_mtl', 'urgence_sante']\n",
    "acteur = 'chum'\n",
    "sous_corpus = False \n",
    "tag = ''\n",
    "\n",
    "# Change the directory\n",
    "base_path = '/Users/camilledemers/Documents/03-corpus/2-data/1-fr/'\n",
    "file_path = path.join(base_path, acteur) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "with open(file_path, \"r\", encoding = \"UTF-8\") as f:\n",
    "        data = read_csv(file_path)\n",
    "        text = data['text'].tolist()\n",
    "        corpus = [(re.sub('\\d', '', t.strip('\\n').lower().replace('’', '\\''))) for t in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraire des bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(['popular'])\n",
    "from nltk import sent_tokenize \n",
    "sents = [[s.strip('.') for s in sent_tokenize(doc)] for doc in corpus]\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Seulement les caractères alphabétiques\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w\\'|\\w+\")\n",
    "\n",
    "tokens = nltk.flatten([[tokenizer_re.tokenize(s) for s in doc] for doc in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = list(bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords fréquents en français (non lemmatisés)\n",
    "file_path = \"/Users/camilledemers/Documents/04-filtrage/stopwords.txt\"\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "    stopwords += [\"l'\", \"d'\", \"s'\"]\n",
    "\n",
    "\n",
    "bg = [b for b in bg if not b[0] in stopwords and not b[1] in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_bg = nltk.FreqDist(bg)\n",
    "\n",
    "tab = pd.DataFrame(fd_bg.items(), columns=[\"Bigramme\",\"Fréquence\"])\n",
    "tab.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "\n",
    "tab.to_csv('/Users/camilledemers/Documents/00-Jupyter-Notebooks/test_bigrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood_ratio(c1, c2, c12, N):\n",
    "    \"\"\"\n",
    "    Compute the ratio of two hypotheses of likelihood and return the ratio.\n",
    "    \n",
    "    Under the Independence hypothesis (H0) we assume that there is \n",
    "    no association between w1 and w2, i.e. they are independent: \n",
    "    let P(w1) and P(w2) be probabilities that \n",
    "    a random token in a text is w1 and w2 respectfully and \n",
    "    P(w1,w2) is the probability that (w1,w2) occur together in the text \n",
    "    (i.e. one follows another) so under H0, P(w1,w2) = P(w1)P(w2)\n",
    "    we can compute the observed probability of P(w1,w2) \n",
    "    and compare it with the probability under H0\n",
    "    if these probabilities are significantly different from each other,\n",
    "    then (w1,w2) is a collocation.\n",
    "    \n",
    "    The formula here and test verification values are taken from \n",
    "    Manning & Schūtze _Foundations of Statistical Natural Language Processing_ p.172-175\n",
    "    \n",
    "    Parameters:\n",
    "    c1: count of word 1\n",
    "    c2: count of word 2\n",
    "    c12: count of bigram (w1, w2)\n",
    "    N: the number of words in the corpus\n",
    "    \n",
    "    The value: -2 * loglikelihood_ratio is asymptotically Chi-squared distributed \n",
    "    so we can use Chi-squared table values to test the Null-Hypothesis\n",
    "    against the second Hypothesis--the observed values--as represented by the ratio.\n",
    "    \n",
    "    The following example is taken from Manning and Schütze _Statistical NLP_ p.107\n",
    "    # 1990 NYT data\n",
    "    >>> N = 14_307_668\n",
    "    >>> c1 = 932 # powerful\n",
    "    >>> c2 = 934 # computers\n",
    "    >>> c12 = 10 # bigram count \n",
    "    >>> res = -2 * loglikelihood_ratio(c1, c2, c12, N)\n",
    "    >>> res \n",
    "    82.37586050140558\n",
    "    >>> # e.g. for a alpha value of: 0.005, \n",
    "    >>> # 1 Degree of freedom requires a chi-squared value of 7.88\n",
    "    >>> from scipy.stats import chi2\n",
    "    >>> print ('p-value: %.30f' % chi2.sf(res, 1)) # 1 degrees of freedom\n",
    "    p-value: 0.000000000000000000112519634099\n",
    "    \n",
    "    \"\"\"\n",
    "    p = c2 / N\n",
    "    p1 = c12 / c1\n",
    "    p2 = (c2 - c12) / (N - c1)   \n",
    "    # We proactively trap a runtimeWarning: divide by zero encountered in log,\n",
    "    # which may occur with extreme collocations\n",
    "    import warnings\n",
    "    with warnings.catch_warnings(): # this will reset our filterwarnings setting\n",
    "        warnings.filterwarnings('error')\n",
    "        try:\n",
    "            return (np.log(binom.pmf(c12, c1, p)) \n",
    "                    + np.log(binom.pmf(c2 - c12, N - c1, p)) \n",
    "                    - np.log(binom.pmf(c12, c1, p1) )\n",
    "                    - np.log(binom.pmf(c2 - c12, N - c1, p2)))             \n",
    "        except Warning:\n",
    "            return np.inf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigramme testé : directrice médicale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-inf p-value: 1.000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "##  Parameters:\n",
    "##    c1: count of word 1\n",
    "##    c2: count of word 2\n",
    "##    c12: count of bigram (w1, w2)\n",
    "##    N: the number of words in the corpus\n",
    "\n",
    "N = len(tokens)\n",
    "c1 = tokens.count(\"fibrillation\")\n",
    "c2 = tokens.count(\"auriculaire\")\n",
    "c12 = bg.count(('fibrillation', 'auriculaire'))\n",
    "\n",
    "res = -2 * loglikelihood_ratio(c1, c2, c12, N)\n",
    "# Note: we multiply the ratio by -2 and \n",
    "# the value is directly comparable to Chi-squared values\n",
    "# e.g. for a alpha value of: 0.005, 1 Degree of freedom \n",
    "# requires a chi-squared value of 7.88\n",
    "p = chi2.sf(res, 1) # 1 degrees of freedom\n",
    "print(res, 'p-value: %.30f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on va le tester sur une liste de bigrammes pour voir comment on pourra l'automatiser dans une fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = [\n",
    "    ('cellules', 'cancéreuses'), \n",
    "    ('radio', 'oncologie'), \n",
    "    ('système', 'immunitaire'),\n",
    "    ('reconstruction', 'mammaire'),\n",
    "    ('fibrillation', 'auriculaire'),\n",
    "    ('cellules', 'auriculaire')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cellules', 'cancéreuses') -  737.1236546967899 p-value: 0.000000000000000000000000000000\n",
      "('radio', 'oncologie') -  1396.6163227649477 p-value: 0.000000000000000000000000000000\n",
      "('système', 'immunitaire') -  1403.6719732953816 p-value: 0.000000000000000000000000000000\n",
      "('reconstruction', 'mammaire') -  1531.3670403761646 p-value: 0.000000000000000000000000000000\n",
      "('fibrillation', 'auriculaire') -  -inf p-value: 1.000000000000000000000000000000\n",
      "('cellules', 'auriculaire') -  0.11453097294959314 p-value: 0.735043502280463556530776259024\n"
     ]
    }
   ],
   "source": [
    "for b in liste:\n",
    "    c1 = tokens.count(b[0])\n",
    "    c2 = tokens.count(b[1])\n",
    "    c12 = bg.count(b)\n",
    "\n",
    "    res = -2 * loglikelihood_ratio(c1, c2, c12, N)\n",
    "    p = chi2.sf(res, 1) # 1 degrees of freedom\n",
    "    print(str(b) + ' - ', res, 'p-value: %.30f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33210152f7d2bd255fb16656f372b633dbf298ed202bbbac20290b0375cadb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
