{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrer l'acteur sur lequel on veut lancer l'analyse globale\n",
    "acteur = 'pinel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, re, pandas\n",
    "from pandas import *\n",
    "import os\n",
    "from os import listdir, chdir, path\n",
    "from pathlib import Path\n",
    "\n",
    "def nlp(file, souscorpus=False):\n",
    "    print(\"Corpus traité : \"+ file)\n",
    "    #Lire le corpus\n",
    "    print(\"Lire le corpus\")\n",
    "\n",
    "    if souscorpus:\n",
    "        base_path = path.join('../03-corpus/2-sous-corpus/', acteur)\n",
    "        file_path = path.join(base_path, acteur + '_' + file +  '.csv')\n",
    "\n",
    "    else:\n",
    "        base_path = '../03-corpus/2-data/1-fr/'\n",
    "        file_path = path.join(base_path, acteur + '.csv')\n",
    "   \n",
    "    with open(file_path, \"r\", encoding = \"UTF-8\") as f:\n",
    "            data = read_csv(file_path)\n",
    "            text = data['text'].tolist()\n",
    "            corpus = \" \".join([(re.sub('\\d', '', t.strip('\\n').lower().replace('’', '\\''))) for t in text])\n",
    "\n",
    "    nb_docs = len(text)\n",
    "\n",
    "    print(\"On a un corpus de {} documents.\".format(nb_docs))\n",
    "\n",
    "    # Prétraitement\n",
    "    print(\"Prétraitement\")\n",
    "    import nltk\n",
    "    #nltk.download(['popular'])\n",
    "\n",
    "    punct = '!#$%&()*+,-/:;<=>?@[\\]^_{|}~©'\n",
    "\n",
    "    for t in punct:\n",
    "        corpus = corpus.replace(t, ' ').replace(\"  \", \" \")\n",
    "\n",
    "\n",
    "    # Filtrage - MWE Stopwords\n",
    "    file_path = '../04-filtrage/mwe_stopwords.txt'\n",
    "\n",
    "    with open (file_path, 'r', encoding='utf-8') as f:\n",
    "        mwe_sw = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "    for mwe in mwe_sw:\n",
    "        corpus = corpus.replace(mwe, '')\n",
    "\n",
    "    # Tokeniser\n",
    "    print(\"Tokenisation\")\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "    # Seulement les caractères alphabétiques\n",
    "    tokenizer_re = RegexpTokenizer(r\"\\w\\'|\\w+\")\n",
    "\n",
    "    tokens = tokenizer_re.tokenize(corpus)\n",
    "    len_corpus = len(tokens)\n",
    "\n",
    "    print(\"Avec le RegExpTokenizer, notre corpus contient {} tokens.\".format(len_corpus))\n",
    "\n",
    "    corpus = \" \".join(tokens).replace(\"' \", \"'\") # Pour \"forcer\" le TreeTagger à tokeniser comme on veut\n",
    "\n",
    "    # POS Tagging (TreeTagger)\n",
    "    print(\"Étiquetage morphosyntaxique\")\n",
    "\n",
    "    import treetaggerwrapper\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')\n",
    "\n",
    "    # Mapper les étiquettes du TreeTagger à celles du Lefff\n",
    "    file_path = '../04-filtrage/mapping_treeTagger_lefff.csv'\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        csv = read_csv(f)\n",
    "\n",
    "    treeTag = [term for term in csv['TreeTagger'].tolist()] \n",
    "    lefff = [term for term in csv['Lefff'].tolist()]\n",
    "\n",
    "    mapping = {term : lefff[treeTag.index(term)] for term in treeTag}\n",
    "\n",
    "    tagged = [[t.split('\\t')[0], mapping[t.split('\\t')[1]]] for t in tagger.tag_text(corpus)]\n",
    "    print(\"(Validation - On a {} tokens taggés).\".format(len(tagged)))\n",
    "\n",
    "    # Lemmatisation\n",
    "    print(\"Lemmatisation\")\n",
    "    from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "    lemmatizer = FrenchLefffLemmatizer()\n",
    "\n",
    "    lemmas = []\n",
    "    for term in tagged:\n",
    "        term_l = []\n",
    "        if lemmatizer.lemmatize(term[0], term[1]) == []:\n",
    "            term_l = [lemmatizer.lemmatize(term[0]), term[1]]\n",
    "        \n",
    "        elif type(lemmatizer.lemmatize(term[0], term[1])) == str:\n",
    "            term_l  = [lemmatizer.lemmatize(term[0], term[1]), term[1]]\n",
    "\n",
    "        else:\n",
    "            term_l = list(lemmatizer.lemmatize(term[0], term[1])[0])\n",
    "        \n",
    "        lemmas.append(term_l)\n",
    "    \n",
    "    print(\"(Validation - On a {} tokens lemmatisés).\".format(len(lemmas)))\n",
    "\n",
    "    base_path = '../04-filtrage/output/'\n",
    "    file_path = path.join(base_path, acteur)\n",
    "    if souscorpus:\n",
    "        file_path = path.join(file_path, file)\n",
    "\n",
    "    # Extraction de collocations (n-grammes)\n",
    "    from nltk.util import ngrams\n",
    "    from nltk.util import bigrams\n",
    "    from nltk.util import everygrams\n",
    "    from nltk.probability import FreqDist\n",
    "    ngrammes = list(everygrams(tagged, min_len=2, max_len=5))\n",
    "    ngrammes_lemmatized = list(everygrams(lemmas, min_len=2, max_len=5))\n",
    "        \n",
    "    # Extraction des patrons syntaxiques des ngrammes\n",
    "    def extract_patterns(ngrammes):\n",
    "        patterns = []\n",
    "\n",
    "        for ng in ngrammes:\n",
    "            phrase = []\n",
    "            pattern = []\n",
    "            for t in ng:\n",
    "                phrase.append(t[0]) # token\n",
    "                pattern.append(t[1]) # POS tag\n",
    "\n",
    "            patterns.append([phrase, pattern])\n",
    "            \n",
    "        return patterns\n",
    "\n",
    "    phrases = extract_patterns(ngrammes)\n",
    "    phrases_lemmatized = extract_patterns(ngrammes_lemmatized)\n",
    "\n",
    "    # Extraction des fréquences d'occurrence des ngrammes\n",
    "    freq = FreqDist([\" \".join(t[0]).replace(\"' \", \"'\") for t in phrases])\n",
    "    freq_lemmatized = FreqDist([\" \".join(t[0]).replace(\"' \", \"'\") for t in phrases_lemmatized])\n",
    "\n",
    "    # Filtrage \n",
    "    # Stopwords lemmatisés\n",
    "    file_path = '../04-filtrage/stopwords_lemmatized.txt'\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords_lemmatized = [w.strip('\\n').lower() for w in f.readlines()]\n",
    "\n",
    "    # Stopwords fréquents en français (non lemmatisés)\n",
    "    file_path = \"../04-filtrage/stopwords.txt\"\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "\n",
    "    # Stopwords fréquents en anglais (non lemmatisés)\n",
    "    file_path = '../04-filtrage/stop_words_english.txt'\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords += [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "    print(\"Filtrage - n-grammes\")\n",
    "    def filtrer_phrases(phrases, freq):\n",
    "        output = []\n",
    "        for term in phrases:\n",
    "                exp = \" \".join(term[0]).replace(\"' \", \"'\")\n",
    "                f = freq[exp]\n",
    "                \n",
    "                # f > 10 and - sans filtrer par fréquence \n",
    "                if  f > 10 and not term[0][0] in stopwords and len(term[0][0]) > 2 \\\n",
    "                and not term[0][-1] in stopwords and len(term[0][-1]) > 2 : \n",
    "                        pattern = \" \".join(term[1])            \n",
    "                        output.append([exp, pattern, f])  \n",
    "                         \n",
    "        return output\n",
    "\n",
    "    phrases = filtrer_phrases(phrases, freq)\n",
    "    phrases_lemmatized = filtrer_phrases(phrases_lemmatized, freq_lemmatized)\n",
    "\n",
    "    print(\"Après le filtrage, on a {} occurrences de nos n-grammes.\".format(len(phrases))) \n",
    "\n",
    "    # 1e extrant : n-grammes filtrés\n",
    "    # On prépare notre dossier pour stocker les output des prochaines étapes\n",
    "    base_path = path.join('../04-filtrage/output/', acteur)\n",
    "    if souscorpus:\n",
    "        base_path = path.join(base_path, file)\n",
    "    file_path = path.join(base_path, file)\n",
    "\n",
    "    Path(base_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    def tabCSV(liste, titre):\n",
    "        tab = DataFrame(liste, columns= [\"Expression\", \"Structure syntaxique\", \"Fréquence\"]).drop_duplicates()\n",
    "        tab.sort_values([\"Fréquence\"], \n",
    "                            axis=0,\n",
    "                            ascending=[False], \n",
    "                            inplace=True)\n",
    "\n",
    "\n",
    "        tab.to_csv(file_path + titre)\n",
    "        return tab.values.tolist()\n",
    "\n",
    "    phrases = tabCSV(phrases, '_n-grams.csv')\n",
    "    phrases_lemmatized = tabCSV(phrases_lemmatized, '_n-grams-lemmatized.csv')\n",
    "\n",
    "    print(\"Après filtrage, on a {} n-grammes uniques.\".format(len(phrases)))\n",
    "\n",
    "    # Filtrage - Patrons syntaxiques\n",
    "    print(\"Filtrage - n-grammes - par patrons syntaxiques\")\n",
    "    file_mesh = '../04-filtrage/mesh_patterns-fr.csv'\n",
    "\n",
    "    with open (file_mesh, 'r') as f:\n",
    "        patterns = read_csv(f)\n",
    "        patterns = patterns['Structure'].tolist()[:100] # Pour prendre seulement les 50 structures syntaxiques les plus fréquentes dans les MeSH\n",
    "\n",
    "    terms = [t for t in phrases if t[1] in patterns]\n",
    "    terms_lemmatized = [t for t in phrases_lemmatized if t[1] in patterns]\n",
    "\n",
    "    print(\"Le filtrage syntaxique élimine environ {} % des termes\".format(round((len(phrases) - len(terms)) / len(phrases) * 100)))\n",
    "    print(\"On avait {} n-grammes, \".format(len(phrases)) + \"on en a maintenant {}.\".format(len(terms)))\n",
    "\n",
    "\n",
    "    # 2e extrant - ngrammes filtrés par patrons syntaxiques\n",
    "    import pandas as pd\n",
    "\n",
    "    def extract_terms(liste_terms, titre):\n",
    "        tab = pd.DataFrame(terms, columns= [\"Expression\", \"Structure syntaxique\", \"Fréquence\"]).drop_duplicates(subset='Expression', keep=\"last\")\n",
    "        tab.sort_values([\"Fréquence\"], \n",
    "                            axis=0,\n",
    "                            ascending=[False], \n",
    "                            inplace=True)\n",
    "\n",
    "\n",
    "        tab.to_csv(file_path + titre)\n",
    "\n",
    "    extract_terms(terms, '_terms.csv')\n",
    "    extract_terms(terms_lemmatized, '_terms-lemmatized.csv')\n",
    "\n",
    "    # Extraction de concordances d'intérêt (KWIC - Keyword in Context)\n",
    "    print(\"Extraction de concordances à partir de mots-clés d'intérêt (KWIC - Keyword in Context)\")\n",
    "    kw = ['programme', 'plan ', 'service', 'intervenant', 'infirmière en', 'institut', 'groupe de recherche', 'personne', 'maladie']\n",
    "\n",
    "    ngrammes_kwic = [\" \".join([t[0].replace(\"' \", \"'\") for t in ng]) for ng in ngrammes]\n",
    "\n",
    "    extrant = pd.DataFrame(columns=['Mot-clé','Concordance', 'Fréquence'])\n",
    "    kwic = {w : [] for w in kw} \n",
    "\n",
    "    for t in ngrammes_kwic: # on pourrait aussi chercher dans les terms, mais on perd certains termes d'intérêt avec le filtrage syntaxique\n",
    "        for w in kw:\n",
    "            if t.startswith(w):\n",
    "                kwic[w].append(t)\n",
    "\n",
    "    kwic = {term: FreqDist(kwic[term]) for term in kwic}\n",
    "\n",
    "    for term in kw:\n",
    "        df = pd.DataFrame(kwic[term].items(), columns=['Concordance', \"Fréquence\"])\n",
    "        df.sort_values([\"Fréquence\"], \n",
    "            axis=0,\n",
    "            ascending=[False], \n",
    "            inplace=True)\n",
    "\n",
    "        df.insert(0, 'Mot-clé', term)\n",
    "    \n",
    "    extrant = pd.concat([extrant, df])\n",
    "\n",
    "    # 3e extrant - KWIC\n",
    "    extrant = extrant[extrant['Fréquence'] > 30] \n",
    "\n",
    "    extrant.to_csv(file_path + '_KWIC.csv')\n",
    "\n",
    "    # Extraction de termes MeSH présents dans nos données\n",
    "    print(\"Extraction de termes MeSH présents dans nos données\")\n",
    "    from nltk.tokenize import MWETokenizer\n",
    "    path_mesh = '../04-filtrage/mesh-fr.txt'\n",
    "\n",
    "    with open (path_mesh, 'r', encoding='utf-8') as f:\n",
    "        mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "        tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "        mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "        mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "\n",
    "    extr_mesh = tokenizer_mesh.tokenize([t[0] for t in terms])\n",
    "\n",
    "    termes_mesh = []\n",
    "\n",
    "    for t in extr_mesh:\n",
    "        if t in mesh:\n",
    "            termes_mesh.append(t)\n",
    "\n",
    "    # 4e extrant - Termes MeSH\n",
    "    df = DataFrame(termes_mesh)\n",
    "    df.to_csv(file_path + '_MeSH.csv')\n",
    "\n",
    "    # Extraction de termes SNOMED présents dans nos données\n",
    "    print(\"Extraction de termes SNOMED présents dans nos données\")\n",
    "    from nltk.tokenize import MWETokenizer\n",
    "    path_snomed = '../04-filtrage/SNOMED_fr.csv'\n",
    "\n",
    "    with open(path_snomed, 'r', encoding='utf-8') as f:\n",
    "        sm = read_csv(f, sep=';')\n",
    "        sm = list(dict.fromkeys([str(t).strip().lower() for t in sm['term'].tolist()]))\n",
    "\n",
    "        sm = [tuple(tokenizer_re.tokenize(w)) for w in sm if len(w.split()) > 1]\n",
    "        tokenizer_sm = MWETokenizer(sm, separator = ' ')\n",
    "\n",
    "        sm = [tokenizer_sm.tokenize(w)[0].lower() for w in sm]\n",
    "\n",
    "    extr_sm = tokenizer_sm.tokenize([t[0] for t in terms])\n",
    "\n",
    "    termes_sm = []\n",
    "\n",
    "    for t in extr_sm:\n",
    "        if t in sm:\n",
    "            termes_sm.append(t)\n",
    "\n",
    "    # 5e extrant - Termes SNOMED\n",
    "    df = DataFrame(termes_sm)\n",
    "    df.to_csv(file_path + '_SNOMED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus traité : pinel\n",
      "Lire le corpus\n",
      "On a un corpus de 523 documents.\n",
      "Prétraitement\n",
      "Tokenisation\n",
      "Avec le RegExpTokenizer, notre corpus contient 794516 tokens.\n",
      "Étiquetage morphosyntaxique\n",
      "(Validation - On a 794506 tokens taggés).\n",
      "Lemmatisation\n",
      "(Validation - On a 794506 tokens lemmatisés).\n",
      "Filtrage - n-grammes\n"
     ]
    }
   ],
   "source": [
    "# On lance l'analyse sur le corpus global\n",
    "nlp(acteur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuite, on lance l'analyse sur les sous-corpus\n",
    "\n",
    "path_acteur = path.join('../03-corpus/2-sous-corpus/', acteur)\n",
    "liste = [tag[tag.index('_')+1:-4] for tag in os.listdir(path_acteur)]\n",
    "\n",
    "liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in liste:\n",
    "    nlp(file, souscorpus=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79bb76bbc4f9ba1f8df5efe8db67aae07079a51dc7b5004f49990e90f5993a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
