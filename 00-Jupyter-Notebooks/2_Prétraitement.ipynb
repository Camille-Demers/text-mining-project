{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c662c7d",
   "metadata": {},
   "source": [
    "## **2. Prétraitement**\n",
    "- Segmentation (phrases)\n",
    "- Tokenization (mots)\n",
    "- Étiquetage morphosyntaxique (POS Tagging) \n",
    "- Lemmatisation\n",
    "- Filtrage (stopwords)\n",
    "- Extraction de termes complexes (MWE / n-grammes / segments répétés)\n",
    "- Chunking / Filtrage par patrons syntaxiques (basés sur les patrons fréquents dans les MeSH)\n",
    "- Extraction de concordances (KWIC) pour un ensemble de mots-clés d'intérêt\n",
    "- Extraction de termes MeSH et SNOMED présents dans les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece38e9",
   "metadata": {},
   "source": [
    "### **Lire le corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a62415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, re, pandas\n",
    "from os import listdir, chdir, path\n",
    "from pathlib import Path\n",
    "\n",
    "acteur = 'msss'\n",
    "sous_corpus = False \n",
    "tag = ''\n",
    "\n",
    "# Change the directory\n",
    "if sous_corpus:\n",
    "    base_path = '../03-corpus/2-sous-corpus/'\n",
    "    file_path = path.join(base_path, acteur, tag)\n",
    "\n",
    "else: \n",
    "    base_path = '../03-corpus/2-data/1-fr/'\n",
    "    file_path = path.join(base_path, acteur) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31c5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "with open(file_path, \"r\", encoding = \"UTF-8\") as f:\n",
    "        data = read_csv(file_path)\n",
    "        text = data['text'].tolist()\n",
    "        corpus = \" \".join([(re.sub('\\d', '', t.strip('\\n').lower().replace('’', '\\''))) for t in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4fda380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a un corpus de 1817 documents.\n"
     ]
    }
   ],
   "source": [
    "nb_docs = len(text)\n",
    "\n",
    "print(\"On a un corpus de {} documents.\".format(nb_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a4b16",
   "metadata": {},
   "source": [
    "**NLTK**\\\n",
    "https://www.nltk.org/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31145e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(['popular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63c6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = '!#$%&()*+,-/:;<=>?@[\\]^_{|}~©'\n",
    "\n",
    "for t in punct:\n",
    "    corpus = corpus.replace(t, ' ').replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b5d73",
   "metadata": {},
   "source": [
    "### **Filtrage (MWE - stopwords formés de plusieurs tokens)**\n",
    "Surtout pour filtrer les expressions relatives à l'architecture d'information / navigation Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b7820d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/mwe_stopwords.txt'\n",
    "\n",
    "with open (file_path, 'r', encoding='utf-8') as f:\n",
    "    mwe_sw = [t.lower().strip('\\n') for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29a19b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mwe in mwe_sw:\n",
    "    corpus = corpus.replace(mwe, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980c335",
   "metadata": {},
   "source": [
    "### **Tokenisation / POS tagging** (TreeTagger)  \n",
    "https://github.com/miotto/treetagger-python/blob/master/README.rst  \n",
    "https://treetaggerwrapper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb94eec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec le RegExpTokenizer, notre corpus contient 3131977 tokens.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Seulement les caractères alphabétiques\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w\\'|\\w+\")\n",
    "\n",
    "tokens = tokenizer_re.tokenize(corpus)\n",
    "len_corpus = len(tokens)\n",
    "\n",
    "print(\"Avec le RegExpTokenizer, notre corpus contient {} tokens.\".format(len_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1415fad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inesss',\n",
       " 'médicaments',\n",
       " 'évaluation',\n",
       " 'aux',\n",
       " 'fins',\n",
       " \"d'\",\n",
       " 'inscription',\n",
       " 'emplois',\n",
       " 'english',\n",
       " 'inesss',\n",
       " 'express',\n",
       " 'algorithmes',\n",
       " 'en',\n",
       " 'cancérologie',\n",
       " 'consulter',\n",
       " 'une',\n",
       " 'publication',\n",
       " 'évaluation',\n",
       " 'des',\n",
       " 'médicaments',\n",
       " 'aux',\n",
       " 'fins',\n",
       " \"d'\",\n",
       " 'inscription',\n",
       " 'guides',\n",
       " \"d'\",\n",
       " 'usage',\n",
       " 'optimal',\n",
       " 'outils',\n",
       " 'cliniques',\n",
       " 'protocoles',\n",
       " 'médicaux',\n",
       " 'nationaux',\n",
       " 'et',\n",
       " 'ordonnances',\n",
       " 'associées',\n",
       " 'covid',\n",
       " 'risques',\n",
       " \"d'\",\n",
       " 'hospitalisation',\n",
       " 'et',\n",
       " 'projections',\n",
       " 'des',\n",
       " 'besoins',\n",
       " 'hospitaliers',\n",
       " 'traitements',\n",
       " 'spécifiques',\n",
       " 'à',\n",
       " 'la',\n",
       " 'covid',\n",
       " 'affections',\n",
       " 'post',\n",
       " 'covid',\n",
       " 'covid',\n",
       " 'longue',\n",
       " 'alternatives',\n",
       " 'de',\n",
       " 'traitements',\n",
       " 'en',\n",
       " 'contexte',\n",
       " 'de',\n",
       " 'pandémie',\n",
       " 'autres',\n",
       " 'traitements',\n",
       " 'cancérologie',\n",
       " 'études',\n",
       " 'cliniques',\n",
       " 'en',\n",
       " 'cours',\n",
       " 'prophylaxie',\n",
       " 'et',\n",
       " 'traitement',\n",
       " 'ordonnances',\n",
       " 'collectives',\n",
       " 'regard',\n",
       " 'sur',\n",
       " 'la',\n",
       " 'pandémie',\n",
       " 'covid',\n",
       " 'au',\n",
       " 'québec',\n",
       " 'présentations',\n",
       " 'cliniques',\n",
       " 'investigation',\n",
       " 'procédures',\n",
       " 'diagnostiques',\n",
       " 'rétablissement',\n",
       " 'et',\n",
       " 'reprise',\n",
       " 'des',\n",
       " 'activités',\n",
       " 'services',\n",
       " 'sociaux',\n",
       " 'méthode',\n",
       " 'de',\n",
       " 'réponse',\n",
       " 'rapide',\n",
       " 'covid',\n",
       " 'contributions',\n",
       " 'des']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07b565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" \".join(tokens).replace(\"' \", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "047334bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c195d",
   "metadata": {},
   "source": [
    "### **Mapping POS Tags** (FRMG)\n",
    "\n",
    "Pour utiliser adéquatement notre lemmatiseur par la suite (FrenchLefffLemmatizer), on va mapper les étiquettes morphosyntaxiques du TreeTagger à celles que prend le lemmatiseur (celles issues de FRMG)\n",
    "\n",
    "http://alpage.inria.fr/frmgwiki/content/tagset-frmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7682234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/mapping_treeTagger_lefff.csv'\n",
    "\n",
    "with open(file_path) as f:\n",
    "    csv = read_csv(f)\n",
    "\n",
    "treeTag = [term for term in csv['TreeTagger'].tolist()] \n",
    "lefff = [term for term in csv['Lefff'].tolist()]\n",
    "\n",
    "mapping = {term : lefff[treeTag.index(term)] for term in treeTag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d991e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = [[t.split('\\t')[0], mapping[t.split('\\t')[1]]] for t in tagger.tag_text(corpus)]\n",
    "\n",
    "#if len(t.split('\\t')) >1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cb36efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['inesss', 'adj'],\n",
       " ['médicaments', 'nc'],\n",
       " ['évaluation', 'nc'],\n",
       " ['aux', 'prep'],\n",
       " ['fins', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['emplois', 'nc'],\n",
       " ['english', 'nc'],\n",
       " ['inesss', 'adj'],\n",
       " ['express', 'adj'],\n",
       " ['algorithmes', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['cancérologie', 'nc'],\n",
       " ['consulter', 'v'],\n",
       " ['une', 'det'],\n",
       " ['publication', 'nc'],\n",
       " ['évaluation', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['médicaments', 'nc'],\n",
       " ['aux', 'prep'],\n",
       " ['fins', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['guides', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['usage', 'nc'],\n",
       " ['optimal', 'adj'],\n",
       " ['outils', 'nc'],\n",
       " ['cliniques', 'adj'],\n",
       " ['protocoles', 'nc'],\n",
       " ['médicaux', 'adj'],\n",
       " ['nationaux', 'adj'],\n",
       " ['et', 'csu'],\n",
       " ['ordonnances', 'nc'],\n",
       " ['associées', 'v'],\n",
       " ['covid', 'adj'],\n",
       " ['risques', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['hospitalisation', 'nc'],\n",
       " ['et', 'csu'],\n",
       " ['projections', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['besoins', 'nc'],\n",
       " ['hospitaliers', 'adj'],\n",
       " ['traitements', 'nc'],\n",
       " ['spécifiques', 'adj'],\n",
       " ['à', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['covid', 'nc'],\n",
       " ['affections', 'v'],\n",
       " ['post', 'nc'],\n",
       " ['covid', 'adj'],\n",
       " ['covid', 'nc'],\n",
       " ['longue', 'adj'],\n",
       " ['alternatives', 'adj'],\n",
       " ['de', 'prep'],\n",
       " ['traitements', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['contexte', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['pandémie', 'nc'],\n",
       " ['autres', 'adj'],\n",
       " ['traitements', 'nc'],\n",
       " ['cancérologie', 'nc'],\n",
       " ['études', 'nc'],\n",
       " ['cliniques', 'adj'],\n",
       " ['en', 'prep'],\n",
       " ['cours', 'nc'],\n",
       " ['prophylaxie', 'nc'],\n",
       " ['et', 'csu'],\n",
       " ['traitement', 'nc'],\n",
       " ['ordonnances', 'nc'],\n",
       " ['collectives', 'adj'],\n",
       " ['regard', 'nc'],\n",
       " ['sur', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['pandémie', 'nc'],\n",
       " ['covid', 'adj'],\n",
       " ['au', 'prep'],\n",
       " ['québec', 'adj'],\n",
       " ['présentations', 'nc'],\n",
       " ['cliniques', 'adj'],\n",
       " ['investigation', 'nc'],\n",
       " ['procédures', 'nc'],\n",
       " ['diagnostiques', 'adj'],\n",
       " ['rétablissement', 'nc'],\n",
       " ['et', 'csu'],\n",
       " ['reprise', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['activités', 'nc'],\n",
       " ['services', 'nc'],\n",
       " ['sociaux', 'adj'],\n",
       " ['méthode', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['réponse', 'nc'],\n",
       " ['rapide', 'adj'],\n",
       " ['covid', 'adj'],\n",
       " ['contributions', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['uetmis', 'nc'],\n",
       " ['sites', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['intérêt', 'nc'],\n",
       " ['mission', 'nc'],\n",
       " ['vision', 'nc'],\n",
       " ['valeurs', 'nc'],\n",
       " ['je', 'pro'],\n",
       " ['veux', 'v'],\n",
       " ['participer', 'v'],\n",
       " ['conseil', 'nc'],\n",
       " ['scientifique', 'adj'],\n",
       " ['comités', 'nc'],\n",
       " ['collaborateurs', 'nc'],\n",
       " ['institutionnels', 'adj'],\n",
       " ['directions', 'nc'],\n",
       " ['documents', 'nc'],\n",
       " ['institutionnels', 'adj'],\n",
       " ['emplois', 'nc'],\n",
       " [\"l'\", 'det'],\n",
       " ['inesss', 'nc'],\n",
       " ['express', 'adj'],\n",
       " ['démarche', 'nc'],\n",
       " ['démarche', 'v'],\n",
       " ['thématiques', 'adj'],\n",
       " ['médicaments', 'nc'],\n",
       " ['évaluation', 'nc'],\n",
       " ['aux', 'prep'],\n",
       " ['fins', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['médicaments', 'nc'],\n",
       " ['usage', 'nc'],\n",
       " ['optimal', 'adj'],\n",
       " ['protocoles', 'nc'],\n",
       " ['médicaux', 'adj'],\n",
       " ['nationaux', 'adj'],\n",
       " ['et', 'csu'],\n",
       " ['ordonnances', 'nc'],\n",
       " ['associées', 'v'],\n",
       " ['biologie', 'nc'],\n",
       " ['médicale', 'adj'],\n",
       " ['et', 'csu'],\n",
       " ['génomique', 'adj'],\n",
       " ['cancérologie', 'nc'],\n",
       " ['cardiologie', 'nc'],\n",
       " ['et', 'csu'],\n",
       " ['maladies', 'nc'],\n",
       " ['neurovasculaires', 'adj'],\n",
       " ['dépistage', 'nc'],\n",
       " ['et', 'csu'],\n",
       " ['pratiques', 'nc'],\n",
       " ['cliniques', 'adj'],\n",
       " ['préventives', 'adj'],\n",
       " ['imagerie', 'nc'],\n",
       " ['médicale', 'adj'],\n",
       " ['innovations', 'nc'],\n",
       " ['technologiques', 'adj'],\n",
       " ['et', 'csu'],\n",
       " ['dispositifs', 'nc'],\n",
       " ['médicaux', 'adj'],\n",
       " ['modes', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['intervention', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['santé', 'nc'],\n",
       " ['produits', 'v'],\n",
       " ['du', 'prep'],\n",
       " ['système', 'nc'],\n",
       " ['du', 'prep'],\n",
       " ['sang', 'nc'],\n",
       " ['soins', 'nc'],\n",
       " ['critiques', 'adj'],\n",
       " ['thérapies', 'nc'],\n",
       " ['cellulaires', 'adj'],\n",
       " ['traumatologie', 'nc'],\n",
       " ['déficiences', 'nc'],\n",
       " ['physiques', 'adj'],\n",
       " ['intellectuelles', 'adj'],\n",
       " ['et', 'csu'],\n",
       " ['tsa', 'adj'],\n",
       " ['dépendance', 'nc'],\n",
       " ['et', 'csu'],\n",
       " ['itinérance', 'nc'],\n",
       " ['jeunes', 'adj'],\n",
       " ['et', 'csu'],\n",
       " ['familles', 'nc'],\n",
       " ['personnes', 'nc'],\n",
       " ['âgées', 'adj'],\n",
       " ['santé', 'nc'],\n",
       " ['mentale', 'adj'],\n",
       " ['services', 'nc'],\n",
       " ['sociaux', 'adj'],\n",
       " ['généraux', 'adj'],\n",
       " ['publications', 'nc'],\n",
       " ['répertoire', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['publications', 'nc'],\n",
       " ['algorithmes', 'nc'],\n",
       " ['commander', 'v'],\n",
       " ['une', 'det'],\n",
       " ['publication', 'nc'],\n",
       " ['guides', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['usage', 'nc'],\n",
       " ['optimal', 'adj'],\n",
       " ['projets', 'nc'],\n",
       " ['projets', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['cours', 'nc'],\n",
       " ['indicateurs', 'adj'],\n",
       " ['de', 'prep'],\n",
       " ['qualité', 'nc'],\n",
       " ['maladies', 'nc'],\n",
       " ['chroniques', 'adj'],\n",
       " ['formations', 'nc'],\n",
       " ['et', 'csu'],\n",
       " ['outils', 'nc'],\n",
       " ['outils', 'nc'],\n",
       " ['cliniques', 'adj'],\n",
       " ['formations', 'nc'],\n",
       " ['et', 'csu'],\n",
       " ['capsules', 'nc'],\n",
       " ['vidéos', 'nc'],\n",
       " ['approche', 'v'],\n",
       " ['méthodologique', 'adj'],\n",
       " ['emplois', 'nc'],\n",
       " ['inesss', 'adj'],\n",
       " ['express', 'adj'],\n",
       " ['médicaments', 'nc'],\n",
       " ['évaluation', 'nc'],\n",
       " ['aux', 'prep'],\n",
       " ['fins', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['avis', 'nc'],\n",
       " ['au', 'prep'],\n",
       " ['ministre', 'nc'],\n",
       " ['avis', 'nc'],\n",
       " ['sur', 'prep'],\n",
       " ['les', 'det'],\n",
       " ['prix', 'nc'],\n",
       " ['approche', 'v'],\n",
       " ['modalités', 'nc'],\n",
       " ['et', 'csu'],\n",
       " ['processus', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['modalités', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['médicaments', 'nc'],\n",
       " ['aux', 'prep'],\n",
       " ['fins', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['médicaments', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['attente', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['un', 'det'],\n",
       " ['avis', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['conformité', 'nc'],\n",
       " ['médicaments', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['exception', 'nc'],\n",
       " ['évaluation', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['médicaments', 'nc'],\n",
       " ['biosimilaires', 'adj'],\n",
       " ['évaluation', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['une', 'det'],\n",
       " ['demande', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['exemption', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['méthode', 'nc'],\n",
       " ['du', 'prep'],\n",
       " ['prix', 'nc'],\n",
       " ['le', 'det'],\n",
       " ['plus', 'adv'],\n",
       " ['bas', 'adj'],\n",
       " ['ppb', 'nc'],\n",
       " ['rendez', 'v'],\n",
       " ['vous', 'pro'],\n",
       " ['des', 'prep'],\n",
       " ['fabricants', 'nc'],\n",
       " ['rencontre', 'v'],\n",
       " ['présoumission', 'nc'],\n",
       " ['avec', 'prep'],\n",
       " ['les', 'det'],\n",
       " ['fabricants', 'nc'],\n",
       " ['préavis', 'nc'],\n",
       " ['obligatoire', 'adj'],\n",
       " ['demandé', 'v'],\n",
       " ['aux', 'prep'],\n",
       " ['fabricants', 'nc'],\n",
       " ['demande', 'v'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['adhésion', 'nc'],\n",
       " ['au', 'prep'],\n",
       " ['dépôt', 'nc'],\n",
       " ['électronique', 'adj'],\n",
       " [\"d'\", 'prep'],\n",
       " ['une', 'det'],\n",
       " ['demande', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['fiche', 'v'],\n",
       " ['dépôt', 'nc'],\n",
       " ['électronique', 'adj'],\n",
       " [\"d'\", 'prep'],\n",
       " ['une', 'det'],\n",
       " ['demande', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['fiche', 'v'],\n",
       " ['dépôt', 'nc'],\n",
       " ['électronique', 'adj'],\n",
       " ['temporaire', 'adj'],\n",
       " ['pour', 'prep'],\n",
       " ['les', 'det'],\n",
       " ['demandes', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['autres', 'adj'],\n",
       " ['fiches', 'nc'],\n",
       " ['rencontre', 'v'],\n",
       " ['postsoumission', 'nc'],\n",
       " ['avec', 'prep'],\n",
       " ['les', 'det'],\n",
       " ['fabricants', 'nc'],\n",
       " ['avis', 'nc'],\n",
       " ['aux', 'prep'],\n",
       " ['fabricants', 'nc'],\n",
       " ['échéancier', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['en', 'prep'],\n",
       " ['échéancier', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['en', 'prep'],\n",
       " ['tarification', 'nc'],\n",
       " ['sur', 'prep'],\n",
       " ['les', 'det'],\n",
       " ['médicaments', 'nc'],\n",
       " ['hépatite', 'nc'],\n",
       " ['c', 'adj'],\n",
       " ['maladie', 'nc'],\n",
       " ['pulmonaire', 'adj'],\n",
       " ['obstructive', 'adj'],\n",
       " ['chronique', 'adj'],\n",
       " ['médicaments', 'nc'],\n",
       " ['biosimilaires', 'adj'],\n",
       " ['médicaments', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['exception', 'nc'],\n",
       " ['oncologie', 'nc'],\n",
       " ['médicaments', 'nc'],\n",
       " ['génériques', 'adj'],\n",
       " ['produits', 'v'],\n",
       " ['de', 'prep'],\n",
       " ['santé', 'nc'],\n",
       " ['naturels', 'adj'],\n",
       " ['virus', 'nc'],\n",
       " ['de', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['immunodéficience', 'nc'],\n",
       " ['humaine', 'adj'],\n",
       " ['médicaments', 'nc'],\n",
       " ['évaluation', 'nc'],\n",
       " ['aux', 'prep'],\n",
       " ['fins', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " [\"l'\", 'det'],\n",
       " ['évaluation', 'nc'],\n",
       " ['du', 'prep'],\n",
       " ['médicament', 'nc'],\n",
       " ['réfère', 'v'],\n",
       " ['aux', 'prep'],\n",
       " ['travaux', 'nc'],\n",
       " ['menant', 'v'],\n",
       " ['à', 'prep'],\n",
       " ['des', 'prep'],\n",
       " ['recommandations', 'nc'],\n",
       " ['quant', 'prep'],\n",
       " ['à', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['pertinence', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['maintenir', 'v'],\n",
       " ['ou', 'csu'],\n",
       " [\"d'\", 'prep'],\n",
       " ['ajouter', 'v'],\n",
       " ['un', 'det'],\n",
       " ['produit', 'nc'],\n",
       " ['à', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['liste', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['médicaments', 'nc'],\n",
       " ['couverts', 'v'],\n",
       " ['par', 'prep'],\n",
       " ['le', 'det'],\n",
       " ['régime', 'nc'],\n",
       " ['public', 'adj'],\n",
       " [\"l'\", 'det'],\n",
       " ['inesss', 'nc'],\n",
       " ['a', 'v'],\n",
       " ['pour', 'prep'],\n",
       " ['mandat', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluer', 'v'],\n",
       " ['des', 'prep'],\n",
       " ['demandes', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['médicaments', 'nc'],\n",
       " ['dans', 'prep'],\n",
       " ['le', 'det'],\n",
       " ['but', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['faire', 'v'],\n",
       " ['des', 'prep'],\n",
       " ['recommandations', 'nc'],\n",
       " ['au', 'prep'],\n",
       " ['ministre', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['santé', 'nc'],\n",
       " ['et', 'csu'],\n",
       " ['des', 'prep'],\n",
       " ['services', 'nc'],\n",
       " ['sociaux', 'adj'],\n",
       " ['dans', 'prep'],\n",
       " ['le', 'det'],\n",
       " ['cadre', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['des', 'prep'],\n",
       " ['listes', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['médicaments', 'nc'],\n",
       " ['les', 'det'],\n",
       " ['médicaments', 'nc'],\n",
       " ['présentés', 'v'],\n",
       " ['dans', 'prep'],\n",
       " ['les', 'det'],\n",
       " ['premiers', '-'],\n",
       " ['onglets', 'nc'],\n",
       " ['représentent', 'v'],\n",
       " ['les', 'det'],\n",
       " ['médicaments', 'nc'],\n",
       " ['qui', 'pro'],\n",
       " ['sont', 'v'],\n",
       " ['en', 'prep'],\n",
       " ['cours', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['le', 'det'],\n",
       " ['dernier', 'adj'],\n",
       " ['onglet', 'nc'],\n",
       " ['présente', 'v'],\n",
       " ['les', 'det'],\n",
       " ['médicaments', 'nc'],\n",
       " ['déjà', 'adv'],\n",
       " ['évalués', 'v'],\n",
       " ['par', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['inesss', 'nc'],\n",
       " ['ainsi', 'csu'],\n",
       " ['que', 'csu'],\n",
       " ['les', 'det'],\n",
       " ['décisions', 'nc'],\n",
       " ['du', 'prep'],\n",
       " ['ministre', 'nc'],\n",
       " ['lorsque', 'csu'],\n",
       " ['disponibles', 'adj'],\n",
       " ['juillet', 'nc'],\n",
       " ['évaluations', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['mode', 'nc'],\n",
       " ['continu', 'adj'],\n",
       " ['autres', 'adj'],\n",
       " ['évaluations', 'nc'],\n",
       " ['médicaments', 'nc'],\n",
       " ['évalués', 'v'],\n",
       " ['médicaments', 'nc'],\n",
       " ['multisources', 'adj'],\n",
       " ['à', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['étude', 'nc'],\n",
       " ['date', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['tombée', 'nc'],\n",
       " ['génériques', 'adj'],\n",
       " ['mai', 'nc'],\n",
       " ['médicaments', 'nc'],\n",
       " ['nom', 'nc'],\n",
       " ['commercial', 'adj'],\n",
       " ['dénomination', 'nc'],\n",
       " ['commune', 'adj'],\n",
       " ['nom', 'nc'],\n",
       " ['du', 'prep'],\n",
       " ['fabricant', 'nc'],\n",
       " ['ag', 'adj'],\n",
       " ['calcium', 'nc'],\n",
       " ['citrate', 'nc'],\n",
       " ['liquid', 'adj'],\n",
       " ['calcium', 'nc'],\n",
       " ['citrate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['angita', 'nc'],\n",
       " ['ag', 'adj'],\n",
       " ['quetiapine', 'adj'],\n",
       " ['xr', 'nc'],\n",
       " ['quétiapine', 'v'],\n",
       " ['fumarate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['angita', 'nc'],\n",
       " ['apo', 'nc'],\n",
       " ['eletriptan', 'adj'],\n",
       " ['élétriptan', 'adj'],\n",
       " ['bromhydrate', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['apotex', 'nc'],\n",
       " ['bicarbonate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['sodium', 'nc'],\n",
       " ['pour', 'prep'],\n",
       " ['injection', 'nc'],\n",
       " ['bicarbonate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['sodium', 'nc'],\n",
       " ['jamp', 'nc'],\n",
       " ['darunavir', 'v'],\n",
       " ['darunavir', 'v'],\n",
       " ['jamp', 'nc'],\n",
       " ['dextrose', 'nc'],\n",
       " ['injection', 'nc'],\n",
       " ['dextrose', 'nc'],\n",
       " ['oméga', 'nc'],\n",
       " ['halycil', 'nc'],\n",
       " ['propylthiouracile', 'adj'],\n",
       " ['accelera', 'v'],\n",
       " ['injection', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['glycopyrrolate', 'nc'],\n",
       " ['glycopyrrolate', 'adj'],\n",
       " ['jamp', 'nc'],\n",
       " ['jamp', 'nc'],\n",
       " ['calcium', 'nc'],\n",
       " ['polystyrene', 'adj'],\n",
       " ['sulfonate', 'nc'],\n",
       " ['polystyrène', 'nc'],\n",
       " ['sulfonate', 'nc'],\n",
       " ['calcique', 'adj'],\n",
       " ['de', 'prep'],\n",
       " ['jamp', 'nc'],\n",
       " ['jamp', 'nc'],\n",
       " ['lurasidone', 'adj'],\n",
       " ['lurasidone', 'adj'],\n",
       " ['chlorhydrate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['jamp', 'nc'],\n",
       " ['lisinopril', 'adj'],\n",
       " ['lisinopril', 'nc'],\n",
       " ['sanis', 'v'],\n",
       " ['m', 'nc'],\n",
       " ['nifedipine', 'nc'],\n",
       " ['er', 'v'],\n",
       " ['nifédipine', 'adj'],\n",
       " ['mantra', 'nc'],\n",
       " ['ph', 'adj'],\n",
       " ['mar', 'adj'],\n",
       " ['ondansetron', 'nc'],\n",
       " ['odt', 'nc'],\n",
       " ['ondansétron', 'nc'],\n",
       " ['marcan', 'nc'],\n",
       " ['mint', 'v'],\n",
       " ['quetiapine', 'adj'],\n",
       " ['xr', 'nc'],\n",
       " ['quétiapine', 'v'],\n",
       " ['fumarate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['mint', 'nc'],\n",
       " ['moxifloxacin', 'adj'],\n",
       " ['moxifloxacine', 'adj'],\n",
       " ['chlorhydrate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['sanis', 'nc'],\n",
       " ['octréotide', 'adj'],\n",
       " ['acétate', 'nc'],\n",
       " ['oméga', 'nc'],\n",
       " ['octréotide', 'adj'],\n",
       " ['acétate', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['oméga', 'nc'],\n",
       " ['ondansetron', 'nc'],\n",
       " ['odt', 'nc'],\n",
       " ['ondansétron', 'nc'],\n",
       " ['jamp', 'nc'],\n",
       " ['pipéracilline', 'adj'],\n",
       " ['et', 'csu'],\n",
       " ['tazobactam', 'nc'],\n",
       " ['pour', 'prep'],\n",
       " ['injection', 'nc'],\n",
       " ['pipéracilline', 'adj'],\n",
       " ['sodique', 'adj'],\n",
       " ['tazobactam', 'nc'],\n",
       " ['sodique', 'adj'],\n",
       " ['sandoz', 'nc'],\n",
       " ['pms', 'nc'],\n",
       " ['fluticasone', 'adj'],\n",
       " ['hfa', 'v'],\n",
       " ['fluticasone', 'adj'],\n",
       " ['propionate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['phmscience', 'nc'],\n",
       " ['pms', 'nc'],\n",
       " ['lurasidone', 'adj'],\n",
       " ['lurasidone', 'adj'],\n",
       " ['chlorhydrate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['phmscience', 'nc'],\n",
       " ['pms', 'nc'],\n",
       " ['ondansetron', 'nc'],\n",
       " ['odt', 'nc'],\n",
       " ['ondansétron', 'nc'],\n",
       " ['phmscience', 'nc'],\n",
       " ['pmsc', 'adj'],\n",
       " ['celecoxib', 'nc'],\n",
       " ['célécoxib', 'nc'],\n",
       " ['phmscience', 'nc'],\n",
       " ['pramipexole', 'adj'],\n",
       " ['pramipexole', 'adj'],\n",
       " ['dichlorhydrate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['sanis', 'nc'],\n",
       " ['quetiapine', 'v'],\n",
       " ['xr', 'nc'],\n",
       " ['quétiapine', 'v'],\n",
       " ['fumarate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['jamp', 'nc'],\n",
       " ['sandoz', 'nc'],\n",
       " ['abiraterone', 'adj'],\n",
       " ['abiratérone', 'adj'],\n",
       " ['acétate', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['sandoz', 'nc'],\n",
       " ['sandoz', 'nc'],\n",
       " ['fesoterodine', 'adj'],\n",
       " ['fumarate', 'nc'],\n",
       " ['fésotérodine', 'adj'],\n",
       " ['fumarate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['sandoz', 'nc'],\n",
       " ['sandoz', 'nc'],\n",
       " ['lurasidone', 'adj'],\n",
       " ['lurasidone', 'adj'],\n",
       " ['chlorhydrate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['sandoz', 'nc'],\n",
       " ['taro', 'nc'],\n",
       " ['lurasidone', 'adj'],\n",
       " ['lurasidone', 'adj'],\n",
       " ['chlorhydrate', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['taro', 'nc'],\n",
       " ['taro', 'nc'],\n",
       " ['ticagrelor', 'adj'],\n",
       " ['ticagrélor', 'nc'],\n",
       " ['taro', 'nc'],\n",
       " ['faire', 'v'],\n",
       " ['un', 'det'],\n",
       " ['commentaire', 'nc'],\n",
       " ['sur', 'prep'],\n",
       " ['un', 'det'],\n",
       " ['médicament', 'nc'],\n",
       " ['les', 'det'],\n",
       " ['médicaments', 'nc'],\n",
       " ['présentés', 'v'],\n",
       " ['dans', 'prep'],\n",
       " ['le', 'det'],\n",
       " ['tableau', 'nc'],\n",
       " ['suivant', 'v'],\n",
       " ['sont', 'v'],\n",
       " ['en', 'prep'],\n",
       " ['attente', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['un', 'det'],\n",
       " ['dépôt', 'nc'],\n",
       " ['pour', 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['attente', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['ou', 'csu'],\n",
       " ['en', 'prep'],\n",
       " ['cours', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['mode', 'nc'],\n",
       " ['continu', 'adj'],\n",
       " ['par', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['inesss', 'adj'],\n",
       " ['statut', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['demande', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['attente', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['un', 'det'],\n",
       " ['dépôt', 'nc'],\n",
       " ['pour', 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['demande', 'v'],\n",
       " ['pour', 'csu'],\n",
       " ['laquelle', 'pro'],\n",
       " ['la', 'det'],\n",
       " ['démarche', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['consultation', 'nc'],\n",
       " ['est', 'v'],\n",
       " ['amorcée', 'v'],\n",
       " ['quatre', '-'],\n",
       " ['semaines', 'nc'],\n",
       " ['avant', 'prep'],\n",
       " ['le', 'det'],\n",
       " ['dépôt', 'nc'],\n",
       " ['annoncé', 'v'],\n",
       " ['par', 'prep'],\n",
       " ['le', 'det'],\n",
       " ['fabricant', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['attente', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['demande', 'v'],\n",
       " ['jugée', 'v'],\n",
       " ['recevable', 'adj'],\n",
       " ['placée', 'v'],\n",
       " ['en', 'prep'],\n",
       " ['attente', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['une', 'det'],\n",
       " ['évaluation', 'nc'],\n",
       " ['scientifique', 'adj'],\n",
       " ['par', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['inesss', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['cours', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['demande', 'v'],\n",
       " ['qui', 'pro'],\n",
       " ['fait', 'v'],\n",
       " ['actuellement', 'adv'],\n",
       " [\"l'\", 'det'],\n",
       " ['objet', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['une', 'det'],\n",
       " ['évaluation', 'nc'],\n",
       " ['scientifique', 'adj'],\n",
       " ['par', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['inesss', 'adj'],\n",
       " ['précisions', 'nc'],\n",
       " ['sur', 'prep'],\n",
       " ['le', 'det'],\n",
       " ['projet', 'nc'],\n",
       " ['projet', 'nc'],\n",
       " ['aligné', 'v'],\n",
       " ['inesss', 'adj'],\n",
       " ['acmts', 'nc'],\n",
       " ['santé', 'nc'],\n",
       " ['canada', 'nc'],\n",
       " ['projet', 'nc'],\n",
       " ['collaboratif', 'adj'],\n",
       " ['inesss', 'adj'],\n",
       " ['acmts', 'nc'],\n",
       " ['dans', 'prep'],\n",
       " ['ce', 'pro'],\n",
       " ['tableau', 'nc'],\n",
       " ['les', 'det'],\n",
       " ['demandes', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['inscription', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['médicaments', 'nc'],\n",
       " ['sont', 'v'],\n",
       " ['affichées', 'v'],\n",
       " ['par', 'prep'],\n",
       " ['ordre', 'nc'],\n",
       " ['chronologique', 'adj'],\n",
       " ['décroissant', 'v'],\n",
       " ['les', 'det'],\n",
       " ['plus', 'adv'],\n",
       " ['récentes', 'adj'],\n",
       " ['étant', 'v'],\n",
       " ['placées', 'v'],\n",
       " ['au', 'prep'],\n",
       " ['début', 'nc'],\n",
       " ['celui', 'pro'],\n",
       " ['ci', 'adv'],\n",
       " ['est', 'v'],\n",
       " ['sujet', 'adj'],\n",
       " ['à', 'prep'],\n",
       " ['des', 'prep'],\n",
       " ['hebdomadaires', 'nc'],\n",
       " ['les', 'det'],\n",
       " ['fabricants', 'nc'],\n",
       " ['des', 'prep'],\n",
       " ['médicaments', 'nc'],\n",
       " ['présentés', 'v'],\n",
       " ['dans', 'prep'],\n",
       " ['le', 'det'],\n",
       " ['tableau', 'nc'],\n",
       " ['ont', 'v'],\n",
       " ['signé', 'v'],\n",
       " ['une', 'det'],\n",
       " ['lettre', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['autorisation', 'nc'],\n",
       " ['permettant', 'v'],\n",
       " ['la', 'det'],\n",
       " ['publication', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['ces', 'pro'],\n",
       " ['renseignements', 'nc'],\n",
       " ['évaluation', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['mode', 'nc'],\n",
       " ['continu', 'adj'],\n",
       " ['médicaments', 'nc'],\n",
       " ['nom', 'nc'],\n",
       " ['commercial', 'adj'],\n",
       " ['dénomination', 'nc'],\n",
       " ['commune', 'adj'],\n",
       " ['nom', 'nc'],\n",
       " ['du', 'prep'],\n",
       " ['fabricant', 'nc'],\n",
       " ['indication', 'nc'],\n",
       " ['type', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['demande', 'nc'],\n",
       " ['statut', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['demande', 'nc'],\n",
       " ['date', 'v'],\n",
       " ['limite', 'nc'],\n",
       " ['pour', 'prep'],\n",
       " ['commentaires', 'nc'],\n",
       " ['abrilada', 'v'],\n",
       " ['adalimumab', 'nc'],\n",
       " ['pfizer', 'nc'],\n",
       " ['produit', 'v'],\n",
       " ['biosimilaire', 'adj'],\n",
       " ['traitement', 'nc'],\n",
       " ['de', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['hidradénite', 'nc'],\n",
       " ['suppurée', 'adj'],\n",
       " ['réévaluation', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['cours', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['addnutriv', 'adj'],\n",
       " ['agent', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['suppléance', 'nc'],\n",
       " ['fresenius', 'nc'],\n",
       " ['agent', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['suppléance', 'nc'],\n",
       " ['première', '-'],\n",
       " ['demande', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['attente', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['aduhelm', 'nc'],\n",
       " ['aducanumab', 'nc'],\n",
       " ['biogen', 'adj'],\n",
       " ['ralentir', 'v'],\n",
       " ['le', 'det'],\n",
       " ['déclin', 'nc'],\n",
       " ['clinique', 'adj'],\n",
       " ['associé', 'v'],\n",
       " ['à', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['évolution', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['maladie', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['alzheimer', 'v'],\n",
       " ['pré', 'nc'],\n",
       " ['ac', '-'],\n",
       " ['en', 'prep'],\n",
       " ['cours', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['ajovy', 'adj'],\n",
       " ['frémanezumab', 'nc'],\n",
       " ['teva', 'v'],\n",
       " ['innov', 'v'],\n",
       " ['prévention', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['migraine', 'nc'],\n",
       " ['première', '-'],\n",
       " ['demande', 'nc'],\n",
       " ['nouvelle', 'adj'],\n",
       " ['forme', 'v'],\n",
       " ['en', 'prep'],\n",
       " ['cours', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['albrioza', 'nc'],\n",
       " ['projet', 'nc'],\n",
       " ['aligné', 'v'],\n",
       " ['inesss', 'adj'],\n",
       " ['acmts', 'nc'],\n",
       " ['santé', 'nc'],\n",
       " ['canada', 'nc'],\n",
       " ['phénylbutyrate', 'adj'],\n",
       " ['sodium', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['ursodoxicoltaurine', 'nc'],\n",
       " ['amylyx', 'adj'],\n",
       " ['traitement', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['sclérose', 'nc'],\n",
       " ['latérale', 'adj'],\n",
       " ['amyotrophique', 'adj'],\n",
       " ['pré', 'nc'],\n",
       " ['ac', '-'],\n",
       " ['en', 'prep'],\n",
       " ['cours', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['amgevita', 'v'],\n",
       " ['adalimumab', 'nc'],\n",
       " ['amgen', 'adj'],\n",
       " ['traitement', 'nc'],\n",
       " ['de', 'prep'],\n",
       " ['la', 'det'],\n",
       " ['colite', 'nc'],\n",
       " ['ulcéreuse', 'adj'],\n",
       " ['chez', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['enfant', 'nc'],\n",
       " ['pré', 'nc'],\n",
       " ['ac', '-'],\n",
       " ['en', 'prep'],\n",
       " ['attente', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['amgevita', 'v'],\n",
       " ['adalimumab', 'nc'],\n",
       " ['amgen', 'adj'],\n",
       " ['produit', 'v'],\n",
       " ['biosimilaire', 'adj'],\n",
       " ['traitement', 'nc'],\n",
       " ['de', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['hidradénite', 'nc'],\n",
       " ['suppurée', 'adj'],\n",
       " ['réévaluation', 'nc'],\n",
       " ['en', 'prep'],\n",
       " ['cours', 'nc'],\n",
       " [\"d'\", 'prep'],\n",
       " ['évaluation', 'nc'],\n",
       " ['amgevita', 'v'],\n",
       " ['adalimumab', 'nc'],\n",
       " ['amgen', 'adj'],\n",
       " ['produit', 'v'],\n",
       " ['biosimilaire', 'adj'],\n",
       " ['traitement', 'nc'],\n",
       " ['de', 'prep'],\n",
       " [\"l'\", 'det'],\n",
       " ['uvéite', 'nc'],\n",
       " ['non', 'adv'],\n",
       " ['infectieuse', 'adj'],\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574866fe",
   "metadata": {},
   "source": [
    "### **Lemmatisation** (FrenchLefffLemmatizer)\n",
    "\n",
    "https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a494e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41c95983",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = FrenchLefffLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f002566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []\n",
    "for term in tagged:\n",
    "    term_l = []\n",
    "    if lemmatizer.lemmatize(term[0], term[1]) == []:\n",
    "        term_l = [lemmatizer.lemmatize(term[0]), term[1]]\n",
    "    \n",
    "    elif type(lemmatizer.lemmatize(term[0], term[1])) == str:\n",
    "        term_l  = [lemmatizer.lemmatize(term[0], term[1]), term[1]]\n",
    "\n",
    "    else:\n",
    "        term_l = list(lemmatizer.lemmatize(term[0], term[1])[0])\n",
    "    \n",
    "    lemmas.append(term_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab9f5e",
   "metadata": {},
   "source": [
    "### **Filtrage** (antidictionnaire)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d96dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer l'antidictionnaire pour filtrer les données\n",
    "\n",
    "# Stopwords lemmatisés\n",
    "file_path = '../04-filtrage/stopwords_lemmatized.txt'\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords_lemmatized = [w.strip('\\n').lower() for w in f.readlines()]\n",
    "\n",
    "# Stopwords fréquents en français (non lemmatisés)\n",
    "file_path = \"../04-filtrage/stopwords.txt\"\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "\n",
    "# Stopwords fréquents en anglais (non lemmatisés)\n",
    "file_path = '../04-filtrage/stop_words_english.txt'\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords += [t.lower().strip('\\n') for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b5f4d",
   "metadata": {},
   "source": [
    "### **Phrases / N-Grammes (MWE)**\n",
    "https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "addba715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4fc7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammes = list(everygrams(tagged, min_len=2, max_len=5))\n",
    "ngrammes_lemmatized = list(everygrams(lemmas, min_len=2, max_len=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36c13319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['inesss', 'adj'], ['médicaments', 'nc']),\n",
       " (['médicaments', 'nc'], ['évaluation', 'nc']),\n",
       " (['évaluation', 'nc'], ['aux', 'prep']),\n",
       " (['aux', 'prep'], ['fins', 'nc']),\n",
       " (['fins', 'nc'], [\"d'\", 'prep']),\n",
       " ([\"d'\", 'prep'], ['inscription', 'nc']),\n",
       " (['inscription', 'nc'], ['emplois', 'nc']),\n",
       " (['emplois', 'nc'], ['english', 'nc']),\n",
       " (['english', 'nc'], ['inesss', 'adj']),\n",
       " (['inesss', 'adj'], ['express', 'adj']),\n",
       " (['express', 'adj'], ['algorithmes', 'nc']),\n",
       " (['algorithmes', 'nc'], ['en', 'prep']),\n",
       " (['en', 'prep'], ['cancérologie', 'nc']),\n",
       " (['cancérologie', 'nc'], ['consulter', 'v']),\n",
       " (['consulter', 'v'], ['une', 'det']),\n",
       " (['une', 'det'], ['publication', 'nc']),\n",
       " (['publication', 'nc'], ['évaluation', 'nc']),\n",
       " (['évaluation', 'nc'], ['des', 'prep']),\n",
       " (['des', 'prep'], ['médicaments', 'nc']),\n",
       " (['médicaments', 'nc'], ['aux', 'prep'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrammes[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18ecac95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant filtrage, on a 3131973 bigrammes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Avant filtrage, on a {} bigrammes.\".format(len(ngrammes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1d405",
   "metadata": {},
   "source": [
    "### **Filtrage (N-grammes)**\n",
    "\n",
    "On retire les n-grammes qui apparaissent moins de 30 fois dans tout le corpus ou qui débutent ou terminent par :\n",
    "- un stopword\n",
    "- un mot de 1 lettre ou moins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3d97d",
   "metadata": {},
   "source": [
    "Pour le reste du traitement, on arrête de considérer les frontières entre les phrases et entre les documents (nos ngrammes les respectent donc on n'en a plus besoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0ecd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patterns(ngrammes):\n",
    "    patterns = []\n",
    "\n",
    "    for ng in ngrammes:\n",
    "        phrase = []\n",
    "        pattern = []\n",
    "        for t in ng:\n",
    "            phrase.append(t[0]) # token\n",
    "            pattern.append(t[1]) # POS tag\n",
    "\n",
    "        patterns.append([phrase, pattern])\n",
    "        \n",
    "    return patterns\n",
    "\n",
    "phrases = extract_patterns(ngrammes)\n",
    "phrases_lemmatized = extract_patterns(ngrammes_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5ee6d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['inesss', 'médicaments'], ['adj', 'nc']],\n",
       " [['médicaments', 'évaluation'], ['nc', 'nc']],\n",
       " [['évaluation', 'aux'], ['nc', 'prep']],\n",
       " [['aux', 'fins'], ['prep', 'nc']],\n",
       " [['fins', \"d'\"], ['nc', 'prep']],\n",
       " [[\"d'\", 'inscription'], ['prep', 'nc']],\n",
       " [['inscription', 'emplois'], ['nc', 'nc']],\n",
       " [['emplois', 'english'], ['nc', 'nc']],\n",
       " [['english', 'inesss'], ['nc', 'adj']],\n",
       " [['inesss', 'express'], ['adj', 'adj']],\n",
       " [['express', 'algorithmes'], ['adj', 'nc']],\n",
       " [['algorithmes', 'en'], ['nc', 'prep']],\n",
       " [['en', 'cancérologie'], ['prep', 'nc']],\n",
       " [['cancérologie', 'consulter'], ['nc', 'v']],\n",
       " [['consulter', 'une'], ['v', 'det']],\n",
       " [['une', 'publication'], ['det', 'nc']],\n",
       " [['publication', 'évaluation'], ['nc', 'nc']],\n",
       " [['évaluation', 'des'], ['nc', 'prep']],\n",
       " [['des', 'médicaments'], ['prep', 'nc']],\n",
       " [['médicaments', 'aux'], ['nc', 'prep']]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b55a7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist([\" \".join(t[0]).replace(\"' \", \"'\") for t in phrases])\n",
    "freq_lemmatized = FreqDist([\" \".join(t[0]).replace(\"' \", \"'\") for t in phrases_lemmatized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cb2026f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({\"d'évaluation\": 48954, 'en attente': 35656, \"attente d'\": 35653, 'première demande': 29120, 'de la': 28714, 'en cours': 28355, 'demande en': 26802, \"de l'\": 24787, \"cours d'\": 22196, 'traitement de': 19893, ...})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a008d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrer_phrases(phrases, freq):\n",
    "        output = []\n",
    "        for term in phrases:\n",
    "                exp = \" \".join(term[0]).replace(\"' \", \"'\")\n",
    "                f = freq[exp]\n",
    "                \n",
    "                # f > 10 and - sans filtrer par fréquence \n",
    "                if  f > 30 and not term[0][0] in stopwords and len(term[0][0]) > 2 \\\n",
    "                and not term[0][-1] in stopwords and len(term[0][-1]) > 2 : \n",
    "                        pattern = \" \".join(term[1])            \n",
    "                        output.append([exp, pattern, f])  \n",
    "                         \n",
    "        return output\n",
    "\n",
    "phrases = filtrer_phrases(phrases, freq)\n",
    "phrases_lemmatized = filtrer_phrases(phrases_lemmatized, freq_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3edd530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après filtrage, on a 981757 occurrences de bigrammes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Après filtrage, on a {} occurrences de bigrammes.\".format(len(phrases))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be52a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabCSV(phrases, titre):\n",
    "    base_path = '../04-filtrage/output/'\n",
    "    tab = DataFrame(phrases, columns= [\"Expression\", \"Structure syntaxique\", \"Fréquence\"]).drop_duplicates()\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "\n",
    "    file_path = path.join(base_path, acteur, acteur)\n",
    "    if sous_corpus:\n",
    "       file_path = path.join(base_path, acteur, tag, tag)\n",
    "    \n",
    "\n",
    "    Path(file_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tab.to_csv(file_path + titre)\n",
    "\n",
    "    return tab.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3f79a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = tabCSV(phrases,'_n-grams.csv')\n",
    "phrases_lemmatized = tabCSV(phrases_lemmatized, '_n-grams-lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4acce1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après filtrage, on a 2013 bigrammes uniques.\n"
     ]
    }
   ],
   "source": [
    "print(\"Après filtrage, on a {} bigrammes uniques.\".format(len(phrases)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386aa5c",
   "metadata": {},
   "source": [
    "### **Filtrage (Patrons syntaxiques)**  \n",
    "Lossio-Ventura, J. A., Jonquet, C., Roche, M., & Teisseire, M. (2014). Biomedical Terminology Extraction : A new combination of Statistical and Web Mining Approaches. 421. https://hal-lirmm.ccsd.cnrs.fr/lirmm-01056598\n",
    "\n",
    "On veut aller extraire les structures syntaxiques les plus courantes dans les MeSH pour filtrer notre corpus selon celles-ci (inspiré de la méthodologie de l'article ci-dessus ; voir le Notebook *Mesh_extract.ipynb*). Pour ce faire, nous allons donc ne sélectionner que les ngrammes qui y correspondent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15945967",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/mesh_patterns-fr.csv'\n",
    "\n",
    "with open (file_path, 'r') as f:\n",
    "    patterns = read_csv(f)\n",
    "    patterns = patterns['Structure'].tolist()[:100] # Pour prendre seulement les 50 structures syntaxiques les plus fréquentes dans les MeSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1e03045",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [t for t in phrases if t[1] in patterns]\n",
    "terms_lemmatized = [t for t in phrases_lemmatized if t[1] in patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4539e651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le filtrage syntaxique élimine environ 17 % des termes\n",
      "On avait 2013 bigrammes, on en a maintenant 1678.\n"
     ]
    }
   ],
   "source": [
    "print(\"Le filtrage syntaxique élimine environ {} % des termes\".format(round((len(phrases) - len(terms)) / len(phrases) * 100)))\n",
    "print(\"On avait {} bigrammes, \".format(len(phrases)) + \"on en a maintenant {}.\".format(len(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b74b50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_terms(liste_terms, titre):\n",
    "    file_path = '../04-filtrage/output/'\n",
    "    tab = pd.DataFrame(terms, columns= [\"Expression\", \"Structure syntaxique\", \"Fréquence\"]).drop_duplicates()\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "    if sous_corpus:\n",
    "        file_path = path.join(file_path, acteur, tag, tag)\n",
    "\n",
    "    else :\n",
    "        file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "                    \n",
    "    tab.to_csv(file_path + titre)\n",
    "\n",
    "extract_terms(terms, '_terms.csv')\n",
    "extract_terms(terms_lemmatized, '_terms-lemmatized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1baa06e",
   "metadata": {},
   "source": [
    "### **Filtrage (Collocations statistiquement significatives)**\n",
    "\n",
    "[Notebook - Collocation extraction methodologies compared](https://notebooks.githubusercontent.com/view/ipynb?azure_maps_enabled=false&browser=chrome&color_mode=auto&commit=33868e847376764d7733cd958986c88dedfaec97&device=unknown&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f746f64642d636f6f6b2f4d4c2d596f752d43616e2d5573652f333338363865383437333736373634643737333363643935383938366338386465646661656339372f70726f626162696c69737469635f6c616e67756167655f6d6f64656c696e672f636f6c6c6f636174696f6e5f65787472616374696f6e732e6970796e62&enterprise_enabled=false&logged_in=false&nwo=todd-cook%2FML-You-Can-Use&path=probabilistic_language_modeling%2Fcollocation_extractions.ipynb&platform=android&repository_id=167140788&repository_type=Repository&version=102)\n",
    "\n",
    "On applique un test d'hypothèse statistique aux n-grammes sur lesquels une probabilité a été mesurée (Log-likelihood ratio) - seuls les n-grammes dont le test est significatif seront conservés.\n",
    "On considère que l'apparition de ces collocations dans notre corpus n'est pas dûe au hasard.\n",
    "\n",
    "On ne conservera probablement pas cette métrique en fin de compte puisque:  \n",
    "1  - Elle filtre trop de bigrammes (env. 70%)  \n",
    "2  - Elle est beaucoup plus complexe à implémenter sur des N-grammes où N est variable (la question a fait l'objet d'un [mémoire de maîtrise](https://www.d.umn.edu/~tpederse/Pubs/bridget-thesis.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20e82354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Au départ, on a 1678 bigrammes.\n"
     ]
    }
   ],
   "source": [
    "# len_prior = len(terms)\n",
    "# print(\"Au départ, on a {} bigrammes.\".format(len_prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1f4594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from random import sample\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "# from scipy.stats import binom, chi2\n",
    "\n",
    "# def loglikelihood_ratio(c1, c2, c12, N):\n",
    "#     \"\"\"\n",
    "#     Compute the ratio of two hypotheses of likelihood and return the ratio.\n",
    "    \n",
    "#     Under the Independence hypothesis (H0) we assume that there is \n",
    "#     no association between w1 and w2, i.e. they are independent: \n",
    "#     let P(w1) and P(w2) be probabilities that \n",
    "#     a random token in a text is w1 and w2 respectfully and \n",
    "#     P(w1,w2) is the probability that (w1,w2) occur together in the text \n",
    "#     (i.e. one follows another) so under H0, P(w1,w2) = P(w1)P(w2)\n",
    "#     we can compute the observed probability of P(w1,w2) \n",
    "#     and compare it with the probability under H0\n",
    "#     if these probabilities are significantly different from each other,\n",
    "#     then (w1,w2) is a collocation.\n",
    "    \n",
    "#     The formula here and test verification values are taken from \n",
    "#     Manning & Schūtze _Foundations of Statistical Natural Language Processing_ p.172-175\n",
    "    \n",
    "#     Parameters:\n",
    "#     c1: count of word 1\n",
    "#     c2: count of word 2\n",
    "#     c12: count of bigram (w1, w2)\n",
    "#     N: the number of words in the corpus\n",
    "#     \"\"\"\n",
    "    \n",
    "#     p = c2 / N\n",
    "#     p1 = c12 / c1\n",
    "#     p2 = (c2 - c12) / (N - c1)   \n",
    "#     # We proactively trap a runtimeWarning: divide by zero encountered in log,\n",
    "#     # which may occur with extreme collocations\n",
    "#     import warnings\n",
    "#     with warnings.catch_warnings(): # this will reset our filterwarnings setting\n",
    "#         warnings.filterwarnings('error')\n",
    "#         try:\n",
    "#             return (np.log(binom.pmf(c12, c1, p)) \n",
    "#                     + np.log(binom.pmf(c2 - c12, N - c1, p)) \n",
    "#                     - np.log(binom.pmf(c12, c1, p1) )\n",
    "#                     - np.log(binom.pmf(c2 - c12, N - c1, p2)))             \n",
    "#         except Warning:\n",
    "#             return np.inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4609ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms_bg = [b[0] for b in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7daa5832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a maintenant 504 bigrammes dont la collocation des termes est significative.\n",
      "Ça veut dire que l'extraction de collocations significatives nous permet de filter environ 69.9642431466031 % des termes.\n"
     ]
    }
   ],
   "source": [
    "# N = len(tokens)\n",
    "# output = []\n",
    "# for b in terms_bg:\n",
    "#     try:\n",
    "#         c1 = tokens.count(b.split()[0])\n",
    "#         c2 = tokens.count(b.split()[1])\n",
    "#         c12 = terms.count(b)\n",
    "\n",
    "#         res = -2 * loglikelihood_ratio(c1, c2, c12, N)\n",
    "#         p = chi2.sf(res, 1) # 1 degrees of freedom\n",
    "\n",
    "#         if p < 0.05 or (res == float('-inf') and p == 1):\n",
    "#             output.append([b, res, p])\n",
    "\n",
    "\n",
    "#         df = pd.DataFrame(output, columns=['Bigramme', 'Log-Likelihood Ratio', 'p-value']).drop_duplicates()\n",
    "#         output_path = '../04-filtrage/output/'\n",
    "\n",
    "#         if sous_corpus:\n",
    "#             output_path = path.join(output_path, acteur, tag, tag)\n",
    "\n",
    "#         else :\n",
    "#             output_path = path.join(output_path, acteur, acteur)\n",
    "\n",
    "#         df.sort_values(['Log-Likelihood Ratio'], \n",
    "#                     axis=0,\n",
    "#                     ascending=[False], \n",
    "#                     inplace=True)\n",
    "\n",
    "#         df.to_csv(output_path + \"_LLR.csv\")\n",
    "\n",
    "#         len_after = len(output)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(str(b) + \" - \" + str(e))\n",
    "\n",
    "# print(\"On a maintenant {} bigrammes dont la collocation des termes est significative.\".format(len_after))\n",
    "# print(\"Ça veut dire que l'extraction de collocations significatives nous permet de filter environ {} % des termes.\".format((len_prior - len_after) / len_prior * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ef6e6",
   "metadata": {},
   "source": [
    "### **KWIC (Keyword in Context)**\n",
    "Termes d'intérêt : \n",
    "- « Programme »\n",
    "- « Plan »\n",
    "- « Service(s) de » \n",
    "- « Intervenant(e) en »\n",
    "- « Professionnel de »\n",
    "- « Institut (du/de) »\n",
    "- « Groupe de recherche en »\n",
    "- « Personne »\n",
    "- « Infirmière (en) »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad7771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans notre cas on veut que ça débute par le mot-clé donc le contexte est un peu plus simple\n",
    "# penser à généraliser avec des expressions régulières\n",
    "kw = ['programme', 'plan ', 'service', 'intervenant', 'infirmière en', 'institut', 'groupe de recherche', 'personne', 'maladie']\n",
    "\n",
    "ngrammes_kwic = [\" \".join([t[0].replace(\"' \", \"'\") for t in ng]) for ng in ngrammes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4c50b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrant = pd.DataFrame(columns=['Mot-clé','Concordance', 'Fréquence'])\n",
    "kwic = {w : [] for w in kw} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "255df07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ngrammes_kwic: # on pourrait aussi chercher dans les terms, mais on perd certains termes d'intérêt avec le filtrage syntaxique\n",
    "    for w in kw:\n",
    "        if t.startswith(w):\n",
    "            kwic[w].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e8fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic = {term: FreqDist(kwic[term]) for term in kwic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "645a8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in kw:\n",
    "    df = pd.DataFrame(kwic[term].items(), columns=['Concordance', \"Fréquence\"])\n",
    "    df.sort_values([\"Fréquence\"], \n",
    "        axis=0,\n",
    "        ascending=[False], \n",
    "        inplace=True)\n",
    "\n",
    "    df.insert(0, 'Mot-clé', term)\n",
    "    extrant = pd.concat([extrant, df])\n",
    "\n",
    "\n",
    "extrant = extrant[extrant['Fréquence'] > 30] \n",
    "\n",
    "file_path = '../04-filtrage/output/'\n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag)\n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "\n",
    "extrant.to_csv(file_path + '_KWIC' +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45949eb3",
   "metadata": {},
   "source": [
    "### **Extraction de termes MeSH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4c50c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "file_path = '../04-filtrage/mesh-fr.txt'\n",
    "\n",
    "with open (file_path, 'r', encoding='utf-8') as f:\n",
    "    mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "    tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "    mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "    mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "    #mesh = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b2c0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_mesh = tokenizer_mesh.tokenize([t[0] for t in terms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f53c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = []\n",
    "\n",
    "for t in extr_mesh:\n",
    "    if t in mesh:\n",
    "        termes_mesh.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb21d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/output/'\n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag)\n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "df = DataFrame(termes_mesh)\n",
    "df.to_csv(file_path + '_MeSH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685131ee",
   "metadata": {},
   "source": [
    "### **Extraction de termes SNOMED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bb27c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "file_path = '../04-filtrage/SNOMED_fr.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    sm = read_csv(f, sep=';')\n",
    "    sm = list(dict.fromkeys([str(t).strip().lower() for t in sm['term'].tolist()]))\n",
    "\n",
    "    sm = [tuple(tokenizer_re.tokenize(w)) for w in sm if len(w.split()) > 1]\n",
    "    tokenizer_sm = MWETokenizer(sm, separator = ' ')\n",
    "\n",
    "    sm = [tokenizer_sm.tokenize(w)[0].lower() for w in sm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3462f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_sm = tokenizer_sm.tokenize([t[0] for t in terms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41c006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = []\n",
    "\n",
    "for t in extr_sm:\n",
    "    if t in sm:\n",
    "        termes_sm.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "645e5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/output' \n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag) \n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "\n",
    "df = DataFrame(termes_sm)\n",
    "\n",
    "df.to_csv(file_path + '_SNOMED.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79bb76bbc4f9ba1f8df5efe8db67aae07079a51dc7b5004f49990e90f5993a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
