{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c662c7d",
   "metadata": {},
   "source": [
    "## **2. Prétraitement**\n",
    "- Segmentation (phrases)\n",
    "- Tokenization (mots)\n",
    "- Étiquetage morphosyntaxique (POS Tagging) \n",
    "- (Lemmatisation)\n",
    "- Filtrage (stopwords)\n",
    "- Extraction de termes complexes (MWE / n-grammes / segments répétés)\n",
    "- Chunking / Filtrage par patrons syntaxiques (basés sur les patrons fréquents dans les MeSH)\n",
    "- Extraction de collocations significatives (en fonction du Log-likelihood ratio)\n",
    "- (Extraction de concordances (KWIC) pour un ensemble de mots-clés d'intérêt)\n",
    "- (Extraction de termes MeSH et SNOMED présents dans les données)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece38e9",
   "metadata": {},
   "source": [
    "### **Lire le corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec78871",
   "metadata": {},
   "outputs": [],
   "source": [
    "lng = 'en'\n",
    "acteur = 'chum'\n",
    "tag = ''\n",
    "if tag:\n",
    "    file = acteur + '/' + acteur + '_' + tag + '.csv'\n",
    "\n",
    "else:\n",
    "    if lng == 'fr':\n",
    "        file = acteur +'.csv'\n",
    "    if lng == 'en':\n",
    "        file = acteur + '_en.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef60676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, re, random\n",
    "from os import listdir, chdir, path\n",
    "from pathlib import Path\n",
    "from pandas import *\n",
    "\n",
    "import nltk\n",
    "#nltk.download(['popular'])\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w\\'|\\w+\")\n",
    "from nltk import bigrams, trigrams, ngrams, everygrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG=lng)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb38ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lire_corpus(csv_file, langue=lng):\n",
    "    base_path = '../03-corpus/2-data/'\n",
    "    file_path = path.join(base_path, '1-' + langue, csv_file)\n",
    "\n",
    "    #encoding='ISO 8859-1'\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        data = read_csv(file_path, encoding='utf-8', sep=',')\n",
    "        data = data[~data[\"url\"].str.contains('pdf')] #  Problèmes \n",
    "        data = data['text']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41376091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a un corpus de 763 documents.\n"
     ]
    }
   ],
   "source": [
    "data = lire_corpus(file) \n",
    "nb_docs = len(data)\n",
    "print(\"On a un corpus de {} documents.\".format(nb_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd537ec8",
   "metadata": {},
   "source": [
    "### **Nettoyage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = '[!#$%&\\(\\)•►*+,-\\/:;<=>?@[\\]^_{|}~©«»—“”–—]'\n",
    "spaces = '\\s+'\n",
    "postals = '([a-zA-Z]+\\d+|\\d+[a-zA-Z]+)+'\n",
    "phones = '\\d{3}\\s\\d{3}-\\d{4}' #très simple (trop)\n",
    "\n",
    "text = [str(t).strip('\\n').lower().replace('’', '\\'') for t in data]\n",
    "text = [re.sub(spaces, ' ', t) for t in text]\n",
    "text = [re.sub(phones, ' ', t) for t in text]\n",
    "text = [re.sub(postals, ' ', t) for t in text]\n",
    "text = [re.sub(punct, ' ', t) for t in text]\n",
    "text = [t.replace(\"  \", \" \" ) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fda380",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_docs = len(text)\n",
    "print(\"On a un corpus de {} documents.\".format(nb_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62437edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[52]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89c4d8",
   "metadata": {},
   "source": [
    "### **Extraire un échantillon aléatoire**\n",
    "\n",
    "Sinon, on n'arrive pas à traiter la totalité du corpus pour des raisons de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69841ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_corpus(corpus, ratio):\n",
    "    n = round(ratio * len(corpus))\n",
    "    corpus = random.sample(text, n)\n",
    "    print(\"On va travailler sur un échantillon correspondant à environ \" + str(ratio * 100) + \" % des documents du corpus, soit {} documents\". format(len(corpus)))\n",
    "    return \" \".join(corpus)\n",
    "    \n",
    "corpus = sample_corpus(text, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b5d73",
   "metadata": {},
   "source": [
    "### **Filtrage (MWE - stopwords formés de plusieurs tokens)**\n",
    "Surtout pour filtrer les expressions relatives à l'architecture d'information / navigation Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mwesw(corpus):\n",
    "    file_mwesw = '../04-filtrage/mwe_stopwords.txt'\n",
    "    with open (file_mwesw, 'r', encoding='utf-8') as f:\n",
    "        mwe_sw = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "    for mwe in mwe_sw:\n",
    "        corpus = corpus.replace(mwe, ' MWE_STOP ').replace('  ', \" \")\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7820d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = filter_mwesw(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980c335",
   "metadata": {},
   "source": [
    "### **Tokenisation / POS tagging** (TreeTagger)  \n",
    "https://github.com/miotto/treetagger-python/blob/master/README.rst  \n",
    "https://treetaggerwrapper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29725ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, on tokenise une première fois avec le Regex Tokenizer de NLTK pour \"forcer\" le Tree Tagger à tokeniser comme on veut\n",
    "def tok(corpus):\n",
    "    # Seulement les caractères alphabétiques\n",
    "    tokens = tokenizer_re.tokenize(corpus)\n",
    "    print(\"Avec le RegExpTokenizer, notre corpus contient {} tokens.\".format(len(tokens)))\n",
    "    temps = round(len(tokens) / 15000 / 60)\n",
    "    print('Le POS tagging devrait prendre environ {} minutes.'.format(temps))\n",
    "    return tokens\n",
    "\n",
    "tokens = tok(corpus)\n",
    "corpus = \" \".join(tokens).replace(\"' \", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237db34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging(corpus):\n",
    "    output = []\n",
    "    for t in tagger.tag_text(corpus):\n",
    "        try: \n",
    "            output.append((t.split('\\t')[0], t.split('\\t')[1]))\n",
    "        except Exception as e:\n",
    "            output.append(('MWE_STOP', 'NAM'))\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6feae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = tagging(corpus)\n",
    "tokens = [t[0] for t in tagged]\n",
    "tagged[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si on veut lemmatiser\n",
    "### **Mapping POS Tags** (FRMG)\n",
    "\n",
    "#Pour utiliser adéquatement notre lemmatiseur par la suite (FrenchLefffLemmatizer), on va mapper les étiquettes morphosyntaxiques du TreeTagger à celles que prend le lemmatiseur (celles issues de FRMG)\n",
    "\n",
    "#http://alpage.inria.fr/frmgwiki/content/tagset-frmg\n",
    "\n",
    "#file_path = '../04-filtrage/mapping_treeTagger_lefff.csv'\n",
    "\n",
    "#with open(file_path) as f:\n",
    "#    csv = read_csv(f)\n",
    "\n",
    "#treeTag = [term for term in csv['TreeTagger'].tolist()] \n",
    "#lefff = [term for term in csv['Lefff'].tolist()]\n",
    "\n",
    "#mapping = {term : lefff[treeTag.index(term)] for term in treeTag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged = [[t.split('\\t')[0], mapping[t.split('\\t')[1]]] for t in tagger.tag_text(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si on veut lemmatiser\n",
    "### **Lemmatisation** (FrenchLefffLemmatizer)\n",
    "#https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer\n",
    "\n",
    "#from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "#lemmatizer = FrenchLefffLemmatizer()\n",
    "\n",
    "#lemmas = []\n",
    "#for term in tagged:\n",
    "#    term_l = []\n",
    "#    if lemmatizer.lemmatize(term[0], term[1]) == []:\n",
    "#        term_l = (lemmatizer.lemmatize(term[0]), term[1])\n",
    "    \n",
    "    # elif type(lemmatizer.lemmatize(term[0], term[1])) == str:\n",
    "    #     term_l  = (lemmatizer.lemmatize(term[0], term[1]), term[1])\n",
    "\n",
    "    # else:\n",
    "    #     term_l = tuple(lemmatizer.lemmatize(term[0], term[1])[0])\n",
    "    \n",
    "    # lemmas.append(term_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b5f4d",
   "metadata": {},
   "source": [
    "### **Collocations / Phrases / N-Grammes (MWE)**\n",
    "https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extr_ngrams(tagged):\n",
    "    ngrammes= list(everygrams(tagged, min_len=2, max_len=6))\n",
    "    print(\"Avant filtrage, on a {} ngrammes.\".format(len(ngrammes)))\n",
    "    return ngrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammes = extr_ngrams(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammes[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1d405",
   "metadata": {},
   "source": [
    "### **Extraction des patrons syntaxiques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ddbd4",
   "metadata": {},
   "source": [
    "*LONG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ecd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patterns(ngrammes):\n",
    "    patterns = []\n",
    "    for ng in ngrammes:\n",
    "        phrase = [t[0] for t in ng]\n",
    "        pattern = [t[1] for t in ng]\n",
    "        patterns.append([phrase, pattern])\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = extract_patterns(ngrammes)\n",
    "# phrases_lemmatized = extract_patterns(ngrammes_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(phrases):\n",
    "    return FreqDist([\" \".join(t[0]).replace(\"' \", \"'\") for t in phrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4629f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = freq(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93e86e",
   "metadata": {},
   "source": [
    "### **Filtrage** \n",
    "On retire les n-grammes qui débutent ou se terminent par un stopword (antidictionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer l'antidictionnaire pour filtrer les données\n",
    "\n",
    "# # Stopwords lemmatisés\n",
    "# file_path = '../04-filtrage/stopwords_lemmatized.txt'\n",
    "# with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "#     stopwords_lemmatized = [w.strip('\\n').lower() for w in f.readlines()]\n",
    "\n",
    "# Stopwords fréquents en français (non lemmatisés)\n",
    "file_path = \"../04-filtrage/stopwords.txt\"\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "\n",
    "# Stopwords fréquents en anglais (non lemmatisés)\n",
    "file_path = '../04-filtrage/stop_words_english.txt'\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords += [t.lower().strip('\\n') for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5131d18",
   "metadata": {},
   "source": [
    "*LONG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52036bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrer_stopwords(x): # On s'assure aussi que le premier terme du ngramme est un NOM pour avoir des syntagmes nominaux\n",
    "    if lng == 'en':\n",
    "        nom = 'NN'\n",
    "    if lng == 'fr':\n",
    "        nom = 'NOM'\n",
    "    return [term for term in x if not 'MWE_STOP' in term[0] and not term[0][0] in stopwords and not term[0][-1] in stopwords and not 'NUM' in term[1] and term[1][0] == nom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86835f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = filtrer_stopwords(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26da835",
   "metadata": {},
   "source": [
    "On retire les n-grammes qui débutent ou se terminent par token dont la longueur est inférieure à 2 caractères ou supérieure à 18 caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28750634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_len(x):\n",
    "    return [term for term in x if \\\n",
    "        len(term[0][0]) > 2 and len(term[0][0]) < 18 and \\\n",
    "        len(term[0][-1]) > 2 and len(term[0][-1]) < 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = filter_len(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a78185",
   "metadata": {},
   "source": [
    "On retire les n-grammes qui apparaissent moins de 3 fois dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dca349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_freq(x):\n",
    "    return [term for term in x if frequencies[\" \".join(term[0]).replace(\"' \", \"'\")] > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = filter_freq(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [[\" \".join(term[0]).replace(\"' \", \"'\"), \" \".join(term[1])] for term in phrases]\n",
    "# phrases_lemmatized = filtrer_phrases(phrases_lemmatized, freq_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ced2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase in phrases:\n",
    "    phrase.append(frequencies[phrase[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3edd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Après filtrage, on a {} occurrences de ngrammes.\".format(len(phrases))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d889bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(phrases, columns=[\"Expression\", \"Patron syntaxique\", \"Fréquence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabCSV(phrases):\n",
    "    base_path = '../04-filtrage/output/'\n",
    "    tab = DataFrame(phrases, columns=[\"Expression\", \"Patron syntaxique\", \"Fréquence\"]).drop_duplicates()\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "\n",
    "    # file_path = path.join(base_path, tag, tag)    \n",
    "    # Path(file_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #tab.to_csv(file_path + '_ngrams.csv') - On va seulement garder les collocations significatives comme output\n",
    "\n",
    "    return tab.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f79a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = tabCSV(phrases)\n",
    "# phrases_lemmatized = tabCSV(phrases_lemmatized, '_n-grams-lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acce1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Après filtrage, on a {} ngrammes uniques.\".format(len(phrases)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386aa5c",
   "metadata": {},
   "source": [
    "### **Filtrage (Patrons syntaxiques)**  \n",
    "Lossio-Ventura, J. A., Jonquet, C., Roche, M., & Teisseire, M. (2014). Biomedical Terminology Extraction : A new combination of Statistical and Web Mining Approaches. 421. https://hal-lirmm.ccsd.cnrs.fr/lirmm-01056598\n",
    "\n",
    "On veut aller extraire les structures syntaxiques les plus courantes dans les MeSH pour filtrer notre corpus selon celles-ci (inspiré de la méthodologie de l'article ci-dessus ; voir le Notebook *Mesh_extract.ipynb*). Pour ce faire, nous allons donc ne sélectionner que les ngrammes qui y correspondent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15945967",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_patterns = '../04-filtrage/MeSH/mesh_patterns-fr.csv'\n",
    "\n",
    "with open (file_patterns, 'r') as f:\n",
    "    patterns = read_csv(f)\n",
    "    patterns = patterns['Structure'].tolist() #[:200] # On prend les 200 structures syntaxiques les plus fréquentes dans les MeSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e03045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_patterns(phrases):\n",
    "    return [t for t in phrases if t[1] in patterns and not 'NOM NOM' in t[1] and not 'NUM' in t[1]] # \n",
    "# terms_lemmatized = [t for t in phrases_lemmatized if t[1] in patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39570e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = filter_patterns(phrases)\n",
    "terms[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le filtrage syntaxique élimine environ {} % des termes\".format(round((len(phrases) - len(terms)) / len(phrases) * 100)))\n",
    "print(\"On avait {} ngrammes, \".format(len(phrases)) + \"on en a maintenant {}.\".format(len(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def extract_terms(liste_terms):\n",
    "#     file_path = '../04-filtrage/output/'\n",
    "#     tab = pd.DataFrame(terms, columns= [\"Expression\", \"Structure syntaxique\", \"Fréquence\"]).drop_duplicates(subset='Expression', keep=\"last\")\n",
    "#     tab.sort_values([\"Fréquence\"], \n",
    "#                         axis=0,\n",
    "#                         ascending=[False], \n",
    "#                         inplace=True)\n",
    "\n",
    "#     return \n",
    "\n",
    "#     # file_path = path.join(file_path, tag, tag)                    \n",
    "#     # tab.to_csv(file_path + '_terms.csv')\n",
    "\n",
    "# extract_terms(terms)\n",
    "# #extract_terms(terms_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms = phrases\n",
    "\n",
    "terms_patterns = DataFrame(terms, columns = [\"Expression\", \"Structure syntaxique\", \"Fréquence\"])\n",
    "terms_patterns = terms_patterns.to_dict('records')\n",
    "dict_patterns = {}\n",
    "for term in terms_patterns:\n",
    "     exp = term['Expression']\n",
    "     pattern = term['Structure syntaxique']\n",
    "     dict_patterns[exp] = pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1baa06e",
   "metadata": {},
   "source": [
    "### **Filtrage (Collocations statistiquement significatives)** Log-Likelihood Ratio\n",
    "\n",
    "[Notebook - Collocation extraction methodologies compared](https://notebooks.githubusercontent.com/view/ipynb?azure_maps_enabled=false&browser=chrome&color_mode=auto&commit=33868e847376764d7733cd958986c88dedfaec97&device=unknown&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f746f64642d636f6f6b2f4d4c2d596f752d43616e2d5573652f333338363865383437333736373634643737333363643935383938366338386465646661656339372f70726f626162696c69737469635f6c616e67756167655f6d6f64656c696e672f636f6c6c6f636174696f6e5f65787472616374696f6e732e6970796e62&enterprise_enabled=false&logged_in=false&nwo=todd-cook%2FML-You-Can-Use&path=probabilistic_language_modeling%2Fcollocation_extractions.ipynb&platform=android&repository_id=167140788&repository_type=Repository&version=102)\n",
    "\n",
    "On applique un test d'hypothèse statistique aux n-grammes sur lesquels une probabilité a été mesurée (Log-likelihood ratio) - seuls les n-grammes dont le test est significatif seront conservés.\n",
    "On considère que l'apparition de ces collocations dans notre corpus n'est pas dûe au hasard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc17f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood_ratio(c_prior, c_n, c_ngram, N):\n",
    "    \"\"\"\n",
    "    Compute the ratio of two hypotheses of likelihood and return the ratio.\n",
    "    The formula here and test verification values are taken from \n",
    "    Manning & Schūtze _Foundations of Statistical Natural Language Processing_ p.172-175\n",
    "    Parameters:\n",
    "    c_prior: count of word 1 if bigrams or count of [w1w2 .. w(n-1)] if ngram\n",
    "    c_n : count of word 2 if bigrams or count of wn if ngram\n",
    "    c12: count of bigram (w1, w2) if bigram or count of ngram if ngram\n",
    "    N: the number of words in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    p = c_n / N\n",
    "    p1 = c_ngram / c_prior\n",
    "    p2 = (c_n - c_ngram) / (N - c_prior)   \n",
    "    # We proactively trap a runtimeWarning: divide by zero encountered in log,\n",
    "    # which may occur with extreme collocations\n",
    "    import warnings\n",
    "    with warnings.catch_warnings(): # this will reset our filterwarnings setting\n",
    "        warnings.filterwarnings('error')\n",
    "        try:\n",
    "            return (np.log(binom.pmf(c_ngram, c_prior, p)) \n",
    "                    + np.log(binom.pmf(c_n - c_ngram, N - c_prior, p)) \n",
    "                    - np.log(binom.pmf(c_ngram, c_prior, p1) )\n",
    "                    - np.log(binom.pmf(c_n - c_ngram, N - c_prior, p2)))             \n",
    "        except Warning:\n",
    "            return np.inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e82354",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_prior = len(terms)\n",
    "print(\"Au départ, on a {} ngrammes.\".format(len_prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le calcul des probabilités, on a besoin de traiter séparément les ngrammes selon la valeur de n\n",
    "N = len(tokens)\n",
    "fd_tokens = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ffe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llr_ngrammes(n):\n",
    "    llr = []\n",
    "\n",
    "    for i in range(2, n+1):\n",
    "        ngrammes = set([tuple(tokenizer_re.tokenize(term[0])) for term in terms if len(tokenizer_re.tokenize(term[0])) == i])\n",
    "        fd = nltk.FreqDist(ngrams(tokens, n=i))\n",
    "        fd_prior = nltk.FreqDist(ngrams(tokens, n=i-1))\n",
    "        \n",
    "        for t in ngrammes:\n",
    "            c_prior = fd_prior[t[:i-1]] # Antécédent = P(w1w2..w_n-1) (si on considère que P(w1w2...wn) = P(wn) | P(w1w2...w_n-1)\n",
    "            c_n = fd_tokens[t[i-1]]     # Dernier mot du ngramme  P(wn)\n",
    "            c_ngram = fd[t]             # Le ngramme lui-même P(w1w2w3..wn)\n",
    "\n",
    "            try:\n",
    "                res = -2 * loglikelihood_ratio(c_prior, c_n, c_ngram, N)\n",
    "                p_value = chi2.sf(res, 1) # 1 degrees of freedom\n",
    "                #if res == float('-inf') :\n",
    "                #    res = 50000\n",
    "\n",
    "                if p_value < 0.001 or (res == float('-inf')):\n",
    "                    llr.append({'Collocation' : \" \".join(t).replace(\"' \", \"'\"), 'Structure syntaxique': dict_patterns[\" \".join(t).replace(\"' \", \"'\")], 'Fréquence' : c_ngram, 'LLR': res, 'p-value': p_value})\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(t, str(e))\n",
    "            \n",
    "\n",
    "    return llr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = llr_ngrammes(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms\n",
    "#terms = [{'Collocation' : t[0], 'Structure syntaxique': t[1], 'Fréquence' : t[2]} for t in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cddc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Après avoir calculé le log-likelihood ratio, on a retiré {} collocations qui n\\'étaient pas statistiquement significatives.'.format(len_prior - len(terms)))\n",
    "print('Ça représente environ {} % de nos n-grammes.'.format(round((len_prior - len(terms)) / len_prior *100 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(terms).sort_values(by = \"Fréquence\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f235b77",
   "metadata": {},
   "source": [
    "### **Filtrage - Fréquence documentaire**\n",
    "** Il y aurait quelque chose à modifier ici ; en tokenisant, on supprimer les frontières entre les mots qui sont des tirets ou autres caractères qu'un espace ou un apostrophe ; ça fait en sorte qu'on a une fréquence documentaire de 0 pour les ngrammes qui n'ont plus la forme exacte qu'ils avaient dans le corpus orginal. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ca161",
   "metadata": {},
   "source": [
    "*LONG*  \n",
    "Est-ce qu'il y a moyen de rendre ça plus efficace ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {term['Collocation']: len([doc for doc in text if term['Collocation'] in doc]) for term in terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = round(0.95 * nb_docs)        # Pour rejeter les termes qui se retrouvent dans plus de 95% des documents \n",
    "max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dfs[max(dfs, key=dfs.get)] # Voir quelle est la fréquence documentaire maximale qu'on retrouve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = {term for term in dfs if dfs[term] == 0}\n",
    "dfs_100 = {term for term in dfs if dfs[term] >= max_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0197d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [t for t in terms if dfs[t['Collocation']] < max_df ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#colloc = [term['Collocation'] for term in terms]\n",
    "#len(colloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for term in terms:\n",
    " #   term['DF'] = dfs[term['Collocation']] \n",
    "DataFrame(terms).sort_values(by=\"Fréquence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c08c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(terms)\n",
    "df.sort_values(['Fréquence'], \n",
    "            axis=0,\n",
    "            ascending=[False], \n",
    "            inplace=True)\n",
    "if lng == 'en':\n",
    "    acteur = acteur + '_' + lng\n",
    "if tag:\n",
    "    output_path = path.join('../04-filtrage/output/', acteur + '_' + tag + '_significant-collocations.csv') \n",
    "else:\n",
    "    output_path = path.join('../04-filtrage/output/', acteur + '_significant-collocations.csv') \n",
    "df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80119408",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_terms = [term['Collocation'] for term in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = DataFrame(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ef6e6",
   "metadata": {},
   "source": [
    "### **KWIC (Keyword in Context)**\n",
    "Termes d'intérêt : \n",
    "- « Programme »\n",
    "- « Plan »\n",
    "- « Service(s) de » \n",
    "- « Intervenant(e) en »\n",
    "- « Professionnel de »\n",
    "- « Institut (du/de) »\n",
    "- « Groupe de recherche en »\n",
    "- « Personne »\n",
    "- « Infirmière (en) »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans notre cas on veut que ça débute par le mot-clé donc le contexte est un peu plus simple\n",
    "# penser à généraliser avec des expressions régulières\n",
    "kw = ['programme', 'plan ', 'service', 'intervenant', 'infirmière en', 'institut', 'groupe de recherche', 'personne', 'maladie']\n",
    "\n",
    "ngrammes_kwic = [\" \".join([t[0] for t in ng]).replace(\"' \", \"'\") for ng in ngrammes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c50b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrant = DataFrame(columns=['Mot-clé','Concordance', 'Fréquence'])\n",
    "kwic = {w : [] for w in kw} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255df07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ngrammes_kwic: # on pourrait aussi chercher dans les terms, mais on perd certains termes d'intérêt avec le filtrage syntaxique\n",
    "    for w in kw:\n",
    "        if t.startswith(w):\n",
    "            kwic[w].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic = {term: FreqDist(kwic[term]) for term in kwic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in kw:\n",
    "    df = DataFrame(kwic[term].items(), columns=['Concordance', \"Fréquence\"])\n",
    "    df.sort_values([\"Fréquence\"], \n",
    "        axis=0,\n",
    "        ascending=[False], \n",
    "        inplace=True)\n",
    "\n",
    "    df.insert(0, 'Mot-clé', term)\n",
    "    extrant = concat([extrant, df])\n",
    "\n",
    "\n",
    "extrant = extrant[extrant['Fréquence'] > 30] \n",
    "\n",
    "file_path = '../04-filtrage/output/'\n",
    "file_path = path.join(file_path, tag, tag)\n",
    "\n",
    "\n",
    "#extrant.to_csv(file_path + '_KWIC' +'.csv')\n",
    "\n",
    "extrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45949eb3",
   "metadata": {},
   "source": [
    "### **Extraction de termes MeSH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "file_path = '../04-filtrage/MeSH/mesh-fr.txt'\n",
    "\n",
    "with open (file_path, 'r', encoding='utf-8') as f:\n",
    "    mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "    tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "    mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "    mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "    #mesh = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extr_mesh = tokenizer_mesh.tokenize([t['Collocation'] for t in terms])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40edc2e1",
   "metadata": {},
   "source": [
    "# **MODIF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFICATION À APPORTER : dans l'extrant où on voit les termes mesh, ajouter la fréquence\n",
    "\n",
    "termes_mesh[:15]\n",
    "\n",
    "for t in termes_mesh:\n",
    "    print(t, corpus.count(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = []\n",
    "\n",
    "for t in extr_mesh:\n",
    "    if t in mesh:\n",
    "        termes_mesh.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/output/'\n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag)\n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "df = DataFrame(termes_mesh)\n",
    "df.to_csv(file_path + '_MeSH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685131ee",
   "metadata": {},
   "source": [
    "### **Extraction de termes SNOMED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "file_path = '../04-filtrage/SNOMED_fr.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    sm = read_csv(f, sep=';')\n",
    "    sm = list(dict.fromkeys([str(t).strip().lower() for t in sm['term'].tolist()]))\n",
    "\n",
    "    sm = [tuple(tokenizer_re.tokenize(w)) for w in sm if len(w.split()) > 1]\n",
    "    tokenizer_sm = MWETokenizer(sm, separator = ' ')\n",
    "\n",
    "    sm = [tokenizer_sm.tokenize(w)[0].lower() for w in sm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_sm = tokenizer_sm.tokenize([t[0] for t in terms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = []\n",
    "\n",
    "for t in extr_sm:\n",
    "    if t in sm:\n",
    "        termes_sm.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/output' \n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag) \n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "\n",
    "df = DataFrame(termes_sm)\n",
    "\n",
    "df.to_csv(file_path + '_SNOMED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10875e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79bb76bbc4f9ba1f8df5efe8db67aae07079a51dc7b5004f49990e90f5993a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
