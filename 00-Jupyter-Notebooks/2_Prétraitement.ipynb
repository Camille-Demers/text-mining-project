{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c662c7d",
   "metadata": {},
   "source": [
    "## **2. Prétraitement**\n",
    "- Segmentation (phrases)\n",
    "- Tokenization (mots)\n",
    "- Étiquetage morphosyntaxique (POS Tagging) \n",
    "- Lemmatisation\n",
    "- Filtrage (stopwords)\n",
    "- Extraction de termes complexes (MWE / n-grammes / segments répétés)\n",
    "- Chunking / Filtrage par patrons syntaxiques (basés sur les patrons fréquents dans les MeSH)\n",
    "- Extraction de concordances (KWIC) pour un ensemble de mots-clés d'intérêt\n",
    "- Extraction de termes MeSH et SNOMED présents dans les données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604b66c",
   "metadata": {},
   "source": [
    "*Suivi des modifications apportées*  \n",
    "2022-07-04\n",
    "- Étiquetage morpho-syntaxique et Lemmatisation plus tôt dans le traitement pour améliorer la performance et la rapidité des deux outils.\n",
    "\n",
    "\n",
    "2022-07-05\n",
    "- Échantillonnage aléatoire du corpus (pour ne pas traiter la totalité des documents)\n",
    "- (Log)-likelihood ratio (à suivre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece38e9",
   "metadata": {},
   "source": [
    "### **Lire le corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a62415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, re\n",
    "from os import listdir, chdir, path\n",
    "from pathlib import Path\n",
    "\n",
    "acteurs = ['asso_ordres', 'chsld', 'chu_iu', 'cisss_ciusss', 'cliniques_medicales', 'csbe', 'gmf', 'inesss', 'inspq', 'msss', 'ophq', 'quebec_sante', 'ramq', 'sante_mtl', 'urgence_sante']\n",
    "acteur = 'msss'\n",
    "sous_corpus = False \n",
    "tag = ''\n",
    "\n",
    "# Change the directory\n",
    "if sous_corpus:\n",
    "    base_path = '../03-corpus/2-sous-corpus/'\n",
    "    file_path = path.join(base_path, acteur, tag)\n",
    "\n",
    "else: \n",
    "    base_path = '../03-corpus/2-data/1-fr/'\n",
    "    file_path = path.join(base_path, acteur) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "with open(file_path, \"r\", encoding = \"UTF-8\") as f:\n",
    "        data = read_csv(file_path)\n",
    "        text = data['text'].tolist()\n",
    "        corpus = [(re.sub('\\d', '', t.strip('\\n').lower().replace('’', '\\''))) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fda380",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_docs = len(corpus)\n",
    "\n",
    "print(\"On a un corpus de {} documents.\".format(nb_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72532f26",
   "metadata": {},
   "source": [
    "### **Extraire un échantillon aléatoire**  \n",
    "On a vu (`0-Description_corpus.ipynb`) qu'à partir d'un certain nombre de documents traités, le nombre de nouvelles formes extraites plafonne et qu'il est donc inutilement trop computationnellement coûteux de traiter la totalité de notre corpus.  \n",
    "\n",
    "On va donc plutôt retenir seulement un échantillon de documents (environ 50% de la taille totale) extraits aléatoirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c2c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = round(len(corpus)/2) # On travaille sur environ 50% des données\n",
    "corpus = random.sample(corpus, n)\n",
    "\n",
    "print(\"On va travailler avec un échantillon de {} documents\".format(len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c732a6",
   "metadata": {},
   "source": [
    "### **Segmentation** (phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a4b16",
   "metadata": {},
   "source": [
    "**NLTK**\\\n",
    "https://www.nltk.org/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31145e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(['popular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize \n",
    "\n",
    "sents = [[s.strip('.') for s in sent_tokenize(doc)] for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sents = len(nltk.flatten(sents))\n",
    "\n",
    "print(\"Notre corpus contient {} phrases.\".format(nb_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = '!#$%&()*+,-/:;<=>?@[\\]^_{|}~©'\n",
    "\n",
    "for t in punct:\n",
    "    sents = [[sent.replace(t, ' ').replace(\"  \", \" \") for sent in doc] for doc in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b5d73",
   "metadata": {},
   "source": [
    "### **Filtrage (MWE - stopwords formés de plusieurs tokens)**\n",
    "Surtout pour filtrer les expressions relatives à l'architecture d'information / navigation Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7820d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/mwe_stopwords.txt'\n",
    "\n",
    "with open (file_path, 'r', encoding='utf-8') as f:\n",
    "    mwe_sw = [t.lower().strip('\\n') for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a19b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mwe in mwe_sw:\n",
    "    sents = [[sent.replace(mwe, ' ').replace('  ', ' ') for sent in doc] for doc in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980c335",
   "metadata": {},
   "source": [
    "### **Tokenisation / POS tagging** (TreeTagger)  \n",
    "https://github.com/miotto/treetagger-python/blob/master/README.rst  \n",
    "https://treetaggerwrapper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Seulement les caractères alphabétiques\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w\\'|\\w+\")\n",
    "\n",
    "tokens = [[tokenizer_re.tokenize(s) for s in doc] for doc in sents]\n",
    "len_corpus = len(nltk.flatten(tokens))\n",
    "\n",
    "print(\"Avec le RegExpTokenizer, notre corpus contient {} tokens.\".format(len_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada00d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [[\" \".join(sent).replace(\"' \", \"'\") for sent in doc] for doc in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047334bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c195d",
   "metadata": {},
   "source": [
    "### **Mapping POS Tags** (FRMG)\n",
    "\n",
    "Pour utiliser adéquatement notre lemmatiseur par la suite (FrenchLefffLemmatizer), on va mapper les étiquettes morphosyntaxiques du TreeTagger à celles que prend le lemmatiseur (celles issues de FRMG)\n",
    "\n",
    "http://alpage.inria.fr/frmgwiki/content/tagset-frmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/mapping_treeTagger_lefff.csv'\n",
    "\n",
    "with open(file_path) as f:\n",
    "    csv = read_csv(f)\n",
    "\n",
    "treeTag = [term for term in csv['TreeTagger'].tolist()] \n",
    "lefff = [term for term in csv['Lefff'].tolist()]\n",
    "\n",
    "mapping = {term : lefff[treeTag.index(term)] for term in treeTag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = [[[[t.split('\\t')[0], mapping[t.split('\\t')[1]]] for t in tagger.tag_text(sent)] for sent in doc] for doc in sents]\n",
    "\n",
    "#if len(t.split('\\t')) >1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574866fe",
   "metadata": {},
   "source": [
    "### **Lemmatisation** (FrenchLefffLemmatizer)\n",
    "\n",
    "https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "lemmatizer = FrenchLefffLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []\n",
    "\n",
    "for doc in tagged:\n",
    "    doc_l = []\n",
    "    for phrase in doc:\n",
    "        phrase_l = []\n",
    "        for term in phrase:\n",
    "            term_l = []\n",
    "            if lemmatizer.lemmatize(term[0], term[1]) == []:\n",
    "                term_l = [lemmatizer.lemmatize(term[0]), term[1]]\n",
    "            \n",
    "            elif type(lemmatizer.lemmatize(term[0], term[1])) == str:\n",
    "                term_l  = [lemmatizer.lemmatize(term[0], term[1]), term[1]]\n",
    "\n",
    "            else:\n",
    "                term_l = list(lemmatizer.lemmatize(term[0], term[1])[0])\n",
    "    \n",
    "            phrase_l.append(term_l)\n",
    "        doc_l.append(phrase_l)\n",
    "    lemmas.append(doc_l)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab9f5e",
   "metadata": {},
   "source": [
    "### **Filtrage** (antidictionnaire)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer l'antidictionnaire pour filtrer les données\n",
    "\n",
    "# Stopwords lemmatisés\n",
    "file_path = '../04-filtrage/stopwords_lemmatized.txt'\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords_lemmatized = [w.strip('\\n').lower() for w in f.readlines()]\n",
    "\n",
    "# Stopwords fréquents en français (non lemmatisés)\n",
    "file_path = \"../Documents/04-filtrage/stopwords.txt\"\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "\n",
    "# Stopwords fréquents en anglais (non lemmatisés)\n",
    "file_path = '../Documents/04-filtrage/stop_words_english.txt'\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords += [t.lower().strip('\\n') for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b5f4d",
   "metadata": {},
   "source": [
    "### **Phrases / N-Grammes (MWE)**\n",
    "https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addba715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammes_lemmatized = [[list(everygrams(sent, min_len=2, max_len=5)) for sent in doc] for doc in lemmas]\n",
    "ngrammes = [[list(everygrams(sent, min_len=2, max_len=5)) for sent in doc] for doc in tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrer_phrases(liste):\n",
    "        return [ng for ng in liste if freq[ng[0]] > 30 and \\\n",
    "                not ng[0].split()[0] in stopwords and len(ng[0].split()[0]) > 2 \\\n",
    "                and not ng[0].split()[-1] in stopwords and len(ng[0].split()[-1]) > 2 \\\n",
    "                and not (ng[0].split()[0] == ng[0].split()[-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1d405",
   "metadata": {},
   "source": [
    "### **Filtrage (N-grammes)**\n",
    "\n",
    "On retire les n-grammes qui apparaissent moins de 30 fois dans tout le corpus ou qui débutent ou terminent par :\n",
    "- un stopword\n",
    "- un mot de 1 lettre ou moins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3d97d",
   "metadata": {},
   "source": [
    "Pour le reste du traitement, on arrête de considérer les frontières entre les phrases et entre les documents (nos ngrammes les respectent donc on n'en a plus besoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la distribution de fréquence de chaque ngramme dans tout le corpus\n",
    "# Pour ça, on va 'applatir' la liste des ngrammes pour ne plus tenir compte des frontières entre les phrases et entre les documents\n",
    "def ng_flat(ngramme):\n",
    "    liste    = [] \n",
    "    for doc in ngramme:\n",
    "        for sent in doc:\n",
    "            for ngram in sent:\n",
    "                liste.append(ngram)\n",
    "    return liste\n",
    "\n",
    "ngrammes_lemmatized = ng_flat(ngrammes_lemmatized)\n",
    "ngrammes = ng_flat(ngrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ecd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patterns(ngrammes):\n",
    "    patterns = []\n",
    "\n",
    "    for ng in ngrammes:\n",
    "        phrase = \"\"\n",
    "        pattern = \"\"\n",
    "        for t in ng:\n",
    "            phrase += t[0]+ \" \" # 1 token\n",
    "            pattern += t[1] + \" \" # 1 POS tag\n",
    "\n",
    "        phrase = phrase.strip().replace(\"' \", \"'\")\n",
    "        pattern = pattern.strip()\n",
    "        patterns.append([phrase, pattern])\n",
    "        \n",
    "    return patterns\n",
    "\n",
    "phrases = extract_patterns(ngrammes)\n",
    "phrases_lemmatized = extract_patterns(ngrammes_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist([phrase[0] for phrase in phrases])\n",
    "freq_lemmatized = FreqDist([phrase[0] for phrase in phrases_lemmatized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a008d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrer_phrases(liste):\n",
    "        return [ng for ng in liste if freq[ng[0]] > 30 and \\\n",
    "                not ng[0].split()[0] in stopwords and len(ng[0].split()[0]) > 2 \\\n",
    "                and not ng[0].split()[-1] in stopwords and len(ng[0].split()[-1]) > 2 \\\n",
    "                and not (ng[0].split()[0] == ng[0].split()[-1])]\n",
    "\n",
    "phrases = filtrer_phrases(phrases)\n",
    "phrases_lemmatized = filtrer_phrases(phrases_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulaire = freq.keys()\n",
    "vocabulaire_lemmatized = freq_lemmatized.keys()\n",
    "\n",
    "def tabCSV(freqd, titre):\n",
    "    base_path = '../04-filtrage/'\n",
    "    tab = DataFrame(freqd.items(), columns= [\"Expression\", \"Fréquence\"])\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "\n",
    "    file_path = path.join(base_path, acteur, acteur)\n",
    "    if sous_corpus:\n",
    "       file_path = path.join(base_path, acteur, tag, tag)\n",
    "    \n",
    "\n",
    "    Path(file_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tab.to_csv(file_path + titre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f79a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabCSV(freq,'_n-grams.csv')\n",
    "tabCSV(freq_lemmatized, '_n-grams-lemmatized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386aa5c",
   "metadata": {},
   "source": [
    "### **Filtrage (Patrons syntaxiques)**  \n",
    "Lossio-Ventura, J. A., Jonquet, C., Roche, M., & Teisseire, M. (2014). Biomedical Terminology Extraction : A new combination of Statistical and Web Mining Approaches. 421. https://hal-lirmm.ccsd.cnrs.fr/lirmm-01056598\n",
    "\n",
    "On veut aller extraire les structures syntaxiques les plus courantes dans les MeSH pour filtrer notre corpus selon celles-ci (inspiré de la méthodologie de l'article ci-dessus ; voir le Notebook *Mesh_extract.ipynb*). Pour ce faire, nous allons donc ne sélectionner que les ngrammes qui y correspondent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15945967",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/MeSH/mesh_patterns-fr.csv'\n",
    "\n",
    "with open (file_path, 'r') as f:\n",
    "    patterns = read_csv(f)\n",
    "    patterns = patterns['Structure'].tolist()[:50] # Pour prendre seulement les 50 structures syntaxiques les plus fréquentes dans les MeSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e03045",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [t for t in phrases if t[1] in patterns]\n",
    "terms_lemmatized = [t for t in phrases_lemmatized if t[1] in patterns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22cebf",
   "metadata": {},
   "source": [
    "Voir combien ça a filtré en % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_terms(liste, titre):\n",
    "    file_path = '../04-filtrage/'\n",
    "    tab = pd.DataFrame(liste, columns= [\"Expression\", \"Structure syntaxique\"])\n",
    "    tab = pd.DataFrame(tab.groupby([\"Expression\", \"Structure syntaxique\"]).size().reset_index(name=\"Fréquence\"))\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "    if sous_corpus:\n",
    "        file_path = path.join(file_path, acteur, tag, tag)\n",
    "\n",
    "    else :\n",
    "        file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "                    \n",
    "    tab.to_csv(file_path + titre)\n",
    "\n",
    "extract_terms(terms, '_terms.csv')\n",
    "extract_terms(terms_lemmatized, '_terms-lemmatized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ef6e6",
   "metadata": {},
   "source": [
    "### **KWIC (Keyword in Context)**\n",
    "Termes d'intérêt : \n",
    "- « Programme »\n",
    "- « Plan »\n",
    "- « Service(s) de » \n",
    "- « Intervenant(e) en »\n",
    "- « Professionnel de »\n",
    "- « Institut (du/de) »\n",
    "- « Groupe de recherche en »\n",
    "- « Personne »\n",
    "- « Infirmière (en) »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans notre cas on veut que ça débute par le mot-clé donc le contexte est un peu plus simple\n",
    "# penser à généraliser avec des expressions régulières\n",
    "kw = ['programme', 'plan ', 'service', 'intervenant', 'infirmière en', 'institut', 'groupe de recherche', 'personne', 'maladie']\n",
    "\n",
    "ngrammes_kwic = [\" \".join([t[0] for t in ng]) for ng in ngrammes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c50b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrant = pd.DataFrame(columns=['Mot-clé','Concordance', 'Fréquence'])\n",
    "kwic = {w : [] for w in kw} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255df07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ngrammes_kwic: # on pourrait aussi chercher dans les terms, mais on perd certains termes d'intérêt avec le filtrage syntaxique\n",
    "    for w in kw:\n",
    "        if t.startswith(w):\n",
    "            kwic[w].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic = {term: FreqDist(kwic[term]) for term in kwic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in kw:\n",
    "    df = pd.DataFrame(kwic[term].items(), columns=['Concordance', \"Fréquence\"])\n",
    "    df.sort_values([\"Fréquence\"], \n",
    "        axis=0,\n",
    "        ascending=[False], \n",
    "        inplace=True)\n",
    "\n",
    "    df.insert(0, 'Mot-clé', term)\n",
    "    extrant = pd.concat([extrant, df])\n",
    "\n",
    "\n",
    "extrant = extrant[extrant['Fréquence'] > 30] \n",
    "\n",
    "file_path = '../04-filtrage/'\n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag)\n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "\n",
    "extrant.to_csv(file_path + '_KWIC' +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c7cd3",
   "metadata": {},
   "source": [
    "### **Filtrage (fréquence)**\n",
    "Pour la suite du traitement, on ne retient que les N expressions (lemmatisées ou non) les plus fréquentes dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_terms = FreqDist([t[0] for t in terms])\n",
    "exp_freq = freq_terms.most_common(10000) # On garde un maximum de 10000 termes\n",
    "\n",
    "freq_terms_lemmatized = FreqDist([t[0] for t in terms_lemmatized])\n",
    "exp_freq_lemmatized = freq_terms_lemmatized.most_common(10000) # On garde un maximum de 10000 termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../05-transformation/'\n",
    "file_path = path.join(base_path, acteur)\n",
    "\n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, tag, tag)\n",
    "\n",
    "else:\n",
    "    file_path = path.join(file_path, acteur)\n",
    "\n",
    "                    \n",
    "Path(file_path).mkdir(parents=True, exist_ok=True)\n",
    "tab = pd.DataFrame([t[0] for t in exp_freq], columns=[\"Terme\"])\n",
    "tab.to_csv(file_path + '_vocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d46bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.DataFrame([t[0] for t in exp_freq_lemmatized], columns=[\"Terme\"])\n",
    "tab.to_csv(file_path + '_vocab-lemmatized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45949eb3",
   "metadata": {},
   "source": [
    "### **Extraction de termes MeSH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "file_path = '../04-filtrage/MeSH/mesh-fr.txt'\n",
    "\n",
    "with open (file_path, 'r', encoding='utf-8') as f:\n",
    "    mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "    tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "    mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "    mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "    #mesh = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_mesh = tokenizer_mesh.tokenize([t[0] for t in exp_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = []\n",
    "\n",
    "for t in extr_mesh:\n",
    "    if t in mesh:\n",
    "        termes_mesh.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/'\n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag)\n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "df = DataFrame(termes_mesh)\n",
    "df.to_csv(file_path + '_MeSH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685131ee",
   "metadata": {},
   "source": [
    "### **Extraction de termes SNOMED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "file_path = '../04-filtrage/SNOMED/SNOMED_fr.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    sm = read_csv(f, sep=';')\n",
    "    sm = list(dict.fromkeys([str(t).strip().lower() for t in sm['term'].tolist()]))\n",
    "\n",
    "    sm = [tuple(tokenizer_re.tokenize(w)) for w in sm if len(w.split()) > 1]\n",
    "    tokenizer_sm = MWETokenizer(sm, separator = ' ')\n",
    "\n",
    "    sm = [tokenizer_sm.tokenize(w)[0].lower() for w in sm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_sm = tokenizer_sm.tokenize([t[0] for t in exp_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = []\n",
    "\n",
    "for t in extr_sm:\n",
    "    if t in sm:\n",
    "        termes_sm.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/' \n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag) \n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "\n",
    "df = DataFrame(termes_sm)\n",
    "\n",
    "df.to_csv(file_path + '_SNOMED.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a33210152f7d2bd255fb16656f372b633dbf298ed202bbbac20290b0375cadb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
