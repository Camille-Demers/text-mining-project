{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c662c7d",
   "metadata": {},
   "source": [
    "## **2. Prétraitement**\n",
    "- Segmentation (phrases)\n",
    "- Tokenization (mots)\n",
    "- Étiquetage morphosyntaxique (POS Tagging) \n",
    "- (Lemmatisation)\n",
    "- Filtrage (stopwords)\n",
    "- Extraction de termes complexes (MWE / n-grammes / segments répétés)\n",
    "- Chunking / Filtrage par patrons syntaxiques (basés sur les patrons fréquents dans les MeSH)\n",
    "- Extraction de collocations significatives (en fonction du Log-likelihood ratio)\n",
    "- Extraction de concordances (KWIC) pour un ensemble de mots-clés d'intérêt\n",
    "- Extraction de termes MeSH et SNOMED présents dans les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece38e9",
   "metadata": {},
   "source": [
    "### **Lire le corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a62415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, re, pandas, random\n",
    "from os import listdir, chdir, path\n",
    "from pathlib import Path\n",
    "\n",
    "acteur = 'chuqc'\n",
    "langue = 'fr'\n",
    "sous_corpus = False \n",
    "tag = ''\n",
    "\n",
    "# Change the directory\n",
    "if sous_corpus:\n",
    "    base_path = '../03-corpus/2-sous-corpus/'\n",
    "    file_path = path.join(base_path, acteur, acteur + '_' + tag + '.csv')\n",
    "\n",
    "else: \n",
    "    base_path = '../03-corpus/2-data/1-fr/'\n",
    "    file_path = path.join(base_path, acteur) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a31c5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "with open(file_path, \"r\", encoding = \"UTF-8\") as f:\n",
    "        data = read_csv(file_path)['text']\n",
    "        #data = data[~data[\"url\"].str.contains('pdf')] # Exclure les PDFs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd537ec8",
   "metadata": {},
   "source": [
    "### **Nettoyage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8c10c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [str(t).strip('\\n').lower().replace('’', '\\'').replace(\"œ\", \"oe\") for t in data]\n",
    "punct = '[!#$%&\\(\\)*+,-/:;<=>?@[\\]^_{|}~©«»—“”–—]'\n",
    "spaces = '\\s+'\n",
    "\n",
    "text = [re.sub(punct, ' ', t).replace(\"' \", \"'\" ).replace(\"'\", \"'\") for t in text]\n",
    "text = [re.sub(spaces, ' ', t) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4fda380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a un corpus de 1270 documents.\n"
     ]
    }
   ],
   "source": [
    "nb_docs = len(text)\n",
    "\n",
    "print(\"On a un corpus de {} documents.\".format(nb_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89c4d8",
   "metadata": {},
   "source": [
    "### **Extraire un échantillon aléatoire**\n",
    "\n",
    "Sinon, on n'arrive pas à traiter la totalité du corpus pour des raisons de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69841ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On va travailler sur un échantillon correspondant à environ x % des documents du corpus, soit 1270 documents\n"
     ]
    }
   ],
   "source": [
    "n = round(1 * nb_docs)\n",
    "corpus = random.sample(text, n)\n",
    "\n",
    "print(\"On va travailler sur un échantillon correspondant à environ x % des documents du corpus, soit {} documents\". format(len(corpus)))\n",
    "\n",
    "corpus = \" \".join(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a4b16",
   "metadata": {},
   "source": [
    "**NLTK**\\\n",
    "https://www.nltk.org/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31145e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(['popular'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b5d73",
   "metadata": {},
   "source": [
    "### **Filtrage (MWE - stopwords formés de plusieurs tokens)**\n",
    "Surtout pour filtrer les expressions relatives à l'architecture d'information / navigation Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b7820d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/mwe_stopwords.txt'\n",
    "\n",
    "with open (file_path, 'r', encoding='utf-8') as f:\n",
    "    mwe_sw = [t.lower().strip('\\n') for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29a19b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mwe in mwe_sw:\n",
    "    corpus = corpus.replace(mwe, ' MWE_STOP ').replace('  ', \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980c335",
   "metadata": {},
   "source": [
    "### **Tokenisation / POS tagging** (TreeTagger)  \n",
    "https://github.com/miotto/treetagger-python/blob/master/README.rst  \n",
    "https://treetaggerwrapper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb94eec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec le RegExpTokenizer, notre corpus contient 1311120 tokens.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Seulement les caractères alphabétiques\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w\\'|\\w+\")\n",
    "\n",
    "tokens = tokenizer_re.tokenize(corpus)\n",
    "\n",
    "print(\"Avec le RegExpTokenizer, notre corpus contient {} tokens.\".format(len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "237db34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le POS tagging devrait prendre environ 1 minutes.\n"
     ]
    }
   ],
   "source": [
    "temps = round(len(tokens) / 15000 / 60)\n",
    "print('Le POS tagging devrait prendre environ {} minutes.'.format(temps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c07b565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" \".join(tokens).replace(\"' \", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "047334bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c195d",
   "metadata": {},
   "source": [
    "### **Mapping POS Tags** (FRMG)\n",
    "\n",
    "Pour utiliser adéquatement notre lemmatiseur par la suite (FrenchLefffLemmatizer), on va mapper les étiquettes morphosyntaxiques du TreeTagger à celles que prend le lemmatiseur (celles issues de FRMG)\n",
    "\n",
    "http://alpage.inria.fr/frmgwiki/content/tagset-frmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7682234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = '../04-filtrage/mapping_treeTagger_lefff.csv'\n",
    "\n",
    "#with open(file_path) as f:\n",
    "#    csv = read_csv(f)\n",
    "\n",
    "#treeTag = [term for term in csv['TreeTagger'].tolist()] \n",
    "#lefff = [term for term in csv['Lefff'].tolist()]\n",
    "\n",
    "#mapping = {term : lefff[treeTag.index(term)] for term in treeTag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d991e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged = [[t.split('\\t')[0], mapping[t.split('\\t')[1]]] for t in tagger.tag_text(corpus)]\n",
    "#  mapping[t.split('\\t')[1]])\n",
    "tagged = [(t.split('\\t')[0], t.split('\\t')[1]) for t in tagger.tag_text(corpus)]\n",
    "\n",
    "\n",
    "#if len(t.split('\\t')) >1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f9363b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('directions', 'NOM'),\n",
       " ('chu', 'VER:pper'),\n",
       " ('de', 'PRP'),\n",
       " ('québec', 'ADJ'),\n",
       " ('université', 'NOM'),\n",
       " ('laval', 'ADJ'),\n",
       " ('contenu', 'VER:pper'),\n",
       " ('de', 'PRP'),\n",
       " ('la', 'DET:ART'),\n",
       " ('page', 'NOM'),\n",
       " ('attention', 'NOM'),\n",
       " ('pour', 'PRP'),\n",
       " ('les', 'DET:ART'),\n",
       " ('personnes', 'NOM'),\n",
       " ('dont', 'PRO:REL'),\n",
       " ('le', 'DET:ART'),\n",
       " ('prochain', 'ADJ'),\n",
       " ('rendez', 'VER:pres'),\n",
       " ('vous', 'PRO:PER'),\n",
       " ('sera', 'VER:futu'),\n",
       " ('au', 'PRP:det'),\n",
       " ('centre', 'NOM'),\n",
       " ('intégré', 'VER:pper'),\n",
       " ('de', 'PRP'),\n",
       " ('cancérologie', 'NOM')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574866fe",
   "metadata": {},
   "source": [
    "### **Lemmatisation** (FrenchLefffLemmatizer)\n",
    "\n",
    "https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a494e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "#lemmatizer = FrenchLefffLemmatizer()\n",
    "\n",
    "#lemmas = []\n",
    "#for term in tagged:\n",
    "#    term_l = []\n",
    "#    if lemmatizer.lemmatize(term[0], term[1]) == []:\n",
    "#        term_l = (lemmatizer.lemmatize(term[0]), term[1])\n",
    "    \n",
    "    # elif type(lemmatizer.lemmatize(term[0], term[1])) == str:\n",
    "    #     term_l  = (lemmatizer.lemmatize(term[0], term[1]), term[1])\n",
    "\n",
    "    # else:\n",
    "    #     term_l = tuple(lemmatizer.lemmatize(term[0], term[1])[0])\n",
    "    \n",
    "    # lemmas.append(term_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b5f4d",
   "metadata": {},
   "source": [
    "### **Collocations / Phrases / N-Grammes (MWE)**\n",
    "https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4fc7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import everygrams\n",
    "ngrammes = list(everygrams(tagged, min_len=2, max_len=6))\n",
    "# ngrammes_lemmatized = list(everygrams(lemmas, min_len=2, max_len=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18ecac95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant filtrage, on a 6555520 ngrammes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Avant filtrage, on a {} ngrammes.\".format(len(ngrammes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1d405",
   "metadata": {},
   "source": [
    "### **Extraction des patrons syntaxiques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0ecd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patterns(ngrammes):\n",
    "    patterns = []\n",
    "\n",
    "    for ng in ngrammes:\n",
    "        phrase = [t[0] for t in ng]\n",
    "        pattern = [t[1] for t in ng]\n",
    "        patterns.append([phrase, pattern])\n",
    "        \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c83c3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = extract_patterns(ngrammes)\n",
    "# phrases_lemmatized = extract_patterns(ngrammes_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b55a7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule la distribution de fréquences de nos n-grammes\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq = FreqDist([\" \".join(t[0]).replace(\"' \", \"'\") for t in phrases])\n",
    "# freq_lemmatized = FreqDist([\" \".join(t[0]).replace(\"' \", \"'\") for t in phrases_lemmatized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "261db806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de la', 11408),\n",
       " (\"de l'\", 8414),\n",
       " ('demande de', 7616),\n",
       " ('rendez vous', 6696),\n",
       " ('à la', 5794),\n",
       " ('de québec', 5226),\n",
       " ('tout sur', 4788),\n",
       " (\"à l'\", 4458),\n",
       " ('pour les', 4347),\n",
       " ('de consultation', 3816),\n",
       " ('demande de consultation', 3776),\n",
       " ('chu de', 3582),\n",
       " ('chu de québec', 3533),\n",
       " ('de rendez', 3453),\n",
       " ('de rendez vous', 3452),\n",
       " (\"d'éthique\", 3394),\n",
       " ('formulaire de', 3276),\n",
       " (\"d'un\", 3239),\n",
       " ('analyses en', 3228),\n",
       " ('des usagers', 3065),\n",
       " ('centre de', 3026),\n",
       " ('votre chirurgie', 2916),\n",
       " ('la recherche', 2914),\n",
       " (\"d'évaluation\", 2882),\n",
       " ('comité des', 2839)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93e86e",
   "metadata": {},
   "source": [
    "### **Filtrage** \n",
    "On retire les n-grammes qui débutent ou se terminent par un stopword (antidictionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd5d7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer l'antidictionnaire pour filtrer les données\n",
    "\n",
    "# # Stopwords lemmatisés\n",
    "# file_path = '../04-filtrage/stopwords_lemmatized.txt'\n",
    "# with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "#     stopwords_lemmatized = [w.strip('\\n').lower() for w in f.readlines()]\n",
    "\n",
    "# Stopwords fréquents en français (non lemmatisés)\n",
    "file_path = \"../04-filtrage/stopwords.txt\"\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "\n",
    "# Stopwords fréquents en anglais (non lemmatisés)\n",
    "file_path = '../04-filtrage/stop_words_english.txt'\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords += [t.lower().strip('\\n') for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52036bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrer_stopwords(x):\n",
    "    return [term for term in x if not 'MWE_STOP' in term[0] and not term[0][0] in stopwords and not term[0][-1] in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "86835f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = filtrer_stopwords(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a89e06",
   "metadata": {},
   "source": [
    "On retire les n-grammes qui débutent ou se terminent par un chiffre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "453fc4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_num(x):\n",
    "    return [term for term in x if not term[0][0].isnumeric() and not term[0][-1].isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c4f428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = filter_num(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26da835",
   "metadata": {},
   "source": [
    "On retire les n-grammes qui débutent ou se terminent par token dont la longueur est inférieure à 2 caractères ou supérieure à 18 caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28750634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_len(x):\n",
    "    return [term for term in x if \\\n",
    "        len(term[0][0]) > 2 and len(term[0][0]) < 18 and \\\n",
    "        len(term[0][-1]) > 2 and len(term[0][-1]) < 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "430f8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = filter_len(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a26b47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [[\" \".join(term[0]).replace(\"' \", \"'\"), \" \".join(term[1])] for term in phrases]\n",
    "# phrases_lemmatized = filtrer_phrases(phrases_lemmatized, freq_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a5ced2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase in phrases:\n",
    "    phrase.append(freq[phrase[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3edd530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après filtrage, on a 1638119 occurrences de ngrammes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Après filtrage, on a {} occurrences de ngrammes.\".format(len(phrases))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77d889bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Expression</th>\n",
       "      <th>Patron syntaxique</th>\n",
       "      <th>Fréquence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>directions chu</td>\n",
       "      <td>NOM VER:pper</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>directions chu de québec</td>\n",
       "      <td>NOM VER:pper PRP ADJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>directions chu de québec université</td>\n",
       "      <td>NOM VER:pper PRP ADJ NOM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chu de québec</td>\n",
       "      <td>VER:pper PRP ADJ</td>\n",
       "      <td>3533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chu de québec université</td>\n",
       "      <td>VER:pper PRP ADJ NOM</td>\n",
       "      <td>2367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638114</th>\n",
       "      <td>enseignement et des affaires universitaires</td>\n",
       "      <td>NOM KON PRP:det NOM ADJ</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638115</th>\n",
       "      <td>affaires universitaires</td>\n",
       "      <td>NOM ADJ</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638116</th>\n",
       "      <td>chu de québec</td>\n",
       "      <td>VER:pper PRP NOM</td>\n",
       "      <td>3533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638117</th>\n",
       "      <td>chu de québec untitled</td>\n",
       "      <td>VER:pper PRP NOM ADJ</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638118</th>\n",
       "      <td>québec untitled</td>\n",
       "      <td>NOM ADJ</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1638119 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Expression  \\\n",
       "0                                     directions chu   \n",
       "1                           directions chu de québec   \n",
       "2                directions chu de québec université   \n",
       "3                                      chu de québec   \n",
       "4                           chu de québec université   \n",
       "...                                              ...   \n",
       "1638114  enseignement et des affaires universitaires   \n",
       "1638115                      affaires universitaires   \n",
       "1638116                                chu de québec   \n",
       "1638117                       chu de québec untitled   \n",
       "1638118                              québec untitled   \n",
       "\n",
       "                Patron syntaxique  Fréquence  \n",
       "0                    NOM VER:pper          1  \n",
       "1            NOM VER:pper PRP ADJ          1  \n",
       "2        NOM VER:pper PRP ADJ NOM          1  \n",
       "3                VER:pper PRP ADJ       3533  \n",
       "4            VER:pper PRP ADJ NOM       2367  \n",
       "...                           ...        ...  \n",
       "1638114   NOM KON PRP:det NOM ADJ        270  \n",
       "1638115                   NOM ADJ        270  \n",
       "1638116          VER:pper PRP NOM       3533  \n",
       "1638117      VER:pper PRP NOM ADJ         44  \n",
       "1638118                   NOM ADJ         44  \n",
       "\n",
       "[1638119 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(phrases, columns=[\"Expression\", \"Patron syntaxique\", \"Fréquence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be52a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabCSV(phrases, titre):\n",
    "    base_path = '../04-filtrage/output/'\n",
    "    tab = DataFrame(phrases, columns=[\"Expression\", \"Patron syntaxique\", \"Fréquence\"]).drop_duplicates()\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "\n",
    "    file_path = path.join(base_path, acteur, acteur)\n",
    "    if sous_corpus:\n",
    "       file_path = path.join(base_path, acteur, tag, tag)\n",
    "    \n",
    "\n",
    "    Path(file_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tab.to_csv(file_path + titre)\n",
    "\n",
    "    return tab.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a3f79a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = tabCSV(phrases,'_n-grams.csv')\n",
    "# phrases_lemmatized = tabCSV(phrases_lemmatized, '_n-grams-lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4acce1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après filtrage, on a 280253 ngrammes uniques.\n"
     ]
    }
   ],
   "source": [
    "print(\"Après filtrage, on a {} ngrammes uniques.\".format(len(phrases)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386aa5c",
   "metadata": {},
   "source": [
    "### **Filtrage (Patrons syntaxiques)**  \n",
    "Lossio-Ventura, J. A., Jonquet, C., Roche, M., & Teisseire, M. (2014). Biomedical Terminology Extraction : A new combination of Statistical and Web Mining Approaches. 421. https://hal-lirmm.ccsd.cnrs.fr/lirmm-01056598\n",
    "\n",
    "On veut aller extraire les structures syntaxiques les plus courantes dans les MeSH pour filtrer notre corpus selon celles-ci (inspiré de la méthodologie de l'article ci-dessus ; voir le Notebook *Mesh_extract.ipynb*). Pour ce faire, nous allons donc ne sélectionner que les ngrammes qui y correspondent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15945967",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/MeSH/mesh_patterns-fr.csv'\n",
    "\n",
    "with open (file_path, 'r') as f:\n",
    "    patterns = read_csv(f)\n",
    "    patterns = patterns['Structure'].tolist()[:200] #[:200] # # On peut aussi seulement prendree les 200 structures syntaxiques les plus fréquentes dans les MeSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d1e03045",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [t for t in phrases if t[1] in patterns] #and not 'NOM NOM' in t[1]] # \n",
    "# terms_lemmatized = [t for t in phrases_lemmatized if t[1] in patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39570e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['demande de consultation', 'NOM PRP NOM', 3776],\n",
       " ['chu de québec', 'VER:pper PRP NOM', 3533],\n",
       " ['comité des usagers', 'NOM PRP:det NOM', 2837],\n",
       " ['comité des usagers', 'NOM PRP:det ADJ', 2837],\n",
       " [\"comité d'éthique\", 'NOM PRP NOM', 2792],\n",
       " ['clinique interdisciplinaire de mémoire', 'NOM ADJ PRP NOM', 2695],\n",
       " ['mémoire cime', 'NOM NOM', 2692],\n",
       " ['québec université', 'NOM NOM', 2368],\n",
       " ['québec université', 'ADJ NOM', 2368],\n",
       " ['professionnels de la santé', 'NOM PRP DET:ART NOM', 2312],\n",
       " ['intervention en santé', 'NOM PRP NOM', 2236],\n",
       " ['évaluation des technologies', 'NOM PRP:det NOM', 2232],\n",
       " [\"modes d'intervention\", 'NOM PRP NOM', 2231],\n",
       " [\"modes d'intervention en santé\", 'NOM PRP NOM PRP NOM', 2226],\n",
       " ['enfant jésus', 'NOM NOM', 1978],\n",
       " ['hôpital saint', 'NOM ADJ', 1977],\n",
       " [\"saint françois d'assise\", 'ADJ NOM PRP NOM', 1977],\n",
       " [\"hôpital de l'enfant\", 'NOM PRP DET:ART NOM', 1966],\n",
       " [\"hôpital de l'enfant jésus\", 'NOM PRP DET:ART NOM NOM', 1965],\n",
       " ['implant cochléaire', 'NOM ADJ', 1849],\n",
       " ['saint sacrement', 'ADJ NOM', 1801],\n",
       " ['hôpital du saint', 'NOM PRP:det ADJ', 1783],\n",
       " ['hôpital du saint sacrement', 'NOM PRP:det ADJ NOM', 1778],\n",
       " ['centres hospitaliers', 'NOM ADJ', 1739],\n",
       " ['complexe hospitalier', 'NOM ADJ', 1726],\n",
       " ['complexe hospitalier', 'ADJ NOM', 1726],\n",
       " ['éthique de la recherche', 'NOM PRP DET:ART NOM', 1683],\n",
       " [\"comité d'éthique de la recherche\", 'NOM PRP NOM PRP DET:ART NOM', 1676],\n",
       " ['implantation bilatérale', 'NOM ADJ', 1671],\n",
       " ['responsabilités et recours', 'NOM KON NOM', 1632],\n",
       " ['droits responsabilités', 'ADJ NOM', 1632],\n",
       " ['droits responsabilités', 'NOM NOM', 1632],\n",
       " ['dépistage néonatal', 'NOM ADJ', 1631],\n",
       " ['dépistage néonatal sanguin', 'NOM ADJ ADJ', 1626],\n",
       " ['proches aidants', 'ADJ NOM', 1625],\n",
       " ['apr chu', 'NOM VER:pper', 1620],\n",
       " ['modalités concernant les visiteurs', 'NOM VER:ppre DET:ART NOM', 1590],\n",
       " ['hôtel dieu', 'NOM NOM', 1430],\n",
       " ['dieu de québec', 'NOM PRP NOM', 1410],\n",
       " ['hôtel dieu de québec', 'NOM NOM PRP NOM', 1409],\n",
       " ['mère enfant', 'NOM NOM', 1236],\n",
       " ['mère enfant', 'ADJ NOM', 1236],\n",
       " ['capitale nationale', 'NOM ADJ', 1202],\n",
       " ['éthique clinique', 'ADJ NOM', 1200],\n",
       " ['éthique clinique', 'NOM ADJ', 1200],\n",
       " ['centre mère enfant', 'NOM ADJ NOM', 1197],\n",
       " ['centre mère', 'NOM ADJ', 1197],\n",
       " ['archives médicales', 'NOM ADJ', 1190],\n",
       " ['imagerie médicale', 'NOM ADJ', 1170],\n",
       " ['évaluation éthique', 'NOM ADJ', 1122]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4539e651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le filtrage syntaxique élimine environ 74 % des termes\n",
      "On avait 280253 ngrammes, on en a maintenant 71662.\n"
     ]
    }
   ],
   "source": [
    "print(\"Le filtrage syntaxique élimine environ {} % des termes\".format(round((len(phrases) - len(terms)) / len(phrases) * 100)))\n",
    "print(\"On avait {} ngrammes, \".format(len(phrases)) + \"on en a maintenant {}.\".format(len(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b74b50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_terms(liste_terms, titre):\n",
    "    file_path = '../04-filtrage/output/'\n",
    "    tab = pd.DataFrame(terms, columns= [\"Expression\", \"Structure syntaxique\", \"Fréquence\"]).drop_duplicates(subset='Expression', keep=\"last\")\n",
    "    tab.sort_values([\"Fréquence\"], \n",
    "                        axis=0,\n",
    "                        ascending=[False], \n",
    "                        inplace=True)\n",
    "\n",
    "    if sous_corpus:\n",
    "        file_path = path.join(file_path, acteur, tag, tag)\n",
    "\n",
    "    else :\n",
    "        file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "                    \n",
    "    tab.to_csv(file_path + titre)\n",
    "\n",
    "extract_terms(terms, '_terms.csv')\n",
    "#extract_terms(terms_lemmatized, '_terms-lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5df1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_patterns = DataFrame(terms, columns = [\"Expression\", \"Structure syntaxique\", \"Fréquence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "41c6a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_patterns = terms_patterns.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6049e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_patterns = {}\n",
    "for term in terms_patterns:\n",
    "    exp = term['Expression']\n",
    "    pattern = term['Structure syntaxique']\n",
    "    dict_patterns[exp] = pattern\n",
    "\n",
    "# dict_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e19bb40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Expression</th>\n",
       "      <th>Structure syntaxique</th>\n",
       "      <th>Fréquence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demande de consultation</td>\n",
       "      <td>NOM PRP NOM</td>\n",
       "      <td>3776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chu de québec</td>\n",
       "      <td>VER:pper PRP NOM</td>\n",
       "      <td>3533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comité des usagers</td>\n",
       "      <td>NOM PRP:det NOM</td>\n",
       "      <td>2837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comité des usagers</td>\n",
       "      <td>NOM PRP:det ADJ</td>\n",
       "      <td>2837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comité d'éthique</td>\n",
       "      <td>NOM PRP NOM</td>\n",
       "      <td>2792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71657</th>\n",
       "      <td>plaie rougeur</td>\n",
       "      <td>NOM NOM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71658</th>\n",
       "      <td>plaie rougeur enflure</td>\n",
       "      <td>NOM NOM NOM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71659</th>\n",
       "      <td>rougeur enflure</td>\n",
       "      <td>NOM NOM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71660</th>\n",
       "      <td>rougeur enflure de la plaie</td>\n",
       "      <td>NOM NOM PRP DET:ART NOM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71661</th>\n",
       "      <td>meilleure décision</td>\n",
       "      <td>ADJ NOM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71662 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Expression     Structure syntaxique  Fréquence\n",
       "0          demande de consultation              NOM PRP NOM       3776\n",
       "1                    chu de québec         VER:pper PRP NOM       3533\n",
       "2               comité des usagers          NOM PRP:det NOM       2837\n",
       "3               comité des usagers          NOM PRP:det ADJ       2837\n",
       "4                 comité d'éthique              NOM PRP NOM       2792\n",
       "...                            ...                      ...        ...\n",
       "71657                plaie rougeur                  NOM NOM          1\n",
       "71658        plaie rougeur enflure              NOM NOM NOM          1\n",
       "71659              rougeur enflure                  NOM NOM          1\n",
       "71660  rougeur enflure de la plaie  NOM NOM PRP DET:ART NOM          1\n",
       "71661           meilleure décision                  ADJ NOM          1\n",
       "\n",
       "[71662 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(terms_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1baa06e",
   "metadata": {},
   "source": [
    "### **Filtrage (Collocations statistiquement significatives)** Log-Likelihood Ratio\n",
    "\n",
    "[Notebook - Collocation extraction methodologies compared](https://notebooks.githubusercontent.com/view/ipynb?azure_maps_enabled=false&browser=chrome&color_mode=auto&commit=33868e847376764d7733cd958986c88dedfaec97&device=unknown&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f746f64642d636f6f6b2f4d4c2d596f752d43616e2d5573652f333338363865383437333736373634643737333363643935383938366338386465646661656339372f70726f626162696c69737469635f6c616e67756167655f6d6f64656c696e672f636f6c6c6f636174696f6e5f65787472616374696f6e732e6970796e62&enterprise_enabled=false&logged_in=false&nwo=todd-cook%2FML-You-Can-Use&path=probabilistic_language_modeling%2Fcollocation_extractions.ipynb&platform=android&repository_id=167140788&repository_type=Repository&version=102)\n",
    "\n",
    "On applique un test d'hypothèse statistique aux n-grammes sur lesquels une probabilité a été mesurée (Log-likelihood ratio) - seuls les n-grammes dont le test est significatif seront conservés.\n",
    "On considère que l'apparition de ces collocations dans notre corpus n'est pas dûe au hasard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5cc17f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood_ratio(c_prior, c_n, c_ngram, N):\n",
    "    \"\"\"\n",
    "    Compute the ratio of two hypotheses of likelihood and return the ratio.\n",
    "    The formula here and test verification values are taken from \n",
    "    Manning & Schūtze _Foundations of Statistical Natural Language Processing_ p.172-175\n",
    "    Parameters:\n",
    "    c_prior: count of word 1 if bigrams or count of [w1w2 .. w(n-1)] if ngram\n",
    "    c_n : count of word 2 if bigrams or count of wn if ngram\n",
    "    c12: count of bigram (w1, w2) if bigram or count of ngram if ngram\n",
    "    N: the number of words in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    p = c_n / N\n",
    "    p1 = c_ngram / c_prior\n",
    "    p2 = (c_n - c_ngram) / (N - c_prior)   \n",
    "    # We proactively trap a runtimeWarning: divide by zero encountered in log,\n",
    "    # which may occur with extreme collocations\n",
    "    import warnings\n",
    "    with warnings.catch_warnings(): # this will reset our filterwarnings setting\n",
    "        warnings.filterwarnings('error')\n",
    "        try:\n",
    "            return (np.log(binom.pmf(c_ngram, c_prior, p)) \n",
    "                    + np.log(binom.pmf(c_n - c_ngram, N - c_prior, p)) \n",
    "                    - np.log(binom.pmf(c_ngram, c_prior, p1) )\n",
    "                    - np.log(binom.pmf(c_n - c_ngram, N - c_prior, p2)))             \n",
    "        except Warning:\n",
    "            return np.inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "20e82354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Au départ, on a 71662 ngrammes.\n"
     ]
    }
   ],
   "source": [
    "len_prior = len(terms)\n",
    "print(\"Au départ, on a {} ngrammes.\".format(len_prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2891d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom, chi2\n",
    "from nltk import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "200f957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le calcul des probabilités, on a besoin de traiter séparément les ngrammes selon la valeur de n\n",
    "N = len(tokens)\n",
    "fd_tokens = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f577b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llr_ngrammes(n):\n",
    "    llr = []\n",
    "\n",
    "    for i in range(2, n+1):\n",
    "        ngrammes = set([tuple(tokenizer_re.tokenize(term[0])) for term in terms if len(tokenizer_re.tokenize(term[0])) == i])\n",
    "        fd = nltk.FreqDist(ngrams(tokens, n=i))\n",
    "        fd_prior = nltk.FreqDist(ngrams(tokens, n=i-1))\n",
    "        \n",
    "        for t in ngrammes:\n",
    "            c_prior = fd_prior[t[:i-1]] # Antécédent = P(w1w2..w_n-1) (si on considère que P(w1w2...wn) = P(wn) | P(w1w2...w_n-1)\n",
    "            c_n = fd_tokens[t[i-1]]     # Dernier mot du ngramme  P(wn)\n",
    "            c_ngram = fd[t]             # Le ngramme lui-même P(w1w2w3..wn)\n",
    "\n",
    "            res = -2 * loglikelihood_ratio(c_prior, c_n, c_ngram, N)\n",
    "            p = chi2.sf(res, 1) # 1 degrees of freedom\n",
    "            #if res == float('-inf') :\n",
    "            #    res = 50000\n",
    "\n",
    "            if p < 0.005 or (res == float('-inf')):\n",
    "                llr.append({'Collocation' : \" \".join(t).replace(\"' \", \"'\"), 'Structure syntaxique': dict_patterns[\" \".join(t).replace(\"' \", \"'\")], 'Fréquence' : c_ngram, 'LLR': res, 'p-value': p})\n",
    "\n",
    "    return llr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "810a7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = llr_ngrammes(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a3cddc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après avoir calculé le log-likelihood ratio, on a retiré 6615 collocations qui n'étaient pas statistiquement significatives.\n",
      "Ça représente environ 9 % de nos n-grammes.\n"
     ]
    }
   ],
   "source": [
    "print('Après avoir calculé le log-likelihood ratio, on a retiré {} collocations qui n\\'étaient pas statistiquement significatives.'.format(len_prior - len(terms)))\n",
    "print('Ça représente environ {} % de nos n-grammes.'.format(round((len_prior - len(terms)) / len_prior *100 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6ffcb628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collocation</th>\n",
       "      <th>Structure syntaxique</th>\n",
       "      <th>Fréquence</th>\n",
       "      <th>LLR</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33271</th>\n",
       "      <td>demande de consultation</td>\n",
       "      <td>NOM PRP NOM</td>\n",
       "      <td>3776</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26245</th>\n",
       "      <td>chu de québec</td>\n",
       "      <td>VER:pper PRP NOM</td>\n",
       "      <td>3533</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30446</th>\n",
       "      <td>comité des usagers</td>\n",
       "      <td>NOM PRP:det ADJ</td>\n",
       "      <td>2837</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36854</th>\n",
       "      <td>comité d'éthique</td>\n",
       "      <td>NOM PRP NOM</td>\n",
       "      <td>2792</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50891</th>\n",
       "      <td>clinique interdisciplinaire de mémoire</td>\n",
       "      <td>NOM ADJ PRP NOM</td>\n",
       "      <td>2695</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26643</th>\n",
       "      <td>neurologie de hej</td>\n",
       "      <td>NOM PRP NOM</td>\n",
       "      <td>1</td>\n",
       "      <td>18.399623</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26645</th>\n",
       "      <td>adaptée au préalable</td>\n",
       "      <td>VER:pper PRP:det NOM</td>\n",
       "      <td>1</td>\n",
       "      <td>19.395162</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26646</th>\n",
       "      <td>transporté par évaq</td>\n",
       "      <td>VER:pper PRP NOM</td>\n",
       "      <td>1</td>\n",
       "      <td>21.033785</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26647</th>\n",
       "      <td>étude des voies</td>\n",
       "      <td>NOM PRP:det NOM</td>\n",
       "      <td>1</td>\n",
       "      <td>19.645812</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65046</th>\n",
       "      <td>pôle d'enseignement universitaire en santé</td>\n",
       "      <td>NOM PRP NOM ADJ PRP NOM</td>\n",
       "      <td>1</td>\n",
       "      <td>10.625860</td>\n",
       "      <td>0.001115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65047 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Collocation     Structure syntaxique  \\\n",
       "33271                     demande de consultation              NOM PRP NOM   \n",
       "26245                               chu de québec         VER:pper PRP NOM   \n",
       "30446                          comité des usagers          NOM PRP:det ADJ   \n",
       "36854                            comité d'éthique              NOM PRP NOM   \n",
       "50891      clinique interdisciplinaire de mémoire          NOM ADJ PRP NOM   \n",
       "...                                           ...                      ...   \n",
       "26643                           neurologie de hej              NOM PRP NOM   \n",
       "26645                        adaptée au préalable     VER:pper PRP:det NOM   \n",
       "26646                         transporté par évaq         VER:pper PRP NOM   \n",
       "26647                             étude des voies          NOM PRP:det NOM   \n",
       "65046  pôle d'enseignement universitaire en santé  NOM PRP NOM ADJ PRP NOM   \n",
       "\n",
       "       Fréquence        LLR   p-value  \n",
       "33271       3776       -inf  1.000000  \n",
       "26245       3533       -inf  1.000000  \n",
       "30446       2837       -inf  1.000000  \n",
       "36854       2792       -inf  1.000000  \n",
       "50891       2695       -inf  1.000000  \n",
       "...          ...        ...       ...  \n",
       "26643          1  18.399623  0.000018  \n",
       "26645          1  19.395162  0.000011  \n",
       "26646          1  21.033785  0.000005  \n",
       "26647          1  19.645812  0.000009  \n",
       "65046          1  10.625860  0.001115  \n",
       "\n",
       "[65047 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(terms).sort_values(by = \"Fréquence\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f235b77",
   "metadata": {},
   "source": [
    "### **Filtrage - Fréquence documentaire**\n",
    "** Il y aurait quelque chose à modifier ici ; en tokenisant, on supprimer les frontières entre les mots qui sont des tirets ou autres caractères qu'un espace ou un apostrophe ; ça fait en sorte qu'on a une fréquence documentaire de 0 pour les ngrammes qui n'ont plus la forme exacte qu'ils avaient dans le corpus orginal. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df69f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {term['Collocation']: len([doc for doc in text if term['Collocation'] in doc]) for term in terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f4ae1733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1080"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_df = round(0.85 * nb_docs)        # Pour rejeter les termes qui se retrouvent dans plus de 85% des documents \n",
    "max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "643e2b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dfs[max(dfs, key=dfs.get)] # Voir quelle est la fréquence documentaire maximale qu'on retrouve\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9194dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = {term for term in dfs if dfs[term] == 0}\n",
    "dfs_100 = {term for term in dfs if dfs[term] >= max_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "50a3e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = round(0.95 * nb_docs)        # Pour rejeter les termes qui se retrouvent dans 95% des documents \n",
    "terms = [t for t in terms if dfs[t['Collocation']] < max_df ] #and dfs[t['Collocation']] > 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "89c4f3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65047"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colloc = [term['Collocation'] for term in terms]\n",
    "len(colloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1359835a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collocation</th>\n",
       "      <th>Structure syntaxique</th>\n",
       "      <th>Fréquence</th>\n",
       "      <th>LLR</th>\n",
       "      <th>p-value</th>\n",
       "      <th>DF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33271</th>\n",
       "      <td>demande de consultation</td>\n",
       "      <td>NOM PRP NOM</td>\n",
       "      <td>3776</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26245</th>\n",
       "      <td>chu de québec</td>\n",
       "      <td>VER:pper PRP NOM</td>\n",
       "      <td>3533</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30446</th>\n",
       "      <td>comité des usagers</td>\n",
       "      <td>NOM PRP:det ADJ</td>\n",
       "      <td>2837</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36854</th>\n",
       "      <td>comité d'éthique</td>\n",
       "      <td>NOM PRP NOM</td>\n",
       "      <td>2792</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50891</th>\n",
       "      <td>clinique interdisciplinaire de mémoire</td>\n",
       "      <td>NOM ADJ PRP NOM</td>\n",
       "      <td>2695</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26643</th>\n",
       "      <td>neurologie de hej</td>\n",
       "      <td>NOM PRP NOM</td>\n",
       "      <td>1</td>\n",
       "      <td>18.399623</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26645</th>\n",
       "      <td>adaptée au préalable</td>\n",
       "      <td>VER:pper PRP:det NOM</td>\n",
       "      <td>1</td>\n",
       "      <td>19.395162</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26646</th>\n",
       "      <td>transporté par évaq</td>\n",
       "      <td>VER:pper PRP NOM</td>\n",
       "      <td>1</td>\n",
       "      <td>21.033785</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26647</th>\n",
       "      <td>étude des voies</td>\n",
       "      <td>NOM PRP:det NOM</td>\n",
       "      <td>1</td>\n",
       "      <td>19.645812</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65046</th>\n",
       "      <td>pôle d'enseignement universitaire en santé</td>\n",
       "      <td>NOM PRP NOM ADJ PRP NOM</td>\n",
       "      <td>1</td>\n",
       "      <td>10.625860</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65047 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Collocation     Structure syntaxique  \\\n",
       "33271                     demande de consultation              NOM PRP NOM   \n",
       "26245                               chu de québec         VER:pper PRP NOM   \n",
       "30446                          comité des usagers          NOM PRP:det ADJ   \n",
       "36854                            comité d'éthique              NOM PRP NOM   \n",
       "50891      clinique interdisciplinaire de mémoire          NOM ADJ PRP NOM   \n",
       "...                                           ...                      ...   \n",
       "26643                           neurologie de hej              NOM PRP NOM   \n",
       "26645                        adaptée au préalable     VER:pper PRP:det NOM   \n",
       "26646                         transporté par évaq         VER:pper PRP NOM   \n",
       "26647                             étude des voies          NOM PRP:det NOM   \n",
       "65046  pôle d'enseignement universitaire en santé  NOM PRP NOM ADJ PRP NOM   \n",
       "\n",
       "       Fréquence        LLR   p-value   DF  \n",
       "33271       3776       -inf  1.000000  531  \n",
       "26245       3533       -inf  1.000000  789  \n",
       "30446       2837       -inf  1.000000  544  \n",
       "36854       2792       -inf  1.000000  537  \n",
       "50891       2695       -inf  1.000000  531  \n",
       "...          ...        ...       ...  ...  \n",
       "26643          1  18.399623  0.000018    1  \n",
       "26645          1  19.395162  0.000011    1  \n",
       "26646          1  21.033785  0.000005    1  \n",
       "26647          1  19.645812  0.000009    1  \n",
       "65046          1  10.625860  0.001115    1  \n",
       "\n",
       "[65047 rows x 6 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for term in terms:\n",
    "    term['DF'] = dfs[term['Collocation']] \n",
    "DataFrame(terms).sort_values(by=\"Fréquence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c08c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(terms)\n",
    "df.sort_values(['Fréquence'], \n",
    "            axis=0,\n",
    "            ascending=[False], \n",
    "            inplace=True)\n",
    "\n",
    "output_path = path.join('../04-filtrage/output/', acteur, acteur + '_significant-collocations.csv') \n",
    "df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ef6e6",
   "metadata": {},
   "source": [
    "### **KWIC (Keyword in Context)**\n",
    "Termes d'intérêt : \n",
    "- « Programme »\n",
    "- « Plan »\n",
    "- « Service(s) de » \n",
    "- « Intervenant(e) en »\n",
    "- « Professionnel de »\n",
    "- « Institut (du/de) »\n",
    "- « Groupe de recherche en »\n",
    "- « Personne »\n",
    "- « Infirmière (en) »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans notre cas on veut que ça débute par le mot-clé donc le contexte est un peu plus simple\n",
    "# penser à généraliser avec des expressions régulières\n",
    "kw = ['programme', 'plan ', 'service', 'intervenant', 'infirmière en', 'institut', 'groupe de recherche', 'personne', 'maladie']\n",
    "\n",
    "ngrammes_kwic = [\" \".join([t[0] for t in ng]).replace(\"' \", \"'\") for ng in ngrammes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c50b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrant = pd.DataFrame(columns=['Mot-clé','Concordance', 'Fréquence'])\n",
    "kwic = {w : [] for w in kw} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255df07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ngrammes_kwic: # on pourrait aussi chercher dans les terms, mais on perd certains termes d'intérêt avec le filtrage syntaxique\n",
    "    for w in kw:\n",
    "        if t.startswith(w):\n",
    "            kwic[w].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic = {term: FreqDist(kwic[term]) for term in kwic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in kw:\n",
    "    df = pd.DataFrame(kwic[term].items(), columns=['Concordance', \"Fréquence\"])\n",
    "    df.sort_values([\"Fréquence\"], \n",
    "        axis=0,\n",
    "        ascending=[False], \n",
    "        inplace=True)\n",
    "\n",
    "    df.insert(0, 'Mot-clé', term)\n",
    "    extrant = pd.concat([extrant, df])\n",
    "\n",
    "\n",
    "extrant = extrant[extrant['Fréquence'] > 30] \n",
    "\n",
    "file_path = '../04-filtrage/output/'\n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag)\n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "\n",
    "extrant.to_csv(file_path + '_KWIC' +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45949eb3",
   "metadata": {},
   "source": [
    "### **Extraction de termes MeSH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "file_path = '../04-filtrage/MeSH/mesh-fr.txt'\n",
    "\n",
    "with open (file_path, 'r', encoding='utf-8') as f:\n",
    "    mesh = [tuple(tokenizer_re.tokenize(w)) for w in f.readlines()]\n",
    "    tokenizer_mesh = MWETokenizer(mesh, separator= ' ')\n",
    "    mesh = [tokenizer_mesh.tokenize(w)[0].lower() for w in mesh]\n",
    "    mesh = [w for w in mesh if len(w.split()) > 1] # On ne retient que les termes complexes\n",
    "    #mesh = [tuple(t.strip('.').lower().split()) for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_mesh = tokenizer_mesh.tokenize([t['Collocation'] for t in terms])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40edc2e1",
   "metadata": {},
   "source": [
    "# **MODIF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400cd29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'termes_mesh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\p1115145\\Documents\\text-mining-project\\00-Jupyter-Notebooks\\2_Prétraitement.ipynb Cellule 89\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pr%C3%A9traitement.ipynb#ch0000084?line=0'>1</a>\u001b[0m \u001b[39m# MODIFICATION À APPORTER : dans l'extrant où on voit les termes mesh, ajouter la fréquence\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pr%C3%A9traitement.ipynb#ch0000084?line=2'>3</a>\u001b[0m termes_mesh[:\u001b[39m15\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pr%C3%A9traitement.ipynb#ch0000084?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m termes_mesh:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/2_Pr%C3%A9traitement.ipynb#ch0000084?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(t, corpus\u001b[39m.\u001b[39mcount(t))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'termes_mesh' is not defined"
     ]
    }
   ],
   "source": [
    "# MODIFICATION À APPORTER : dans l'extrant où on voit les termes mesh, ajouter la fréquence\n",
    "\n",
    "termes_mesh[:15]\n",
    "\n",
    "for t in termes_mesh:\n",
    "    print(t, corpus.count(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_mesh = []\n",
    "\n",
    "for t in extr_mesh:\n",
    "    if t in mesh:\n",
    "        termes_mesh.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/output/'\n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag)\n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "df = DataFrame(termes_mesh)\n",
    "df.to_csv(file_path + '_MeSH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685131ee",
   "metadata": {},
   "source": [
    "### **Extraction de termes SNOMED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "file_path = '../04-filtrage/SNOMED_fr.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    sm = read_csv(f, sep=';')\n",
    "    sm = list(dict.fromkeys([str(t).strip().lower() for t in sm['term'].tolist()]))\n",
    "\n",
    "    sm = [tuple(tokenizer_re.tokenize(w)) for w in sm if len(w.split()) > 1]\n",
    "    tokenizer_sm = MWETokenizer(sm, separator = ' ')\n",
    "\n",
    "    sm = [tokenizer_sm.tokenize(w)[0].lower() for w in sm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_sm = tokenizer_sm.tokenize([t[0] for t in terms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "termes_sm = []\n",
    "\n",
    "for t in extr_sm:\n",
    "    if t in sm:\n",
    "        termes_sm.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../04-filtrage/output' \n",
    "if sous_corpus:\n",
    "    file_path = path.join(file_path, acteur, tag, tag) \n",
    "\n",
    "else :\n",
    "    file_path = path.join(file_path, acteur, acteur)\n",
    "\n",
    "\n",
    "df = DataFrame(termes_sm)\n",
    "\n",
    "df.to_csv(file_path + '_SNOMED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10875e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79bb76bbc4f9ba1f8df5efe8db67aae07079a51dc7b5004f49990e90f5993a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
