{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Keywords Clustering** \n",
    "We will compare different models implemeting each of these parameters:\n",
    "- K-Means vs Expectation maximization VS Agglomerative algorithm\n",
    "- One-hot vs Sentence transformers embedding\n",
    "- [Distance measure (Euclidean VS Cosine)][?]\n",
    "- Number extracted features (25%, 50%, 75%, 100% of total number of features)\n",
    "- Number of clusters (50, 100, 150, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*En grande partie basé sur le tutoriel suivant* :   \n",
    "https://colab.research.google.com/drive/1HHNFjKlip1AaFIuvvn0AicWyv6egLOZw?usp=sharing#scrollTo=zhP1daroRzRV    \n",
    "(Une approche à base de Word embedding - on pourrait utiliser les scores TF-IDF ou OKapi pour les traits discriminants plutôt que la fréquence (voir plus bas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithme</th>\n",
       "      <th>embedding</th>\n",
       "      <th>N features</th>\n",
       "      <th>K (nb clusters)</th>\n",
       "      <th>Score Silhouette</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-means</td>\n",
       "      <td>One-Hot</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K-means</td>\n",
       "      <td>Sentence transformers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Expectation-Maximization</td>\n",
       "      <td>One-Hot</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Expectation-Maximization</td>\n",
       "      <td>Sentence transformers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AgglomerativeClustering</td>\n",
       "      <td>One-Hot</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AgglomerativeClustering</td>\n",
       "      <td>Sentence transformers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 algorithme              embedding N features K (nb clusters)  \\\n",
       "0                   K-means                One-Hot       None            None   \n",
       "1                   K-means  Sentence transformers       None            None   \n",
       "2  Expectation-Maximization                One-Hot       None            None   \n",
       "3  Expectation-Maximization  Sentence transformers       None            None   \n",
       "4   AgglomerativeClustering                One-Hot       None            None   \n",
       "5   AgglomerativeClustering  Sentence transformers       None            None   \n",
       "\n",
       "  Score Silhouette  \n",
       "0             None  \n",
       "1             None  \n",
       "2             None  \n",
       "3             None  \n",
       "4             None  \n",
       "5             None  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithmes = ['K-means', 'Expectation-Maximization', 'AgglomerativeClustering']\n",
    "embeddings = ['One-Hot', 'Sentence transformers']\n",
    "features = [25, 50, 100, 150, 200]\n",
    "\n",
    "results = []\n",
    "for algorithme in algorithmes:\n",
    "    for embedding in embeddings:\n",
    "            results.append(\\\n",
    "            {'algorithme' : algorithme,\\\n",
    "                'embedding': embedding, \\\n",
    "                'N features': None, \\\n",
    "                'Score Silhouette': None})\n",
    "\n",
    "\n",
    "# On va remplir ce dictionnaire avec les bons scores au fur et à mesure qu'on expérimente\n",
    "results = DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_results(algo, embed, dist, n_f, k, silhouette, results=results):\n",
    "    results.loc[ \\\n",
    "        (results['algorithme'] == algo) & \\\n",
    "        (results['embedding'] == embed), 'N features'] = n_f\n",
    "\n",
    "    results.loc[ \\\n",
    "        (results['algorithme'] == algo) & \\\n",
    "        (results['embedding'] == embed), 'distance'] = dist\n",
    "\n",
    "    results.loc[ \\\n",
    "        (results['algorithme'] == algo) & \\\n",
    "        (results['embedding'] == embed), 'K (nb clusters)'] = k\n",
    "\n",
    "    results.loc[ \\\n",
    "        (results['algorithme'] == algo) & \\\n",
    "        (results['embedding'] == embed), 'Score Silhouette'] = silhouette\n",
    "\n",
    "    results=results[['algorithme', 'embedding', 'N features', 'distance', 'K (nb clusters)', 'Score Silhouette']]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importer la liste de termes candidats avec leur fréquence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# get data file names\n",
    "path ='../05-transformation'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    dfs.append(pd.read_csv(filename))\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "big_frame = pd.concat(dfs, ignore_index=True).drop(columns=[\"Unnamed: 0\", 'Structure syntaxique', 'LLR', 'TF (sklearn)', 'DF (sklearn)', 'TF-IDF', 'OkapiBM25', 'Terme formatté'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Terme</th>\n",
       "      <th>Fréquence (TF)</th>\n",
       "      <th>Fréquence documentaire (DF)</th>\n",
       "      <th>isMeSHTerm</th>\n",
       "      <th>isTaxoTerm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ciusss_centresud</td>\n",
       "      <td>clinique de cognition</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ciusss_centresud</td>\n",
       "      <td>problèmes liés</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciusss_centresud</td>\n",
       "      <td>usagers du sud-ouest-verdun</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ciusss_centresud</td>\n",
       "      <td>accès aux services secteurs des faubourgs</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ciusss_centresud</td>\n",
       "      <td>gynécologie</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12646</th>\n",
       "      <td>pinel</td>\n",
       "      <td>statut temps complet permanent salaire</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12647</th>\n",
       "      <td>pinel</td>\n",
       "      <td>acquisition</td>\n",
       "      <td>75</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12648</th>\n",
       "      <td>pinel</td>\n",
       "      <td>philippe-pinel</td>\n",
       "      <td>233</td>\n",
       "      <td>94</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12649</th>\n",
       "      <td>pinel</td>\n",
       "      <td>statut</td>\n",
       "      <td>138</td>\n",
       "      <td>66</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12650</th>\n",
       "      <td>pinel</td>\n",
       "      <td>événement</td>\n",
       "      <td>45</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12651 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Corpus                                      Terme  \\\n",
       "0      ciusss_centresud                      clinique de cognition   \n",
       "1      ciusss_centresud                             problèmes liés   \n",
       "2      ciusss_centresud                usagers du sud-ouest-verdun   \n",
       "3      ciusss_centresud  accès aux services secteurs des faubourgs   \n",
       "4      ciusss_centresud                                gynécologie   \n",
       "...                 ...                                        ...   \n",
       "12646             pinel     statut temps complet permanent salaire   \n",
       "12647             pinel                                acquisition   \n",
       "12648             pinel                             philippe-pinel   \n",
       "12649             pinel                                     statut   \n",
       "12650             pinel                                  événement   \n",
       "\n",
       "       Fréquence (TF)  Fréquence documentaire (DF)  isMeSHTerm  isTaxoTerm  \n",
       "0                  24                            6       False       False  \n",
       "1                  42                           30       False       False  \n",
       "2                  24                           24       False       False  \n",
       "3                  58                           58       False       False  \n",
       "4                  20                           14       False        True  \n",
       "...               ...                          ...         ...         ...  \n",
       "12646              19                           19       False       False  \n",
       "12647              75                           70       False       False  \n",
       "12648             233                           94       False       False  \n",
       "12649             138                           66       False       False  \n",
       "12650              45                           31       False       False  \n",
       "\n",
       "[12651 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va additionner les fréquences brutes (TF) et fréquences documentaires (DF) de chaque terme dans les différents corpus ; ensuite on va travailler avec ces valeurs uniquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Terme</th>\n",
       "      <th>Fréquence (TF)</th>\n",
       "      <th>Fréquence documentaire (DF)</th>\n",
       "      <th>isMeSHTerm</th>\n",
       "      <th>isTaxoTerm</th>\n",
       "      <th>Fréquence totale (TF)</th>\n",
       "      <th>Fréquence documentaire totale (DF)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ciusss_centresud</td>\n",
       "      <td>clinique de cognition</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ciusss_centresud</td>\n",
       "      <td>problèmes liés</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciusss_centresud</td>\n",
       "      <td>usagers du sud-ouest-verdun</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ciusss_centresud</td>\n",
       "      <td>accès aux services secteurs des faubourgs</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ciusss_centresud</td>\n",
       "      <td>gynécologie</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12646</th>\n",
       "      <td>pinel</td>\n",
       "      <td>statut temps complet permanent salaire</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12647</th>\n",
       "      <td>pinel</td>\n",
       "      <td>acquisition</td>\n",
       "      <td>75</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>133</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12648</th>\n",
       "      <td>pinel</td>\n",
       "      <td>philippe-pinel</td>\n",
       "      <td>233</td>\n",
       "      <td>94</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>233</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12649</th>\n",
       "      <td>pinel</td>\n",
       "      <td>statut</td>\n",
       "      <td>138</td>\n",
       "      <td>66</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>450</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12650</th>\n",
       "      <td>pinel</td>\n",
       "      <td>événement</td>\n",
       "      <td>45</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>65</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12651 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Corpus                                      Terme  \\\n",
       "0      ciusss_centresud                      clinique de cognition   \n",
       "1      ciusss_centresud                             problèmes liés   \n",
       "2      ciusss_centresud                usagers du sud-ouest-verdun   \n",
       "3      ciusss_centresud  accès aux services secteurs des faubourgs   \n",
       "4      ciusss_centresud                                gynécologie   \n",
       "...                 ...                                        ...   \n",
       "12646             pinel     statut temps complet permanent salaire   \n",
       "12647             pinel                                acquisition   \n",
       "12648             pinel                             philippe-pinel   \n",
       "12649             pinel                                     statut   \n",
       "12650             pinel                                  événement   \n",
       "\n",
       "       Fréquence (TF)  Fréquence documentaire (DF)  isMeSHTerm  isTaxoTerm  \\\n",
       "0                  24                            6       False       False   \n",
       "1                  42                           30       False       False   \n",
       "2                  24                           24       False       False   \n",
       "3                  58                           58       False       False   \n",
       "4                  20                           14       False        True   \n",
       "...               ...                          ...         ...         ...   \n",
       "12646              19                           19       False       False   \n",
       "12647              75                           70       False       False   \n",
       "12648             233                           94       False       False   \n",
       "12649             138                           66       False       False   \n",
       "12650              45                           31       False       False   \n",
       "\n",
       "       Fréquence totale (TF)  Fréquence documentaire totale (DF)  \n",
       "0                         24                                   6  \n",
       "1                         42                                  30  \n",
       "2                         24                                  24  \n",
       "3                         58                                  58  \n",
       "4                         20                                  14  \n",
       "...                      ...                                 ...  \n",
       "12646                     19                                  19  \n",
       "12647                    133                                 124  \n",
       "12648                    233                                  94  \n",
       "12649                    450                                 254  \n",
       "12650                     65                                 171  \n",
       "\n",
       "[12651 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame['Fréquence totale (TF)'] = big_frame.groupby(['Terme'])['Fréquence (TF)'].transform('sum')\n",
    "big_frame['Fréquence documentaire totale (DF)'] = big_frame.groupby(['Terme'])['Fréquence documentaire (DF)'].transform('sum')\n",
    "\n",
    "big_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terme</th>\n",
       "      <th>isMeSHTerm</th>\n",
       "      <th>isTaxoTerm</th>\n",
       "      <th>Fréquence totale (TF)</th>\n",
       "      <th>Fréquence documentaire totale (DF)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clinique de cognition</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>problèmes liés</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usagers du sud-ouest-verdun</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accès aux services secteurs des faubourgs</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gynécologie</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12641</th>\n",
       "      <td>vie professionnelle favorisant la formation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12643</th>\n",
       "      <td>jours de congé</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>69</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12645</th>\n",
       "      <td>jour d'utilisation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12646</th>\n",
       "      <td>statut temps complet permanent salaire</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12648</th>\n",
       "      <td>philippe-pinel</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>233</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11837 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Terme  isMeSHTerm  isTaxoTerm  \\\n",
       "0                            clinique de cognition       False       False   \n",
       "1                                   problèmes liés       False       False   \n",
       "2                      usagers du sud-ouest-verdun       False       False   \n",
       "3        accès aux services secteurs des faubourgs       False       False   \n",
       "4                                      gynécologie       False        True   \n",
       "...                                            ...         ...         ...   \n",
       "12641  vie professionnelle favorisant la formation       False       False   \n",
       "12643                               jours de congé       False       False   \n",
       "12645                           jour d'utilisation       False       False   \n",
       "12646       statut temps complet permanent salaire       False       False   \n",
       "12648                               philippe-pinel       False       False   \n",
       "\n",
       "       Fréquence totale (TF)  Fréquence documentaire totale (DF)  \n",
       "0                         24                                   6  \n",
       "1                         42                                  30  \n",
       "2                         24                                  24  \n",
       "3                         58                                  58  \n",
       "4                         20                                  14  \n",
       "...                      ...                                 ...  \n",
       "12641                     68                                  68  \n",
       "12643                     69                                  61  \n",
       "12645                     61                                  61  \n",
       "12646                     19                                  19  \n",
       "12648                    233                                  94  \n",
       "\n",
       "[11837 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame = big_frame.drop(columns = ['Corpus', 'Fréquence (TF)', 'Fréquence documentaire (DF)'])\n",
    "\n",
    "big_frame = big_frame.drop_duplicates(subset=['Terme'])\n",
    "big_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_frame['TF + DF'] = big_frame['Fréquence totale (TF)'] + big_frame['Fréquence documentaire totale (DF)']\n",
    "big_frame['Terme'] = big_frame['Terme'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Embedding : One-hot encoding*  \n",
    "> One Hot encoding is a representation of categorical variables as binary vectors. Each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Créer une fonction avec N en paramètres = nombre de features retenus souhaités*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"(\\w+\\'|\\w+-\\w+|\\(|\\)|\\w+)\")\n",
    "\n",
    "file_path = \"../04-filtrage/stopwords.txt\"\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "\n",
    "def to_tokens(kw, min_chars=2):\n",
    "    tokens = tokenizer.tokenize(str(kw)) # split the string into a list of words\n",
    "    tokens = [word for word in tokens if len(word) > min_chars] \n",
    "    tokens = [str(word) for word in tokens if word not in stopwords] \n",
    "    \n",
    "    tokens = set(tokens) # to remove duplicates\n",
    "    tokens = sorted(tokens) # converts our set back to a list and sorts words in alphabetical order\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_oh = big_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_oh[\"tokens\"] = keywords_oh[\"Terme\"].apply(lambda x: to_tokens(\n",
    "    x,\n",
    "    min_chars=2,\n",
    ")).astype(str)\n",
    "\n",
    "# Test - Seulement retenir des n-grammes où n est au-dessus de 2\n",
    "keywords_oh[\"len\"] = keywords_oh[\"tokens\"].apply(lambda x : len(x))\n",
    "keywords_oh = keywords_oh[keywords_oh['len'] > 1].drop(columns=[\"len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terme</th>\n",
       "      <th>isMeSHTerm</th>\n",
       "      <th>isTaxoTerm</th>\n",
       "      <th>Fréquence totale (TF)</th>\n",
       "      <th>Fréquence documentaire totale (DF)</th>\n",
       "      <th>TF + DF</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clinique de cognition</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>['clinique', 'cognition']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>problèmes liés</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>72</td>\n",
       "      <td>['liés', 'problèmes']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usagers du sud-ouest-verdun</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "      <td>['sud-ouest', 'usagers', 'verdun']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accès aux services secteurs des faubourgs</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>116</td>\n",
       "      <td>['accès', 'faubourgs', 'secteurs', 'services']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gynécologie</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>['gynécologie']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12641</th>\n",
       "      <td>vie professionnelle favorisant la formation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>136</td>\n",
       "      <td>['favorisant', 'formation', 'professionnelle',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12643</th>\n",
       "      <td>jours de congé</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>69</td>\n",
       "      <td>61</td>\n",
       "      <td>130</td>\n",
       "      <td>['congé', 'jours']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12645</th>\n",
       "      <td>jour d'utilisation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>122</td>\n",
       "      <td>['jour', 'utilisation']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12646</th>\n",
       "      <td>statut temps complet permanent salaire</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>['complet', 'permanent', 'salaire', 'statut', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12648</th>\n",
       "      <td>philippe-pinel</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>233</td>\n",
       "      <td>94</td>\n",
       "      <td>327</td>\n",
       "      <td>['philippe-pinel']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11837 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Terme  isMeSHTerm  isTaxoTerm  \\\n",
       "0                            clinique de cognition       False       False   \n",
       "1                                   problèmes liés       False       False   \n",
       "2                      usagers du sud-ouest-verdun       False       False   \n",
       "3        accès aux services secteurs des faubourgs       False       False   \n",
       "4                                      gynécologie       False        True   \n",
       "...                                            ...         ...         ...   \n",
       "12641  vie professionnelle favorisant la formation       False       False   \n",
       "12643                               jours de congé       False       False   \n",
       "12645                           jour d'utilisation       False       False   \n",
       "12646       statut temps complet permanent salaire       False       False   \n",
       "12648                               philippe-pinel       False       False   \n",
       "\n",
       "       Fréquence totale (TF)  Fréquence documentaire totale (DF)  TF + DF  \\\n",
       "0                         24                                   6       30   \n",
       "1                         42                                  30       72   \n",
       "2                         24                                  24       48   \n",
       "3                         58                                  58      116   \n",
       "4                         20                                  14       34   \n",
       "...                      ...                                 ...      ...   \n",
       "12641                     68                                  68      136   \n",
       "12643                     69                                  61      130   \n",
       "12645                     61                                  61      122   \n",
       "12646                     19                                  19       38   \n",
       "12648                    233                                  94      327   \n",
       "\n",
       "                                                  tokens  \n",
       "0                              ['clinique', 'cognition']  \n",
       "1                                  ['liés', 'problèmes']  \n",
       "2                     ['sud-ouest', 'usagers', 'verdun']  \n",
       "3         ['accès', 'faubourgs', 'secteurs', 'services']  \n",
       "4                                        ['gynécologie']  \n",
       "...                                                  ...  \n",
       "12641  ['favorisant', 'formation', 'professionnelle',...  \n",
       "12643                                 ['congé', 'jours']  \n",
       "12645                            ['jour', 'utilisation']  \n",
       "12646  ['complet', 'permanent', 'salaire', 'statut', ...  \n",
       "12648                                 ['philippe-pinel']  \n",
       "\n",
       "[11837 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fontion prend en paramètres :\n",
    "# - un DataFrame qui contient un champ où sont consignés des vecteurs contenant les tokens de chaque mot-clé\n",
    "# - le nombre de features maximal qui doit être retenu pour constituer le plongement lexical (embedding)\n",
    "\n",
    "# Elle retourne le dataframe où une colonne 'vector' a été ajoutée avec le bon nombre de features\n",
    "\n",
    "def features_embeddings(df, n_features):\n",
    "    vocab = sorted(set(df[\"tokens\"].explode()))\n",
    "    len(vocab)\n",
    "\n",
    "    counter = Counter(df[\"tokens\"].explode().to_list())\n",
    "    vocab = []\n",
    "\n",
    "    # Ici, ça pourrait être intéressant de retenir sur la base du score TF-IDF ou OKapi\n",
    "    for key,value in counter.most_common(n_features):\n",
    "        vocab.append(key)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "vocab = features_embeddings(keywords_oh, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terme</th>\n",
       "      <th>isMeSHTerm</th>\n",
       "      <th>isTaxoTerm</th>\n",
       "      <th>Fréquence totale (TF)</th>\n",
       "      <th>Fréquence documentaire totale (DF)</th>\n",
       "      <th>TF + DF</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clinique de cognition</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>['clinique', 'cognition']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>problèmes liés</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>72</td>\n",
       "      <td>['liés', 'problèmes']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usagers du sud-ouest-verdun</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "      <td>['sud-ouest', 'usagers', 'verdun']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accès aux services secteurs des faubourgs</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>116</td>\n",
       "      <td>['accès', 'faubourgs', 'secteurs', 'services']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gynécologie</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>['gynécologie']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12641</th>\n",
       "      <td>vie professionnelle favorisant la formation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>136</td>\n",
       "      <td>['favorisant', 'formation', 'professionnelle',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12643</th>\n",
       "      <td>jours de congé</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>69</td>\n",
       "      <td>61</td>\n",
       "      <td>130</td>\n",
       "      <td>['congé', 'jours']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12645</th>\n",
       "      <td>jour d'utilisation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>122</td>\n",
       "      <td>['jour', 'utilisation']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12646</th>\n",
       "      <td>statut temps complet permanent salaire</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>['complet', 'permanent', 'salaire', 'statut', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12648</th>\n",
       "      <td>philippe-pinel</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>233</td>\n",
       "      <td>94</td>\n",
       "      <td>327</td>\n",
       "      <td>['philippe-pinel']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11837 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Terme  isMeSHTerm  isTaxoTerm  \\\n",
       "0                            clinique de cognition       False       False   \n",
       "1                                   problèmes liés       False       False   \n",
       "2                      usagers du sud-ouest-verdun       False       False   \n",
       "3        accès aux services secteurs des faubourgs       False       False   \n",
       "4                                      gynécologie       False        True   \n",
       "...                                            ...         ...         ...   \n",
       "12641  vie professionnelle favorisant la formation       False       False   \n",
       "12643                               jours de congé       False       False   \n",
       "12645                           jour d'utilisation       False       False   \n",
       "12646       statut temps complet permanent salaire       False       False   \n",
       "12648                               philippe-pinel       False       False   \n",
       "\n",
       "       Fréquence totale (TF)  Fréquence documentaire totale (DF)  TF + DF  \\\n",
       "0                         24                                   6       30   \n",
       "1                         42                                  30       72   \n",
       "2                         24                                  24       48   \n",
       "3                         58                                  58      116   \n",
       "4                         20                                  14       34   \n",
       "...                      ...                                 ...      ...   \n",
       "12641                     68                                  68      136   \n",
       "12643                     69                                  61      130   \n",
       "12645                     61                                  61      122   \n",
       "12646                     19                                  19       38   \n",
       "12648                    233                                  94      327   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0                              ['clinique', 'cognition']   \n",
       "1                                  ['liés', 'problèmes']   \n",
       "2                     ['sud-ouest', 'usagers', 'verdun']   \n",
       "3         ['accès', 'faubourgs', 'secteurs', 'services']   \n",
       "4                                        ['gynécologie']   \n",
       "...                                                  ...   \n",
       "12641  ['favorisant', 'formation', 'professionnelle',...   \n",
       "12643                                 ['congé', 'jours']   \n",
       "12645                            ['jour', 'utilisation']   \n",
       "12646  ['complet', 'permanent', 'salaire', 'statut', ...   \n",
       "12648                                 ['philippe-pinel']   \n",
       "\n",
       "                                                  vector  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "12641  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12643  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12645  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12646  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12648  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[11837 rows x 8 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_vector(keyword,vocab):\n",
    "    vector = []\n",
    "    for word in vocab:\n",
    "        if word in keyword:\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    return vector\n",
    "    \n",
    "keywords_oh[\"vector\"] = keywords_oh[\"tokens\"].apply(lambda x: to_vector(x,vocab))\n",
    "keywords_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Embedding : Sentence transformers*  \n",
    "\n",
    "> \"A **transformer** is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part  of the input data.\n",
    "Transformers are increasingly the model of choice for NLP problems, replacing RNN models such as long short-term memory (LSTM). The additional  training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.\"   \n",
    "  \n",
    "(https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_st = big_frame[['Terme', 'isMeSHTerm',\t'isTaxoTerm',\t'Fréquence totale (TF)',\t'Fréquence documentaire totale (DF)',\t'TF + DF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:560: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# On va utiliser un modèle BERT/sentence transformers (fr) pour extraire nos embeddings plutôt que des simples one-hot encoding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model =  SentenceTransformer(\"dangvantuan/sentence-camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\p1115145\\Documents\\text-mining-project\\00-Jupyter-Notebooks\\4-Keywords-clustering_brouillon.ipynb Cellule 24\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sentences \u001b[39m=\u001b[39m keywords_st[\u001b[39m'\u001b[39m\u001b[39mTerme\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embeddings_st \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(sentences)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m keywords_st\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:164\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    161\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    163\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 164\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    167\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrans_features, return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:848\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    839\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    841\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    842\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    843\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    846\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    847\u001b[0m )\n\u001b[1;32m--> 848\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    849\u001b[0m     embedding_output,\n\u001b[0;32m    850\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    851\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    852\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    853\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    854\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    855\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    856\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    857\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    858\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    859\u001b[0m )\n\u001b[0;32m    860\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    861\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    515\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    516\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    517\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    525\u001b[0m         hidden_states,\n\u001b[0;32m    526\u001b[0m         attention_mask,\n\u001b[0;32m    527\u001b[0m         layer_head_mask,\n\u001b[0;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    530\u001b[0m         past_key_value,\n\u001b[0;32m    531\u001b[0m         output_attentions,\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:409\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    398\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    399\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    406\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    407\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    410\u001b[0m         hidden_states,\n\u001b[0;32m    411\u001b[0m         attention_mask,\n\u001b[0;32m    412\u001b[0m         head_mask,\n\u001b[0;32m    413\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    414\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    418\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:336\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    327\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    328\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    334\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    335\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 336\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    337\u001b[0m         hidden_states,\n\u001b[0;32m    338\u001b[0m         attention_mask,\n\u001b[0;32m    339\u001b[0m         head_mask,\n\u001b[0;32m    340\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    341\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    342\u001b[0m         past_key_value,\n\u001b[0;32m    343\u001b[0m         output_attentions,\n\u001b[0;32m    344\u001b[0m     )\n\u001b[0;32m    345\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    346\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:200\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    191\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    192\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    199\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 200\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[0;32m    202\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentences = keywords_st['Terme'].tolist()\n",
    "embeddings_st = model.encode(sentences)\n",
    "\n",
    "keywords_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **K-means clustering** (*sklearn*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Un premier essai sur nos one-hot embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_kmeans(df, embedding, embedding_str):\n",
    "    K = range(20, round(len(vocab)))\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # On commence par tester différentes valeurs de k pour trouver celle pour laquelle le score Silhouette est\n",
    "    # le plus élevé\n",
    "    for k in K:\n",
    "        X = embedding\n",
    "        km = KMeans(n_clusters=k, init='k-means++', algorithm='elkan', max_iter=200, n_init=1).fit(X)\n",
    "\n",
    "        # Run LSA\n",
    "        # Since LSA/SVD results are not normalized,\n",
    "        # we redo the normalization to improve the k-means result.\n",
    "        svd = TruncatedSVD(n_components= round(len(vocab)/2))\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "        X = lsa.fit_transform(X)\n",
    "        #### FIN LSA\n",
    "        km.fit(X)\n",
    "\n",
    "        labels = km.labels_\n",
    "        silhouette_scores.append([k, metrics.silhouette_score(X, labels)])\n",
    "\n",
    "    df = DataFrame(silhouette_scores, columns=['Nombre de clusters (k)', 'Score Silhouette'])\n",
    "    true_k = int(df[df['Score Silhouette'] == df['Score Silhouette'].max()]['Nombre de clusters (k)'])\n",
    "    resultats = df[df['Score Silhouette'] == df['Score Silhouette'].max()]\n",
    "\n",
    "    print(\"Score Silhouette\")\n",
    "    print(\"On va regrouper nos termes en \" + str(true_k) + \" clusters.\")\n",
    "\n",
    "    # On stocke le score dans notre tableau de résultats\n",
    "    algorithme = 'K-means'\n",
    "    distance = 'Euclidean'\n",
    "    features = len(vocab)\n",
    "\n",
    "    ### On reroule ensuite avec la valeur retenue et on va stocker les résultats dans un CSV\n",
    "    X = embedding\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', algorithm='elkan', max_iter=200, n_init=1).fit(X)\n",
    "\n",
    "    # Run LSA\n",
    "    # Since LSA/SVD results are not normalized,\n",
    "    # we redo the normalization to improve the k-means result.\n",
    "    svd = TruncatedSVD(n_components=round(len(vocab)/2))\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "    km.fit(X)\n",
    "\n",
    "    labels = km.labels_\n",
    "    df[\"kmeans\"] = km.labels_\n",
    "\n",
    "\n",
    "    # Pour mieux interpréter, on assigne un label significatif à nos clusters\n",
    "    # On retient le terme pour chaque cluster dont la valeur TF + DF est la plus élevée\n",
    "    current_labels = set(km.labels_.tolist())\n",
    "\n",
    "    desired_labels = {x : None for x in current_labels} # (on initialise à None)\n",
    "\n",
    "    for label in current_labels:\n",
    "        cluster = df[df[\"kmeans\"] == label]\n",
    "        max_freq = cluster['TF + DF'].max()\n",
    "        new_label = cluster[cluster['TF + DF'] == max_freq]['Terme'].values[0]\n",
    "\n",
    "        desired_labels[label] = new_label\n",
    "\n",
    "    df['Cluster_kmeans_euclidean'] = df['kmeans'].map(desired_labels)\n",
    "\n",
    "    df.sort_values([\"Cluster_kmeans_euclidean\"], \n",
    "            axis=0,\n",
    "            ascending=[False], \n",
    "            inplace=True)\n",
    "\n",
    "    df = df[['Cluster_kmeans_euclidean', 'Terme', 'Fréquence totale (TF)', 'Fréquence documentaire totale (DF)', 'TF + DF']]\n",
    "    df = df.sort_values(['Cluster_kmeans_euclidean', 'Fréquence totale (TF)', 'Fréquence documentaire totale (DF)'],\n",
    "                ascending = [True, False, False])\n",
    "\n",
    "\n",
    "    # On stocke les résultats dans un CSV\n",
    "    base_path = '../06-clustering/'\n",
    "    file_path = base_path + algorithme + '_' + embedding_str + '_' + distance + '_' + str(features) + '.csv'\n",
    "    df.to_csv(file_path)\n",
    "\n",
    "    return resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Silhouette\n",
      "On va regrouper nos termes en 49 clusters.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (11837) does not match length of index (30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\p1115145\\Documents\\text-mining-project\\00-Jupyter-Notebooks\\4-Keywords-clustering_brouillon.ipynb Cellule 28\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m embeddings_oh \u001b[39m=\u001b[39m keywords_oh[\u001b[39m\"\u001b[39m\u001b[39mvector\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto_list()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m resultats_km \u001b[39m=\u001b[39m results_kmeans(keywords_oh, embeddings_oh, \u001b[39m'\u001b[39;49m\u001b[39mOne-Hot\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m resultats_km\n",
      "\u001b[1;32mc:\\Users\\p1115145\\Documents\\text-mining-project\\00-Jupyter-Notebooks\\4-Keywords-clustering_brouillon.ipynb Cellule 28\u001b[0m in \u001b[0;36mresults_kmeans\u001b[1;34m(df, embedding, embedding_str)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X36sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m km\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X36sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m labels \u001b[39m=\u001b[39m km\u001b[39m.\u001b[39mlabels_\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X36sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mkmeans\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m km\u001b[39m.\u001b[39mlabels_\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X36sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Pour mieux interpréter, on assigne un label significatif à nos clusters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X36sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# On retient le terme pour chaque cluster dont la valeur TF + DF est la plus élevée\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/p1115145/Documents/text-mining-project/00-Jupyter-Notebooks/4-Keywords-clustering_brouillon.ipynb#X36sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m current_labels \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(km\u001b[39m.\u001b[39mlabels_\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3823\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3825\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3831\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3832\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   3834\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   3835\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   3836\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   3837\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   3838\u001b[0m     ):\n\u001b[0;32m   3839\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   3840\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4535\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4532\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   4534\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4535\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   4536\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\p1115145\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m--> 557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    562\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (11837) does not match length of index (30)"
     ]
    }
   ],
   "source": [
    "embeddings_oh = keywords_oh[\"vector\"].to_list()\n",
    "resultats_km = results_kmeans(keywords_oh, embeddings_oh, 'One-Hot')\n",
    "resultats_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On stocke le score dans notre tableau de résultats\n",
    "algorithme = 'K-means'\n",
    "embedding = 'One-Hot'\n",
    "distance = 'Euclidean'\n",
    "features = len(vocab)\n",
    "true_k = resultats_km['Nombre de clusters (k)'].values[0]\n",
    "score = resultats_km['Score Silhouette'].values[0]\n",
    "\n",
    "add_results(algorithme, embedding, distance, features, true_k, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_km = results_kmeans(keywords_st, embeddings_st, 'Sentence transformers')\n",
    "resultats_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On stocke le score dans notre tableau de résultats\n",
    "algorithme = 'K-means'\n",
    "embedding = 'Sentence transformers'\n",
    "distance = 'Euclidean'\n",
    "features = len(vocab)\n",
    "true_k = resultats_km['Nombre de clusters (k)'].values[0]\n",
    "score = resultats_km['Score Silhouette'].values[0]\n",
    "\n",
    "add_results(algorithme, embedding, distance, features, true_k, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reprendre ici si ça fonctionne*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **K-means clustering** (*sklearn*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Un deuxième essai, cette fois sur sur nos transformers embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(20,len(vocab))\n",
    "#Sum_of_squared_distances = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K:\n",
    "    #true_k = int(input())\n",
    "    X = embeddings\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', algorithm='elkan', max_iter=200, n_init=1).fit(X)\n",
    "    #Sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "    # Run LSA\n",
    "    # Since LSA/SVD results are not normalized,\n",
    "    # we redo the normalization to improve the k-means result.\n",
    "    svd = TruncatedSVD(n_components=round(k/4))\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "    #keywords[\"Cluster\"] = list(kmeans.labels_)\n",
    "\n",
    "    #original_space_centroids = svd.inverse_transform(X)\n",
    "    #order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    silhouette_scores.append([k, metrics.silhouette_score(X, labels)])\n",
    "\n",
    "df = DataFrame(silhouette_scores, columns=['Nombre de clusters (k)', 'Score Silhouette'])\n",
    "true_k = int(df[df['Score Silhouette'] == df['Score Silhouette'].max()]['Nombre de clusters (k)'])\n",
    "\n",
    "# print(\"Méthode Elbow\")\n",
    "# plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Sum_of_squared_distances')\n",
    "# plt.title('Elbow Method For Optimal k')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Score Silhouette\")\n",
    "print(\"On va regrouper nos termes en \" + str(true_k) + \" clusters.\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithme = 'K-means'\n",
    "embedding = 'Sentence transformers'\n",
    "distance = 'Euclidean'\n",
    "features = len(vocab)\n",
    "\n",
    "add_results(algorithme, embedding, distance, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embeddings\n",
    "kmeans = KMeans(n_clusters=true_k, init='k-means++', algorithm='elkan', max_iter=200, n_init=1).fit(X)\n",
    "\n",
    "# Run LSA\n",
    "# Since LSA/SVD results are not normalized,\n",
    "# we redo the normalization to improve the k-means result.\n",
    "svd = TruncatedSVD(n_components=true_k)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "kmeans.fit(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "keywords_st[\"kmeans\"] = list(kmeans.labels_)\n",
    "\n",
    "keywords_st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_labels = set(kmeans.labels_.tolist())\n",
    "\n",
    "desired_labels = {x : None for x in current_labels} # (on initialise à None)\n",
    "\n",
    "for label in current_labels:\n",
    "    cluster = keywords_st[keywords_st[\"kmeans\"] == label]\n",
    "    max_freq = cluster['TF + DF'].max()\n",
    "    new_label = cluster[cluster['TF + DF'] == max_freq]['Terme'].values[0]\n",
    "\n",
    "    desired_labels[label] = new_label\n",
    "\n",
    "keywords_st['Cluster_kmeans_euclidean'] = keywords_st['kmeans'].map(desired_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords = keywords[['Terme', 'Fréquence (TF)', 'Fréquence documentaire (DF)', 'Cluster']]\n",
    "keywords_st.sort_values([\"Cluster_kmeans_euclidean\"], \n",
    "        axis=0,\n",
    "        ascending=[False], \n",
    "        inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords = keywords.drop(columns=['TF + DF', 'tokens', 'vector', 'kmeans'])\n",
    "\n",
    "keywords_st = keywords_st[['Corpus', 'Cluster_kmeans_euclidean', 'kmeans', 'Terme', 'Fréquence (TF)', 'Fréquence documentaire (DF)', 'TF + DF']]\n",
    "keywords_st = keywords_st.sort_values(['Cluster_kmeans_euclidean', 'Fréquence (TF)', 'Fréquence documentaire (DF)'],\n",
    "              ascending = [True, False, False])\n",
    "\n",
    "keywords_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../06-clustering/'\n",
    "file_path = base_path + '_KMeans_transformers_euclidean.csv'\n",
    "keywords_st.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_st.groupby(\"Cluster_kmeans_euclidean\")[\"Terme\"].count()\n",
    "\n",
    "keywords_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **K-means clustering** (*NLTK*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but d'utiliser NLTK est de pouvoir prendre la distance cosinus entre les vecteurs plutôt que la distance Euclidienne, pour les embeddings basés sur le sentence transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import cluster\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from nltk.cluster import cosine_distance\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy import array, ndarray\n",
    "  \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the clusterer (will also assign the vectors to clusters)\n",
    "\n",
    "K = range(20,len(vocab))\n",
    "silhouette_scores = []\n",
    "for k in K:\n",
    "    X = embeddings\n",
    "    clusterer = cluster.KMeansClusterer(true_k, distance=cosine_distance, avoid_empty_clusters=True, repeats=25)\n",
    "    # Run LSA\n",
    "    # Since LSA/SVD results are not normalized,\n",
    "    # we redo the normalization to improve the result.\n",
    "    svd = TruncatedSVD(n_components=round(k/4))\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    X = lsa.fit_transform(X)\n",
    "    labels = clusterer.cluster(X, assign_clusters= True)\n",
    "    silhouette_scores.append([k, metrics.silhouette_score(X, labels)])\n",
    "\n",
    "df = DataFrame(silhouette_scores, columns=['Nombre de clusters (k)', 'Score Silhouette'])\n",
    "true_k = int(df[df['Score Silhouette'] == df['Score Silhouette'].max()]['Nombre de clusters (k)'])\n",
    "\n",
    "df[df['Nombre de clusters (k)'] == true_k]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithme = 'K-means'\n",
    "embedding = 'Sentence transformers'\n",
    "distance = 'cosine'\n",
    "features = len(vocab)\n",
    "\n",
    "add_results(algorithme, embedding, distance, features)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_st[\"kmeans_cosine\"] = list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_labels = set(labels)\n",
    "\n",
    "desired_labels = {x : None for x in current_labels} # (on initialise à None)\n",
    "\n",
    "for label in current_labels:\n",
    "    cluster = keywords_st[keywords_st[\"kmeans_cosine\"] == label]\n",
    "    max_freq = cluster['TF + DF'].max()\n",
    "    new_label = cluster[cluster['TF + DF'] == max_freq]['Terme'].values[0]\n",
    "\n",
    "    desired_labels[label] = new_label\n",
    "\n",
    "keywords_st['Cluster_kmeans_cosine'] = keywords_st['kmeans_cosine'].map(desired_labels)\n",
    "\n",
    "keywords_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EM clustering** (*sklearn*)\n",
    "\n",
    "> L'algorithme espérance-maximisation (en anglais expectation-maximization algorithm, souvent abrégé EM), est un algorithme itératif qui permet de trouver les paramètres du maximum de vraisemblance d'un modèle probabiliste lorsque ce dernier dépend de variables latentes non observables. \n",
    "\n",
    "(https://fr.wikipedia.org/wiki/Algorithme_esp%C3%A9rance-maximisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn.mixture GaussianMixture**  \n",
    "https://www.analyticsvidhya.com/blog/2019/10/gaussian-mixture-models-clustering/  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Encore une fois, on fait un premier essai sur nos one-hot embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "K = range(20,len(vocab))\n",
    "silhouette_scores = []\n",
    "for k in K:\n",
    "    X = keywords_oh[\"vector\"].to_list()\n",
    "    gmm = GaussianMixture(n_components=k, init_params='k-means++').fit(X)\n",
    "    # Run LSA\n",
    "    # Since LSA/SVD results are not normalized,\n",
    "    # we redo the normalization to improve the result.\n",
    "    svd = TruncatedSVD(n_components=k)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    X = lsa.fit_transform(X)\n",
    "    gmm.fit(X)\n",
    "    labels = gmm.predict(X)\n",
    "    silhouette_scores.append([k, metrics.silhouette_score(X, labels)])\n",
    "\n",
    "df = DataFrame(silhouette_scores, columns=['Nombre de clusters (k)', 'Score Silhouette'])\n",
    "true_k = int(df[df['Score Silhouette'] == df['Score Silhouette'].max()]['Nombre de clusters (k)'])\n",
    "\n",
    "df[df['Nombre de clusters (k)'] == true_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithme = 'Expectation-Maximization'\n",
    "embedding = 'One-Hot'\n",
    "distance = None\n",
    "features = len(vocab)\n",
    "\n",
    "add_results(algorithme, embedding, distance, features)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_oh[\"E-M\"] = list(labels)\n",
    "current_labels = set(labels)\n",
    "\n",
    "desired_labels = {x : None for x in current_labels} # (on initialise à None)\n",
    "\n",
    "for label in current_labels:\n",
    "    cluster = keywords_oh[keywords_oh[\"E-M\"] == label]\n",
    "    max_freq = cluster['TF + DF'].max()\n",
    "    new_label = cluster[cluster['TF + DF'] == max_freq]['Terme'].values[0]\n",
    "\n",
    "    desired_labels[label] = new_label\n",
    "\n",
    "keywords_oh['Cluster_E-M'] = keywords_oh['E-M'].map(desired_labels)\n",
    "\n",
    "keywords_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = keywords_oh[\"vector\"].tolist()\n",
    "\n",
    "pca = PCA(n_components=2).fit(vectors)\n",
    "pca_2d = pca.transform(vectors)\n",
    "\n",
    "plt.scatter(pca_2d[:,0], pca_2d[:,1], c=keywords_oh[\"GMM\"], s=keywords_oh[\"TF + DF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_labels = set(labels)\n",
    "\n",
    "desired_labels = {x : None for x in current_labels} # (on initialise à None)\n",
    "\n",
    "for label in current_labels:\n",
    "    cluster = keywords_oh[keywords_oh[\"GMM\"] == label]\n",
    "    max_freq = cluster['TF + DF'].max()\n",
    "    new_label = cluster[cluster['TF + DF'] == max_freq]['Terme'].values[0]\n",
    "\n",
    "    desired_labels[label] = new_label\n",
    "\n",
    "keywords_oh['Cluster_GMM'] = keywords_oh['GMM'].map(desired_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords_oh = keywords_oh[['Cluster_kmeans_euclidean', 'Cluster_GMM', 'Terme', 'Fréquence (TF)', 'Fréquence documentaire (DF)', 'TF + DF']]\n",
    "keywords_oh = keywords_oh[['Corpus', 'Cluster_kmeans_euclidean', 'Cluster_GMM', 'Terme', 'Fréquence (TF)', 'Fréquence documentaire (DF)', 'TF + DF', 'isMeSHTerm', 'isTaxoTerm', 'vector', 'kmeans']]\n",
    "keywords_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Deuxième essai, cette fois sur les sentence embeddings / transformers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(20,len(vocab))\n",
    "silhouette_scores = []\n",
    "for k in K:\n",
    "    X = embeddings\n",
    "    gmm = GaussianMixture(n_components=k, init_params='k-means++').fit(X)\n",
    "    # Run LSA\n",
    "    # Since LSA/SVD results are not normalized,\n",
    "    # we redo the normalization to improve the result.\n",
    "    svd = TruncatedSVD(n_components=round(k/4))\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    X = lsa.fit_transform(X)\n",
    "    gmm.fit(X)\n",
    "    labels = gmm.predict(X)\n",
    "    silhouette_scores.append([k, metrics.silhouette_score(X, labels)])\n",
    "\n",
    "df = DataFrame(silhouette_scores, columns=['Nombre de clusters (k)', 'Score Silhouette'])\n",
    "true_k = int(df[df['Score Silhouette'] == df['Score Silhouette'].max()]['Nombre de clusters (k)'])\n",
    "\n",
    "df[df['Nombre de clusters (k)'] == true_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithme = 'E-M'\n",
    "embedding = 'Sentence transformers'\n",
    "distance = None\n",
    "features = len(vocab)\n",
    "\n",
    "add_results(algorithme, embedding, distance, features)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[...] to do : ajouter le EM clustering / sklearn avec le sentence embeddings\n",
    "\n",
    "En fait je ne suis pas convaincue, parce que dans tous les cas ça ne donne jamais d'aussi bons résultats avec les sentence embeddings quant au scoresilhouette ; peut-être plus tard, mais peut-être pas une priorité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EM clustering** (*NLTK*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Premier essai, sur les one-hot embeddings / bag-of-words*\n",
    "\n",
    "Pas prioritaire non plus puisqu'on a déjà le EM Clustering fonctionnel avec celui de sk-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import cluster\n",
    "# from nltk.cluster import KMeansClusterer, euclidean_distance\n",
    "\n",
    "# #On initialise sur les kmeans\n",
    "# vectors = [array(f) for f in keywords_oh['vector'].tolist()]\n",
    "\n",
    "# clusterer = KMeansClusterer(true_k, euclidean_distance, initial_means=None, repeats=10)\n",
    "# means = clusterer.cluster(vectors, True, trace=True)\n",
    "\n",
    "# ##########\n",
    "# clusterer = cluster.EMClusterer(means, bias=0.1)\n",
    "# clusters = clusterer.cluster(vectors, True, trace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Deuxième essai, cette fois sur les sentence embeddings / transformers*  \n",
    "(même chose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Agglomerative clustering** (*NLTK / sklearn*)\n",
    "> In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:  \n",
    "> - Agglomerative: This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
    "> - Divisive: This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.    \n",
    "  \n",
    "(https://en.wikipedia.org/wiki/Hierarchical_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn AgglomerativeClustering / one-hot embeddings**  \n",
    "(le clusterer agglomératif de sklearn, qui permet d'utiliser la distance Euclidienne, mais pas celui d'NLTK)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "K = range(20,50)\n",
    "Sum_of_squared_distances = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K:\n",
    "    X = keywords_oh['vector'].tolist()\n",
    "    clustering = AgglomerativeClustering().fit(X)\n",
    "\n",
    "    #Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "    # Run LSA\n",
    "    # Since LSA/SVD results are not normalized,\n",
    "    # we redo the normalization to improve the k-means result.\n",
    "    svd = TruncatedSVD(n_components=round(k/4))\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    clusters = clustering.labels_\n",
    "    #keywords[\"Cluster\"] = list(kmeans.labels_)\n",
    "\n",
    "    #original_space_centroids = svd.inverse_transform(X)\n",
    "    #order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    silhouette_scores.append([k, metrics.silhouette_score(X, clusters)])\n",
    "\n",
    "df = DataFrame(silhouette_scores, columns=['Nombre de clusters (k)', 'Score Silhouette'])\n",
    "true_k = int(df[df['Score Silhouette'] == df['Score Silhouette'].max()]['Nombre de clusters (k)'])\n",
    "\n",
    "# print(\"Méthode Elbow\")\n",
    "# plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Sum_of_squared_distances')\n",
    "# plt.title('Elbow Method For Optimal k')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Score Silhouette\")\n",
    "print(\"On va regrouper nos termes en \" + str(true_k) + \" clusters.\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithme = 'AgglomerativeClustering'\n",
    "embedding = 'One-Hot'\n",
    "distance = 'Euclidean'\n",
    "features = len(vocab)\n",
    "\n",
    "add_results(algorithme, embedding, distance, features)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK Group Average Agglomerative Clustering (GAAC) / Sentence transformers embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import GAAClusterer\n",
    "\n",
    "K = range(20,50)\n",
    "Sum_of_squared_distances = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K:\n",
    "    X = embeddings\n",
    "    clusterer = GAAClusterer(k)\n",
    "\n",
    "    #Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "    # Run LSA\n",
    "    # Since LSA/SVD results are not normalized,\n",
    "    # we redo the normalization to improve the k-means result.\n",
    "    svd = TruncatedSVD(n_components=round(k/4))\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    clusters = clusterer.cluster(X, True)\n",
    "    #keywords[\"Cluster\"] = list(kmeans.labels_)\n",
    "\n",
    "    #original_space_centroids = svd.inverse_transform(X)\n",
    "    #order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    silhouette_scores.append([k, metrics.silhouette_score(X, clusters)])\n",
    "\n",
    "df = DataFrame(silhouette_scores, columns=['Nombre de clusters (k)', 'Score Silhouette'])\n",
    "true_k = int(df[df['Score Silhouette'] == df['Score Silhouette'].max()]['Nombre de clusters (k)'])\n",
    "\n",
    "# print(\"Méthode Elbow\")\n",
    "# plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Sum_of_squared_distances')\n",
    "# plt.title('Elbow Method For Optimal k')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Score Silhouette\")\n",
    "print(\"On va regrouper nos termes en \" + str(true_k) + \" clusters.\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithme = 'AgglomerativeClustering'\n",
    "embedding = 'Sentence transformers'\n",
    "distance = 'Cosine'\n",
    "features = len(vocab)\n",
    "\n",
    "add_results(algorithme, embedding, distance, features)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import GAAClusterer\n",
    "\n",
    "# use a set of tokens with 2D indices\n",
    "vectors = embeddings\n",
    "\n",
    "# test the GAAC clusterer with 4 clusters\n",
    "clusterer = GAAClusterer(50)\n",
    "clusters = clusterer.cluster(vectors, True)\n",
    "\n",
    "keywords_st[\"GAAC\"] = clusters\n",
    "\n",
    "current_labels = set(clusters)\n",
    "\n",
    "desired_labels = {x : None for x in current_labels} # (on initialise à None)\n",
    "\n",
    "for label in current_labels:\n",
    "    cluster = keywords_oh[keywords_st[\"GAAC\"] == label]\n",
    "    max_freq = cluster['TF + DF'].max()\n",
    "    new_label = cluster[cluster['TF + DF'] == max_freq]['Terme'].values[0]\n",
    "\n",
    "    desired_labels[label] = new_label\n",
    "\n",
    "keywords_st['Cluster_GAAC'] = keywords_st['GAAC'].map(desired_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Résultats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79bb76bbc4f9ba1f8df5efe8db67aae07079a51dc7b5004f49990e90f5993a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
