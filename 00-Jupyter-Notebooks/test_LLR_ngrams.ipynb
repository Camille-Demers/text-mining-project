{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log likelihood ratio - Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lire le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom, chi2\n",
    "\n",
    "import shutil, re\n",
    "from os import listdir, chdir, path\n",
    "from pathlib import Path\n",
    "\n",
    "acteur = 'pinel'\n",
    "sous_corpus = False \n",
    "tag = ''\n",
    "\n",
    "# Change the directory\n",
    "base_path = '../03-corpus/2-data/1-fr/'\n",
    "file_path = path.join(base_path, acteur) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\", encoding = \"UTF-8\") as f:\n",
    "        data = read_csv(file_path)\n",
    "        text = data['text'].tolist()\n",
    "        corpus = [(re.sub('\\d', '', t.strip('\\n').lower().replace('’', '\\''))) for t in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraire des bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Seulement les caractères alphabétiques\n",
    "tokenizer_re = RegexpTokenizer(r\"\\w\\'|\\w+\")\n",
    "\n",
    "tokens = nltk.flatten([tokenizer_re.tokenize(doc) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = list(bigrams(tokens))\n",
    "tg = list(trigrams(tokens))\n",
    "qg = list(ngrams(tokens, n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords fréquents en français (non lemmatisés)\n",
    "file_path = \"../04-filtrage/stopwords.txt\"\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = [t.lower().strip('\\n') for t in f.readlines()]\n",
    "    stopwords += [\"l'\", \"d'\", \"s'\"]\n",
    "\n",
    "# Filtrage des ngrammes\n",
    "bg = [t for t in bg if not t[0] in stopwords and not t[-1] in stopwords and len(t[0]) > 2 and len(t[-1]) > 2]\n",
    "tg = [t for t in tg if not t[0] in stopwords and not t[-1] in stopwords and len(t[0]) > 2 and len(t[-1]) > 2]\n",
    "qg = [t for t in qg if not t[0] in stopwords and not t[-1] in stopwords and len(t[0]) > 2 and len(t[-1]) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(tokens)\n",
    "fd_ug = nltk.FreqDist(tokens)\n",
    "\n",
    "# On prend les distributions de fréquences à partir des ngrammes issus de la liste des tokens parce que si on le fait à partir de la liste filtrée ci-dessus,\n",
    "# on risque de se retrouver avec des divisions par zéro à l'étape suivante\n",
    "fd_bg = nltk.FreqDist(bigrams(tokens)) #Bigrammes\n",
    "fd_tg = nltk.FreqDist(trigrams(tokens)) #Trigrammes\n",
    "fd_qg = nltk.FreqDist(ngrams(tokens, n=4)) # 4-grammes\n",
    "\n",
    "tab_b = pd.DataFrame(fd_bg.items(), columns=[\"Collocation\",\"Fréquence\"]) \n",
    "tab_t = pd.DataFrame(fd_tg.items(), columns=[\"Collocation\",\"Fréquence\"])\n",
    "tab_q = pd.DataFrame(fd_qg.items(), columns=[\"Collocation\",\"Fréquence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p1115145\\AppData\\Local\\Temp\\ipykernel_10732\\1418721661.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tab_n = tab_b.append(tab_t).append(tab_q).drop_duplicates()\n"
     ]
    }
   ],
   "source": [
    "tab_n = tab_b.append(tab_t).append(tab_q).drop_duplicates()\n",
    "tab_n.sort_values([\"Fréquence\"], \n",
    "                    axis=0,\n",
    "                    ascending=[False], \n",
    "                    inplace=True)\n",
    "\n",
    "\n",
    "tab_n.to_csv('../00-Jupyter-Notebooks/test_ngrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Au départ, on a 65200 ngrammes.\n"
     ]
    }
   ],
   "source": [
    "len_prior = len(tab_n)\n",
    "print(\"Au départ, on a {} ngrammes.\".format(len_prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood_ratio(c_prior, c_n, c_ngram, N):\n",
    "    \"\"\"\n",
    "    Compute the ratio of two hypotheses of likelihood and return the ratio.\n",
    "    The formula here and test verification values are taken from \n",
    "    Manning & Schūtze _Foundations of Statistical Natural Language Processing_ p.172-175\n",
    "    Parameters:\n",
    "    c_prior: count of word 1 if bigrams or count of [w1w2 .. w(n-1)] if ngram\n",
    "    c_n : count of word 2 if bigrams or count of wn if ngram\n",
    "    c12: count of bigram (w1, w2) if bigram or count of ngram if ngram\n",
    "    N: the number of words in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    p = c_n / N\n",
    "    p1 = c_ngram / c_prior\n",
    "    p2 = (c_n - c_ngram) / (N - c_prior)   \n",
    "    # We proactively trap a runtimeWarning: divide by zero encountered in log,\n",
    "    # which may occur with extreme collocations\n",
    "    import warnings\n",
    "    with warnings.catch_warnings(): # this will reset our filterwarnings setting\n",
    "        warnings.filterwarnings('error')\n",
    "        try:\n",
    "            return (np.log(binom.pmf(c_ngram, c_prior, p)) \n",
    "                    + np.log(binom.pmf(c_n - c_ngram, N - c_prior, p)) \n",
    "                    - np.log(binom.pmf(c_ngram, c_prior, p1) )\n",
    "                    - np.log(binom.pmf(c_n - c_ngram, N - c_prior, p2)))             \n",
    "        except Warning:\n",
    "            return np.inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "llr_bigrammes = []\n",
    "\n",
    "for b in set(bg):\n",
    "    c1 = fd_ug[b[0]]\n",
    "    c2 = fd_ug[b[1]]\n",
    "    c12 = fd_bg[b]\n",
    "\n",
    "    res = -2 * loglikelihood_ratio(c1, c2, c12, N)\n",
    "    p = chi2.sf(res, 1) # 1 degrees of freedom\n",
    "\n",
    "    if p < 0.05 or (res == float('-inf')):\n",
    "        llr_bigrammes.append({'Collocation' : b, 'LLR': res, 'p-value': p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "llr_trigrammes = []\n",
    "\n",
    "for t in set(tg):\n",
    "    c_prior = fd_bg[t[:2]] # Antécédent = P(w1w2) (si on considère que P (w1w2w3) = P(w3) | P(w1w2)\n",
    "    c_n = fd_ug[t[2]]\n",
    "    c_ngram = fd_tg[t] \n",
    "\n",
    "    res = -2 * loglikelihood_ratio(c_prior, c_n, c_ngram, N)\n",
    "    p = chi2.sf(res, 1) # 1 degrees of freedom\n",
    "\n",
    "    if p < 0.05 or (res == float('-inf')):\n",
    "        llr_trigrammes.append({'Collocation' : t, 'LLR': res, 'p-value': p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llr_quadgrammes = []\n",
    "\n",
    "for q in set(qg):\n",
    "    c_prior = fd_tg[q[:3]] # Antécédent = P(w1w2w3) si on considère que P (w1w2w3w4) = P(w4 | P(w1w2w3)\n",
    "    c_n = fd_ug[q[3]]\n",
    "    c_ngram = fd_qg[q]\n",
    "\n",
    "    res = -2 * loglikelihood_ratio(c_prior, c_n, c_ngram, N)\n",
    "    p = chi2.sf(res, 1) # 1 degrees of freedom\n",
    "\n",
    "    if p < 0.05 or (res == float('-inf')):\n",
    "        llr_quadgrammes.append({'Collocation' : q, 'LLR': res, 'p-value': p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p1115145\\AppData\\Local\\Temp\\ipykernel_10732\\3894320982.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df_bg.append(df_tg).append(df_qg)\n"
     ]
    }
   ],
   "source": [
    "df_bg = pd.DataFrame(llr_bigrammes)\n",
    "df_tg = pd.DataFrame(llr_trigrammes)\n",
    "df_qg = pd.DataFrame(llr_quadgrammes)\n",
    "\n",
    "df = df_bg.append(df_tg).append(df_qg)\n",
    "df.sort_values(['p-value'], \n",
    "            axis=0,\n",
    "            ascending=[True], \n",
    "            inplace=True)\n",
    "\n",
    "output_path = '../00-Jupyter-Notebooks/test_LLR_ngrams_CHUM.csv'\n",
    "df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on va le tester sur tous les bigrammes du corpus et filtrer pour ne conserver uniquement que ceux pour lesquels p est significatif (< 0.05) OU ceux pour lesquels LLR = -ing ET p-value = 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79bb76bbc4f9ba1f8df5efe8db67aae07079a51dc7b5004f49990e90f5993a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
